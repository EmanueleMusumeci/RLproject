LOGGER started at 2020-01-27 12:32:00.627051.
Currently active debug channels:
	rollouts
	training
	batch_info
	linesearch
	learning
	thread_rollouts
[2020-01-27 12:32:00.788140][__main__.TRPOAgent.log][learning]: Episode #0

[2020-01-27 12:32:01.035546][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 3000

[2020-01-27 12:32:01.049377][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 12:32:01.050191][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 12:32:01.050109][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 12:32:01.054293][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 12:32:03.612347][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 962

[2020-01-27 12:32:20.515870][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1670

[2020-01-27 12:32:20.516364][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 12:32:20.517021][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 12:32:20.517070][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 12:32:20.520369][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 12:32:36.859808][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 2833

[2020-01-27 12:32:49.457000][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 3165

[2020-01-27 12:32:49.457508][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 12:32:49.458109][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 12:32:49.458054][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 12:32:49.458980][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 12:33:05.024913][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 2456

[2020-01-27 12:33:06.091894][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 3615

[2020-01-27 12:33:06.092369][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 12:33:06.092999][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 12:33:06.093066][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 12:33:06.094747][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 12:33:24.630676][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 3700

[2020-01-27 12:34:03.909220][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 2432

[2020-01-27 12:34:03.909766][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 12:34:03.910595][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 12:34:03.910651][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 12:34:03.914272][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 12:34:06.070164][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 882

[2020-01-27 12:34:15.998285][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 2534

[2020-01-27 12:34:15.998714][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 12:34:15.999356][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 12:34:15.999287][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 12:34:16.000293][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 12:35:08.900985][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 2349

[2020-01-27 12:35:09.258616][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 2645

[2020-01-27 12:35:09.259318][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 12:35:09.260031][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 12:35:09.260090][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 12:35:09.263271][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 12:35:14.497599][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 2144

[2020-01-27 12:35:24.566770][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 3971

[2020-01-27 12:35:24.567532][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 12:35:24.568238][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 12:35:36.127143][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 3382

[2020-01-27 12:35:36.127550][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 12:35:36.137830][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 12:35:36.498033][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 12:35:36.504471][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 12:35:36.505905][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 38740, Batch size: 4500, Number of batches: 9

[2020-01-27 12:35:36.506289][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 12:35:36.538581][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 12:35:40.302383][__main__.TRPOAgent.log][training]: policy_gradient: [-0.07886161  0.04512066 -0.01348524 ...  0.11016659 -0.10781991
 -0.00234669]

[2020-01-27 12:35:40.302767][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:35:40.716560][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ -0.64121845  -7.40582275  -4.22468264 ...   2.730801    12.5742696
 -15.3050706 ], shape=(4547,), dtype=float64)

[2020-01-27 12:35:40.796436][__main__.TRPOAgent.log][linesearch]: improvement: -0.1913055639146961

[2020-01-27 12:35:40.825124][__main__.TRPOAgent.log][linesearch]: improvement: -0.0750043837870038

[2020-01-27 12:35:40.849663][__main__.TRPOAgent.log][linesearch]: improvement: -0.05173610434174236

[2020-01-27 12:35:40.875264][__main__.TRPOAgent.log][linesearch]: improvement: -0.037011761114357

[2020-01-27 12:35:40.899927][__main__.TRPOAgent.log][linesearch]: improvement: -0.038355818017521415

[2020-01-27 12:35:40.928140][__main__.TRPOAgent.log][linesearch]: improvement: -0.035632954570644415

[2020-01-27 12:35:40.955923][__main__.TRPOAgent.log][linesearch]: improvement: -0.025314189523015784

[2020-01-27 12:35:40.979321][__main__.TRPOAgent.log][linesearch]: improvement: -0.015428880871098727

[2020-01-27 12:35:41.007544][__main__.TRPOAgent.log][linesearch]: improvement: -0.00994507750425555

[2020-01-27 12:35:41.031304][__main__.TRPOAgent.log][linesearch]: improvement: -0.006272445395538284

[2020-01-27 12:35:41.031786][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.618302086714048e-07, Discarded policy loss value: -97.90806600744021

[2020-01-27 12:35:41.995592][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 118.85986817367501

[2020-01-27 12:35:42.003774][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 12:35:45.742276][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00537038 -0.00292859  0.00298503 ... -0.0001819   0.0218191
 -0.02163719]

[2020-01-27 12:35:45.742668][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:35:46.140332][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.1414609  -0.34846593  0.46679956 ... -0.82101808  0.42828672
  0.39273136], shape=(4547,), dtype=float64)

[2020-01-27 12:35:46.226269][__main__.TRPOAgent.log][linesearch]: improvement: 0.004260069769097541

[2020-01-27 12:35:46.254035][__main__.TRPOAgent.log][linesearch]: improvement: 0.0007162414389165761

[2020-01-27 12:35:46.279046][__main__.TRPOAgent.log][linesearch]: improvement: -2.467302414821404e-06

[2020-01-27 12:35:46.309505][__main__.TRPOAgent.log][linesearch]: improvement: -0.00012295308131982097

[2020-01-27 12:35:46.333974][__main__.TRPOAgent.log][linesearch]: improvement: -0.00048232056006103363

[2020-01-27 12:35:46.361376][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008324169412547988

[2020-01-27 12:35:46.386815][__main__.TRPOAgent.log][linesearch]: improvement: -0.0007825170709647722

[2020-01-27 12:35:46.412142][__main__.TRPOAgent.log][linesearch]: improvement: -0.0007707026764043334

[2020-01-27 12:35:46.440364][__main__.TRPOAgent.log][linesearch]: improvement: -0.0006084866485212004

[2020-01-27 12:35:46.466021][__main__.TRPOAgent.log][linesearch]: improvement: -0.0004354684225598149

[2020-01-27 12:35:46.466628][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.026056396980424e-07, Discarded policy loss value: -4.289975717310077

[2020-01-27 12:35:47.241388][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 114.5875552032304

[2020-01-27 12:35:47.246731][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 12:35:50.984362][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01557479 -0.00583686 -0.00014256 ...  0.03613964 -0.09005218
  0.05391254]

[2020-01-27 12:35:50.984821][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:35:51.379789][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.20473926  0.63497844  0.1440153  ...  0.28819343  1.11086059
 -1.39905402], shape=(4547,), dtype=float64)

[2020-01-27 12:35:51.459734][__main__.TRPOAgent.log][linesearch]: improvement: 0.0144781488081005

[2020-01-27 12:35:51.491481][__main__.TRPOAgent.log][linesearch]: improvement: -0.006229278041196551

[2020-01-27 12:35:51.515682][__main__.TRPOAgent.log][linesearch]: improvement: -0.011988906902546503

[2020-01-27 12:35:51.516137][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 4.797725116114878

[2020-01-27 12:35:52.265262][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 209.70044577363765

[2020-01-27 12:35:52.265662][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:35:52.276926][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 12:35:55.984770][__main__.TRPOAgent.log][training]: policy_gradient: [-0.05516546  0.02591787  0.00333549 ... -0.02538283 -0.10394215
  0.12932498]

[2020-01-27 12:35:55.985168][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:35:56.388602][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-2.04421741 -0.14694339  1.20879491 ... -2.83896127  0.40889192
  2.43006935], shape=(4547,), dtype=float64)

[2020-01-27 12:35:56.474960][__main__.TRPOAgent.log][linesearch]: improvement: -0.06366902894834148

[2020-01-27 12:35:56.500552][__main__.TRPOAgent.log][linesearch]: improvement: -0.03439553711918597

[2020-01-27 12:35:56.501003][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 3.991842943600956

[2020-01-27 12:35:57.273552][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 280.2610280177342

[2020-01-27 12:35:57.273958][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:35:57.281794][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 12:36:01.082231][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00188719 -0.02824547 -0.00081757 ... -0.22332292  0.45737849
 -0.23405557]

[2020-01-27 12:36:01.082616][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:36:01.475742][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.7270102  -0.2794682   0.02586155 ...  0.36823756 -0.43009616
  0.0618586 ], shape=(4547,), dtype=float64)

[2020-01-27 12:36:01.553350][__main__.TRPOAgent.log][linesearch]: improvement: -0.12752155352971872

[2020-01-27 12:36:01.582594][__main__.TRPOAgent.log][linesearch]: improvement: -0.08984665957857363

[2020-01-27 12:36:01.610261][__main__.TRPOAgent.log][linesearch]: improvement: -0.05305416761459725

[2020-01-27 12:36:01.636962][__main__.TRPOAgent.log][linesearch]: improvement: -0.029513066600996396

[2020-01-27 12:36:01.665182][__main__.TRPOAgent.log][linesearch]: improvement: -0.016187782756966307

[2020-01-27 12:36:01.690314][__main__.TRPOAgent.log][linesearch]: improvement: -0.009551813701951684

[2020-01-27 12:36:01.717125][__main__.TRPOAgent.log][linesearch]: improvement: -0.005800491397070395

[2020-01-27 12:36:01.744733][__main__.TRPOAgent.log][linesearch]: improvement: -0.003626473634099625

[2020-01-27 12:36:01.768882][__main__.TRPOAgent.log][linesearch]: improvement: -0.002244626433796526

[2020-01-27 12:36:01.797181][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014306815906652837

[2020-01-27 12:36:01.797617][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.469020848659765e-07, Discarded policy loss value: -3.109311066694016

[2020-01-27 12:36:02.565154][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.44331147637034

[2020-01-27 12:36:02.570514][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 12:36:06.331922][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00090713  0.00679223  0.0002384  ...  0.11227211 -0.13373178
  0.02145967]

[2020-01-27 12:36:06.332301][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:36:06.726816][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.34400437 -0.66024425 -0.61557302 ... -0.55188247  0.13393529
  0.41794718], shape=(4547,), dtype=float64)

[2020-01-27 12:36:06.806694][__main__.TRPOAgent.log][linesearch]: improvement: 0.03201392786621565

[2020-01-27 12:36:06.833746][__main__.TRPOAgent.log][linesearch]: improvement: -0.005064780373465638

[2020-01-27 12:36:06.834194][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.3012987563797158

[2020-01-27 12:36:07.599946][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.81215822943832

[2020-01-27 12:36:07.600355][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:36:07.608131][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 12:36:11.383716][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01646525  0.00551619 -0.         ...  0.02030802  0.00448268
 -0.0247907 ]

[2020-01-27 12:36:11.384113][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:36:11.786524][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 2.08940415  0.0820062   0.         ... -0.33055667  0.42111652
 -0.09055986], shape=(4547,), dtype=float64)

[2020-01-27 12:36:11.868586][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021586513999897194

[2020-01-27 12:36:11.897917][__main__.TRPOAgent.log][linesearch]: improvement: -0.004442587734852788

[2020-01-27 12:36:11.898372][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.42255208205692846

[2020-01-27 12:36:12.623721][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 204.46563395969

[2020-01-27 12:36:12.624114][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:36:12.635642][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 12:36:16.354586][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01727029 -0.         ... -0.04534272 -0.00689661
  0.05223932]

[2020-01-27 12:36:16.354971][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:36:16.758988][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.44791219  0.         ... -0.10711866 -0.44040921
  0.54752787], shape=(4547,), dtype=float64)

[2020-01-27 12:36:16.838978][__main__.TRPOAgent.log][linesearch]: improvement: -0.08228987069711602

[2020-01-27 12:36:16.867574][__main__.TRPOAgent.log][linesearch]: improvement: -0.04926481355839196

[2020-01-27 12:36:16.891632][__main__.TRPOAgent.log][linesearch]: improvement: -0.032120821950392786

[2020-01-27 12:36:16.919430][__main__.TRPOAgent.log][linesearch]: improvement: -0.019996914373783703

[2020-01-27 12:36:16.948801][__main__.TRPOAgent.log][linesearch]: improvement: -0.014293401595057209

[2020-01-27 12:36:16.972971][__main__.TRPOAgent.log][linesearch]: improvement: -0.008531921445396051

[2020-01-27 12:36:16.999726][__main__.TRPOAgent.log][linesearch]: improvement: -0.00485630265671344

[2020-01-27 12:36:17.027693][__main__.TRPOAgent.log][linesearch]: improvement: -0.002732087483836132

[2020-01-27 12:36:17.053361][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015761139635398491

[2020-01-27 12:36:17.079360][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009861231459613595

[2020-01-27 12:36:17.079925][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.142574927886585e-07, Discarded policy loss value: -0.7100690121878176

[2020-01-27 12:36:17.837085][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 117.49624398828101

[2020-01-27 12:36:17.841645][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 2740

[2020-01-27 12:36:20.150341][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00444713 -0.         ... -0.24322643 -0.35528307
  0.5985095 ]

[2020-01-27 12:36:20.150722][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:36:20.446874][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.44365824  0.         ... -1.45821556 -0.99378837
  2.45200393], shape=(4547,), dtype=float64)

[2020-01-27 12:36:20.508500][__main__.TRPOAgent.log][linesearch]: improvement: -1.2652345026860874

[2020-01-27 12:36:20.530224][__main__.TRPOAgent.log][linesearch]: improvement: -0.623868669453699

[2020-01-27 12:36:20.551228][__main__.TRPOAgent.log][linesearch]: improvement: -0.30557323421776283

[2020-01-27 12:36:20.551779][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 4.296837962983719

[2020-01-27 12:36:21.016876][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.64622112510534

[2020-01-27 12:36:21.017281][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:36:21.023738][__main__.TRPOAgent.log][learning]: Episode #1

[2020-01-27 12:36:21.024096][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 3000

[2020-01-27 12:36:21.038538][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 12:36:21.039215][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 12:36:21.039120][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 12:36:21.040362][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 12:36:40.422507][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 3958

[2020-01-27 12:37:04.793971][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 3577

[2020-01-27 12:37:04.794529][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 12:37:04.795298][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 12:37:04.795394][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 12:37:04.799515][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 12:37:30.760361][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 2785

[2020-01-27 12:38:12.234469][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 2824

[2020-01-27 12:38:12.235052][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 12:38:12.235925][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 12:38:12.235984][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 12:38:12.239874][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 12:39:06.436077][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 2420

[2020-01-27 12:39:06.785499][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 2801

[2020-01-27 12:39:06.785910][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 12:39:06.786668][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 12:39:06.786620][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 12:39:06.787811][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 12:39:25.057461][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 3520

[2020-01-27 12:39:30.895123][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1069

[2020-01-27 12:39:30.895544][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 12:39:30.896195][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 12:39:30.896253][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 12:39:30.898023][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 12:39:58.148759][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 3302

[2020-01-27 12:40:23.128333][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 3310

[2020-01-27 12:40:23.128873][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 12:40:23.129624][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 12:40:23.129677][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 12:40:23.133071][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 12:40:38.292025][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 2247

[2020-01-27 12:41:55.924968][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 2865

[2020-01-27 12:41:55.925650][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 12:41:55.926434][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 12:41:55.926604][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 12:41:55.928504][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 12:42:21.979621][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 2350

[2020-01-27 12:42:34.518714][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 2277

[2020-01-27 12:42:34.519133][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 12:42:34.519734][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 12:42:42.089549][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 3171

[2020-01-27 12:42:42.090100][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 12:42:42.101601][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 12:42:42.542360][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 12:42:42.567688][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 12:42:42.569354][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 42476, Batch size: 4500, Number of batches: 10

[2020-01-27 12:42:42.569856][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 12:42:42.605864][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 12:42:46.361952][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00522041 -0.         ... -0.19030044 -0.07775532
  0.26805576]

[2020-01-27 12:42:46.362375][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:42:46.756199][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.55366719  0.         ... -0.81783079 -0.03341094
  0.85124174], shape=(4547,), dtype=float64)

[2020-01-27 12:42:46.834291][__main__.TRPOAgent.log][linesearch]: improvement: -0.19903317869606418

[2020-01-27 12:42:46.861163][__main__.TRPOAgent.log][linesearch]: improvement: -0.12747230689891786

[2020-01-27 12:42:46.886998][__main__.TRPOAgent.log][linesearch]: improvement: -0.0819479965343195

[2020-01-27 12:42:46.915026][__main__.TRPOAgent.log][linesearch]: improvement: -0.05368512888447441

[2020-01-27 12:42:46.941934][__main__.TRPOAgent.log][linesearch]: improvement: -0.03415494289178689

[2020-01-27 12:42:46.968812][__main__.TRPOAgent.log][linesearch]: improvement: -0.021250716345607046

[2020-01-27 12:42:46.994573][__main__.TRPOAgent.log][linesearch]: improvement: -0.013025496868309006

[2020-01-27 12:42:47.022196][__main__.TRPOAgent.log][linesearch]: improvement: -0.00789222364369735

[2020-01-27 12:42:47.051546][__main__.TRPOAgent.log][linesearch]: improvement: -0.004813694072819175

[2020-01-27 12:42:47.076694][__main__.TRPOAgent.log][linesearch]: improvement: -0.002922251561143341

[2020-01-27 12:42:47.077154][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.537398482631493e-07, Discarded policy loss value: -10.166287723750557

[2020-01-27 12:42:47.855596][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.78200086013587

[2020-01-27 12:42:47.862633][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 12:42:51.610671][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00231501 -0.         ...  0.09266632 -0.09376145
  0.00109512]

[2020-01-27 12:42:51.611055][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:42:52.020994][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.50298295  0.         ... -0.29109048  0.34349344
 -0.05240296], shape=(4547,), dtype=float64)

[2020-01-27 12:42:52.098940][__main__.TRPOAgent.log][linesearch]: improvement: 0.022959187565488648

[2020-01-27 12:42:52.099382][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 2.1546934080112043

[2020-01-27 12:42:52.854381][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 203.22646279026202

[2020-01-27 12:42:52.854771][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:42:52.866112][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 12:42:56.569060][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01355975 -0.         ... -0.07954033  0.2153853
 -0.13584497]

[2020-01-27 12:42:56.569441][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:42:56.971459][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.05554816  0.         ...  0.10934841  0.11076795
 -0.22011635], shape=(4547,), dtype=float64)

[2020-01-27 12:42:57.052713][__main__.TRPOAgent.log][linesearch]: improvement: -0.04201909795888986

[2020-01-27 12:42:57.078449][__main__.TRPOAgent.log][linesearch]: improvement: -0.04968242697793851

[2020-01-27 12:42:57.078898][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.3258938918422014

[2020-01-27 12:42:57.829716][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 206.17863393170006

[2020-01-27 12:42:57.830109][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:42:57.838600][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 12:43:01.600812][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02682523 -0.         ... -0.04542126  0.15067307
 -0.10525181]

[2020-01-27 12:43:01.601220][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:43:01.993755][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.09326953  0.         ... -0.02684355 -0.01751802
  0.04436157], shape=(4547,), dtype=float64)

[2020-01-27 12:43:02.073122][__main__.TRPOAgent.log][linesearch]: improvement: -0.04270497189324596

[2020-01-27 12:43:02.103092][__main__.TRPOAgent.log][linesearch]: improvement: -0.02950751231466775

[2020-01-27 12:43:02.126874][__main__.TRPOAgent.log][linesearch]: improvement: -0.018569502194576426

[2020-01-27 12:43:02.154269][__main__.TRPOAgent.log][linesearch]: improvement: -0.011911961703282348

[2020-01-27 12:43:02.179320][__main__.TRPOAgent.log][linesearch]: improvement: -0.0071337964885827

[2020-01-27 12:43:02.207843][__main__.TRPOAgent.log][linesearch]: improvement: -0.004217582882108939

[2020-01-27 12:43:02.231872][__main__.TRPOAgent.log][linesearch]: improvement: -0.0027410029987602336

[2020-01-27 12:43:02.259245][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016366153732110256

[2020-01-27 12:43:02.288131][__main__.TRPOAgent.log][linesearch]: improvement: -0.001014387240983794

[2020-01-27 12:43:02.314801][__main__.TRPOAgent.log][linesearch]: improvement: -0.0006307529871851791

[2020-01-27 12:43:02.315285][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.850628590275799e-07, Discarded policy loss value: -2.118855978595886

[2020-01-27 12:43:03.082502][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 107.56363261776296

[2020-01-27 12:43:03.087912][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 12:43:06.857708][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.0705896  -0.         ... -0.26332112  0.25690693
  0.00641419]

[2020-01-27 12:43:06.858081][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:43:07.251526][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.42825991  0.         ...  0.26111735 -0.38518829
  0.12407095], shape=(4547,), dtype=float64)

[2020-01-27 12:43:07.331256][__main__.TRPOAgent.log][linesearch]: improvement: -0.4081646967101844

[2020-01-27 12:43:07.359243][__main__.TRPOAgent.log][linesearch]: improvement: -0.19248374038866722

[2020-01-27 12:43:07.384197][__main__.TRPOAgent.log][linesearch]: improvement: -0.08757728368649564

[2020-01-27 12:43:07.410147][__main__.TRPOAgent.log][linesearch]: improvement: -0.042398196358311546

[2020-01-27 12:43:07.437389][__main__.TRPOAgent.log][linesearch]: improvement: -0.02241113657585725

[2020-01-27 12:43:07.464077][__main__.TRPOAgent.log][linesearch]: improvement: -0.012537039676443262

[2020-01-27 12:43:07.488878][__main__.TRPOAgent.log][linesearch]: improvement: -0.0076061446205493155

[2020-01-27 12:43:07.512936][__main__.TRPOAgent.log][linesearch]: improvement: -0.004824997140081999

[2020-01-27 12:43:07.541780][__main__.TRPOAgent.log][linesearch]: improvement: -0.0031267401204573275

[2020-01-27 12:43:07.565105][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018948045133002633

[2020-01-27 12:43:07.565565][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.542476054344882e-07, Discarded policy loss value: -1.7785469560770144

[2020-01-27 12:43:08.338627][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.57288159912079

[2020-01-27 12:43:08.344118][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 12:43:12.121154][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.04859479 -0.         ...  0.02068565 -0.2823639
  0.26167825]

[2020-01-27 12:43:12.121548][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:43:12.513217][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.64893036  0.         ...  0.49027899 -0.80194112
  0.31166213], shape=(4547,), dtype=float64)

[2020-01-27 12:43:12.593550][__main__.TRPOAgent.log][linesearch]: improvement: 0.0769621274352903

[2020-01-27 12:43:12.617717][__main__.TRPOAgent.log][linesearch]: improvement: 0.0043243485005763915

[2020-01-27 12:43:12.651498][__main__.TRPOAgent.log][linesearch]: improvement: -0.011700383040074591

[2020-01-27 12:43:12.651979][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 2.796800790625245

[2020-01-27 12:43:13.405686][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 292.19056254232396

[2020-01-27 12:43:13.406076][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:43:13.413364][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 12:43:17.201294][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.09991177 -0.         ... -0.21219001  0.25436151
 -0.0421715 ]

[2020-01-27 12:43:17.201685][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:43:17.595795][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.28198129  0.         ...  0.4751441  -0.05570752
 -0.41943657], shape=(4547,), dtype=float64)

[2020-01-27 12:43:17.678566][__main__.TRPOAgent.log][linesearch]: improvement: -0.07901942936545003

[2020-01-27 12:43:17.706746][__main__.TRPOAgent.log][linesearch]: improvement: -0.09789576379634912

[2020-01-27 12:43:17.731043][__main__.TRPOAgent.log][linesearch]: improvement: -0.06825366734785687

[2020-01-27 12:43:17.759923][__main__.TRPOAgent.log][linesearch]: improvement: -0.04330212892267937

[2020-01-27 12:43:17.784105][__main__.TRPOAgent.log][linesearch]: improvement: -0.02695921309228977

[2020-01-27 12:43:17.814475][__main__.TRPOAgent.log][linesearch]: improvement: -0.0158896250506384

[2020-01-27 12:43:17.840249][__main__.TRPOAgent.log][linesearch]: improvement: -0.009209168752744024

[2020-01-27 12:43:17.870482][__main__.TRPOAgent.log][linesearch]: improvement: -0.0053330904477650876

[2020-01-27 12:43:17.896570][__main__.TRPOAgent.log][linesearch]: improvement: -0.003136367462572842

[2020-01-27 12:43:17.928021][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018554840024540553

[2020-01-27 12:43:17.928468][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.303091384660931e-07, Discarded policy loss value: -2.341579483653781

[2020-01-27 12:43:18.700361][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 107.85147413432583

[2020-01-27 12:43:18.706382][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 12:43:22.434810][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00475129 -0.         ... -0.12120867  0.02246784
  0.09874083]

[2020-01-27 12:43:22.435198][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:43:22.835217][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.15388296  0.         ...  0.05188823 -0.07053674
  0.01864851], shape=(4547,), dtype=float64)

[2020-01-27 12:43:22.916604][__main__.TRPOAgent.log][linesearch]: improvement: -0.04128488895430493

[2020-01-27 12:43:22.945002][__main__.TRPOAgent.log][linesearch]: improvement: -0.02355352313743664

[2020-01-27 12:43:22.974111][__main__.TRPOAgent.log][linesearch]: improvement: -0.014232439299253219

[2020-01-27 12:43:22.974555][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 2.3482119206416043

[2020-01-27 12:43:23.740769][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 206.32647191652774

[2020-01-27 12:43:23.741214][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:43:23.757507][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 12:43:27.511339][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.13935435 -0.         ... -0.37811578  0.4168831
 -0.03876732]

[2020-01-27 12:43:27.511714][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:43:27.912143][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.07866586  0.         ... -0.08712034 -0.09768529
  0.18480562], shape=(4547,), dtype=float64)

[2020-01-27 12:43:27.990083][__main__.TRPOAgent.log][linesearch]: improvement: -0.13015811589780935

[2020-01-27 12:43:28.016604][__main__.TRPOAgent.log][linesearch]: improvement: -0.07302640333559918

[2020-01-27 12:43:28.042637][__main__.TRPOAgent.log][linesearch]: improvement: -0.04556988314662824

[2020-01-27 12:43:28.070878][__main__.TRPOAgent.log][linesearch]: improvement: -0.030026076458689932

[2020-01-27 12:43:28.098381][__main__.TRPOAgent.log][linesearch]: improvement: -0.01927127122236305

[2020-01-27 12:43:28.124031][__main__.TRPOAgent.log][linesearch]: improvement: -0.01191740550738496

[2020-01-27 12:43:28.151736][__main__.TRPOAgent.log][linesearch]: improvement: -0.007329874842751494

[2020-01-27 12:43:28.177572][__main__.TRPOAgent.log][linesearch]: improvement: -0.004445926635106456

[2020-01-27 12:43:28.200910][__main__.TRPOAgent.log][linesearch]: improvement: -0.002695049464170207

[2020-01-27 12:43:28.229835][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016196587020975706

[2020-01-27 12:43:28.230278][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.333230945716247e-07, Discarded policy loss value: -3.120877002770367

[2020-01-27 12:43:28.975267][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.69953078158028

[2020-01-27 12:43:28.978728][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 1976

[2020-01-27 12:43:30.649759][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00688938 -0.         ... -0.04261223 -0.08285556
  0.12546779]

[2020-01-27 12:43:30.650150][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:43:30.906498][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.90767703  0.         ... -0.85342866 -0.09855876
  0.95198742], shape=(4547,), dtype=float64)

[2020-01-27 12:43:30.963906][__main__.TRPOAgent.log][linesearch]: improvement: -0.12722270938489233

[2020-01-27 12:43:30.984396][__main__.TRPOAgent.log][linesearch]: improvement: -0.1457967408215357

[2020-01-27 12:43:31.005099][__main__.TRPOAgent.log][linesearch]: improvement: -0.10783260798687855

[2020-01-27 12:43:31.005524][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 1.7243027392581995

[2020-01-27 12:43:31.343933][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 227.49356645524006

[2020-01-27 12:43:31.344316][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:43:31.363969][__main__.TRPOAgent.log][learning]: Episode #2

[2020-01-27 12:43:31.364347][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 3000

[2020-01-27 12:43:31.393310][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 12:43:31.394098][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 12:43:31.393996][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 12:43:31.395169][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 12:43:38.630503][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 3000

[2020-01-27 12:43:38.699633][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 3000

[2020-01-27 12:43:38.700180][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 12:43:38.701035][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 12:43:38.700965][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 12:43:38.702068][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 12:43:45.908848][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 3000

[2020-01-27 12:43:45.958125][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 3000

[2020-01-27 12:43:45.958588][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 12:43:45.959069][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 12:43:45.959224][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 12:43:45.962335][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 12:43:50.498335][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1840

[2020-01-27 12:43:51.708729][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 3000

[2020-01-27 12:43:51.709317][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 12:43:51.710040][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 12:43:51.710166][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 12:43:51.712846][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 12:43:59.077030][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 3000

[2020-01-27 12:43:59.089657][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 3000

[2020-01-27 12:43:59.090230][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 12:43:59.091096][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 12:43:59.091277][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 12:43:59.094372][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 12:44:06.531870][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 3000

[2020-01-27 12:44:06.543757][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 3000

[2020-01-27 12:44:06.544232][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 12:44:06.545018][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 12:44:06.545619][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 12:44:06.544913][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 12:44:13.946718][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 3000

[2020-01-27 12:44:13.947373][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 3000

[2020-01-27 12:44:13.948356][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 12:44:13.949018][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 12:44:13.949092][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 12:44:13.952818][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 12:44:20.874842][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 2787

[2020-01-27 12:44:21.039213][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 3000

[2020-01-27 12:44:21.039721][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 12:44:21.040293][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 12:44:21.040402][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 12:44:21.042232][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 12:44:28.348049][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 3000

[2020-01-27 12:44:28.404933][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 2973

[2020-01-27 12:44:28.405367][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 12:44:28.406018][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 12:44:28.406101][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 12:44:28.408069][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 12:44:36.807028][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 3000

[2020-01-27 12:44:36.866728][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 3000

[2020-01-27 12:44:36.867247][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 12:44:36.867978][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 12:44:36.868075][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 12:44:36.871859][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 12:44:42.062489][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1945

[2020-01-27 12:44:43.495025][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 3000

[2020-01-27 12:44:43.495720][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 12:44:43.496693][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 12:44:43.496577][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 12:44:43.497959][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 12:44:51.190520][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 3000

[2020-01-27 12:44:51.235483][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 3000

[2020-01-27 12:44:51.235984][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 12:44:51.236477][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 12:44:51.236552][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 12:44:51.238783][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 12:44:58.612390][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 3000

[2020-01-27 12:44:58.691957][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 3000

[2020-01-27 12:44:58.692415][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 12:44:58.693124][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 12:44:58.693035][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 12:44:58.694160][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 12:45:06.024509][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 2926

[2020-01-27 12:45:06.083785][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 3000

[2020-01-27 12:45:06.084321][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 12:45:06.084926][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 12:45:06.084844][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 12:45:06.086049][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 12:45:13.662291][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 3000

[2020-01-27 12:45:13.680769][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 3000

[2020-01-27 12:45:13.681298][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 12:45:13.681949][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 12:45:13.682021][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 12:45:13.684962][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 12:45:21.272895][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 2553

[2020-01-27 12:45:21.741012][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 3000

[2020-01-27 12:45:21.741527][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 12:45:21.763541][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 12:45:23.723980][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 12:45:23.761032][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 12:45:23.762779][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 87024, Batch size: 4500, Number of batches: 20

[2020-01-27 12:45:23.763256][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 12:45:23.832490][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 12:45:28.011286][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.07654224 -0.         ... -0.02496281 -0.2367236
  0.2616864 ]

[2020-01-27 12:45:28.011696][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:45:28.426724][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.14811408  0.         ... -0.07610202  0.11515114
 -0.03904912], shape=(4547,), dtype=float64)

[2020-01-27 12:45:28.510124][__main__.TRPOAgent.log][linesearch]: improvement: -0.017658607576301444

[2020-01-27 12:45:28.536143][__main__.TRPOAgent.log][linesearch]: improvement: -0.03068399915349762

[2020-01-27 12:45:28.561218][__main__.TRPOAgent.log][linesearch]: improvement: -0.020595093255292074

[2020-01-27 12:45:28.561825][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.5377861592900791

[2020-01-27 12:45:29.367050][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.59567844933898

[2020-01-27 12:45:29.367443][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:45:29.375216][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 12:45:33.990899][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05241505 -0.         ...  0.09207471  0.03779062
 -0.12986533]

[2020-01-27 12:45:33.991532][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:45:34.514283][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.69726737  0.         ...  0.18365345 -0.45260099
  0.26894754], shape=(4547,), dtype=float64)

[2020-01-27 12:45:34.602498][__main__.TRPOAgent.log][linesearch]: improvement: -0.19296874012029042

[2020-01-27 12:45:34.642655][__main__.TRPOAgent.log][linesearch]: improvement: -0.12321203097061062

[2020-01-27 12:45:34.671114][__main__.TRPOAgent.log][linesearch]: improvement: -0.07317005175659386

[2020-01-27 12:45:34.671618][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 1.407758455694422

[2020-01-27 12:45:35.502783][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 203.8545635094092

[2020-01-27 12:45:35.503183][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:45:35.511801][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 12:45:39.995128][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03330035 -0.         ...  0.06192029  0.103522
 -0.16544229]

[2020-01-27 12:45:39.995521][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:45:40.426059][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.65197885  0.         ... -0.33530888 -0.65322273
  0.98853161], shape=(4547,), dtype=float64)

[2020-01-27 12:45:40.514058][__main__.TRPOAgent.log][linesearch]: improvement: -0.14691932114249306

[2020-01-27 12:45:40.538975][__main__.TRPOAgent.log][linesearch]: improvement: -0.07754375120802814

[2020-01-27 12:45:40.569537][__main__.TRPOAgent.log][linesearch]: improvement: -0.036925293553780936

[2020-01-27 12:45:40.597547][__main__.TRPOAgent.log][linesearch]: improvement: -0.021273131549976565

[2020-01-27 12:45:40.624413][__main__.TRPOAgent.log][linesearch]: improvement: -0.012865291517864197

[2020-01-27 12:45:40.657426][__main__.TRPOAgent.log][linesearch]: improvement: -0.007648218177791932

[2020-01-27 12:45:40.684831][__main__.TRPOAgent.log][linesearch]: improvement: -0.0043932088756086785

[2020-01-27 12:45:40.716195][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025590761417122465

[2020-01-27 12:45:40.745466][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015137855709572712

[2020-01-27 12:45:40.770757][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009030943823549187

[2020-01-27 12:45:40.771195][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.921242762342324e-07, Discarded policy loss value: -2.2722187736552497

[2020-01-27 12:45:41.592157][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 107.72381163540086

[2020-01-27 12:45:41.598237][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 12:45:46.110087][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.12262173 -0.         ...  0.06079436 -0.40833481
  0.34754045]

[2020-01-27 12:45:46.110477][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:45:46.549754][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.44865997  0.         ... -0.29811861  0.29497376
  0.00314484], shape=(4547,), dtype=float64)

[2020-01-27 12:45:46.644877][__main__.TRPOAgent.log][linesearch]: improvement: -0.002365811465811163

[2020-01-27 12:45:46.645388][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 4.070356806090151

[2020-01-27 12:45:47.456431][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.7728119813354

[2020-01-27 12:45:47.456828][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:45:47.469264][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 12:45:51.870983][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03151431 -0.         ... -0.13247245  0.23677753
 -0.10430509]

[2020-01-27 12:45:51.871391][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:45:52.297623][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.17480774  0.         ... -0.47704921  0.11312192
  0.36392729], shape=(4547,), dtype=float64)

[2020-01-27 12:45:52.383384][__main__.TRPOAgent.log][linesearch]: improvement: -0.11886507169475147

[2020-01-27 12:45:52.410337][__main__.TRPOAgent.log][linesearch]: improvement: -0.07244103519805867

[2020-01-27 12:45:52.439138][__main__.TRPOAgent.log][linesearch]: improvement: -0.043110351467124275

[2020-01-27 12:45:52.467419][__main__.TRPOAgent.log][linesearch]: improvement: -0.02550110680596651

[2020-01-27 12:45:52.494099][__main__.TRPOAgent.log][linesearch]: improvement: -0.015137141079671235

[2020-01-27 12:45:52.521304][__main__.TRPOAgent.log][linesearch]: improvement: -0.009043420441758876

[2020-01-27 12:45:52.549680][__main__.TRPOAgent.log][linesearch]: improvement: -0.005418780885409369

[2020-01-27 12:45:52.575748][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032629016627372387

[2020-01-27 12:45:52.605440][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019656866408799267

[2020-01-27 12:45:52.637790][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011873463014611119

[2020-01-27 12:45:52.638235][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.139562895042266e-07, Discarded policy loss value: -3.5748524384518134

[2020-01-27 12:45:53.445528][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.88567681952321

[2020-01-27 12:45:53.451715][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 12:45:57.861615][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02144798 -0.         ...  0.15932212 -0.13849174
 -0.02083038]

[2020-01-27 12:45:57.862090][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:45:58.277028][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.36595144  0.         ...  0.37377094 -0.39360184
  0.0198309 ], shape=(4547,), dtype=float64)

[2020-01-27 12:45:58.369495][__main__.TRPOAgent.log][linesearch]: improvement: 0.07258963999502033

[2020-01-27 12:45:58.397417][__main__.TRPOAgent.log][linesearch]: improvement: -0.002931471296999888

[2020-01-27 12:45:58.397952][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.863545271656573

[2020-01-27 12:45:59.217053][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.64110295893192

[2020-01-27 12:45:59.217478][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:45:59.224833][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 12:46:03.687982][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00640027 -0.         ... -0.15273097 -0.15131847
  0.30404944]

[2020-01-27 12:46:03.688396][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:04.128751][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.02159285  0.         ... -0.36960227 -0.73743426
  1.10703653], shape=(4547,), dtype=float64)

[2020-01-27 12:46:04.223374][__main__.TRPOAgent.log][linesearch]: improvement: -0.17028900465858088

[2020-01-27 12:46:04.255321][__main__.TRPOAgent.log][linesearch]: improvement: -0.10025384350817654

[2020-01-27 12:46:04.255762][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.09460421088374359

[2020-01-27 12:46:05.102398][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.98043346738103

[2020-01-27 12:46:05.102791][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:46:05.115805][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 12:46:10.013907][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01628204 -0.         ... -0.01516118  0.13223013
 -0.11706895]

[2020-01-27 12:46:10.014387][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:10.452542][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.18857483  0.         ... -0.06707943 -0.15268531
  0.21976474], shape=(4547,), dtype=float64)

[2020-01-27 12:46:10.539261][__main__.TRPOAgent.log][linesearch]: improvement: -0.08615216040505436

[2020-01-27 12:46:10.564634][__main__.TRPOAgent.log][linesearch]: improvement: -0.05530856270641671

[2020-01-27 12:46:10.595500][__main__.TRPOAgent.log][linesearch]: improvement: -0.03330274436470981

[2020-01-27 12:46:10.622115][__main__.TRPOAgent.log][linesearch]: improvement: -0.0205412196268413

[2020-01-27 12:46:10.653838][__main__.TRPOAgent.log][linesearch]: improvement: -0.012786283630083606

[2020-01-27 12:46:10.679754][__main__.TRPOAgent.log][linesearch]: improvement: -0.007857378382513325

[2020-01-27 12:46:10.709507][__main__.TRPOAgent.log][linesearch]: improvement: -0.004740473582605542

[2020-01-27 12:46:10.737299][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028180303550819907

[2020-01-27 12:46:10.762514][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016902772734077587

[2020-01-27 12:46:10.792371][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010138484073838328

[2020-01-27 12:46:10.792815][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.444706740702364e-07, Discarded policy loss value: -2.3663901009895043

[2020-01-27 12:46:11.624846][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.59327910266133

[2020-01-27 12:46:11.639000][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 12:46:16.557263][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00790873 -0.         ...  0.04709098 -0.48054912
  0.43345814]

[2020-01-27 12:46:16.557659][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:17.051893][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.82034015  0.         ... -0.54673179  0.17027408
  0.37645771], shape=(4547,), dtype=float64)

[2020-01-27 12:46:17.141886][__main__.TRPOAgent.log][linesearch]: improvement: -0.34936124388310974

[2020-01-27 12:46:17.172195][__main__.TRPOAgent.log][linesearch]: improvement: -0.230374079529565

[2020-01-27 12:46:17.172659][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.4111255258871704

[2020-01-27 12:46:18.041995][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.62404892799566

[2020-01-27 12:46:18.042427][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:46:18.049829][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 12:46:22.645118][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00399391 -0.         ... -0.1603261   0.00724998
  0.15307612]

[2020-01-27 12:46:22.645512][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:23.118413][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.15583683  0.         ... -0.26004519 -0.01981518
  0.27986037], shape=(4547,), dtype=float64)

[2020-01-27 12:46:23.207412][__main__.TRPOAgent.log][linesearch]: improvement: -0.11003950896139791

[2020-01-27 12:46:23.235817][__main__.TRPOAgent.log][linesearch]: improvement: -0.06465828250246108

[2020-01-27 12:46:23.264417][__main__.TRPOAgent.log][linesearch]: improvement: -0.034333982796698415

[2020-01-27 12:46:23.296413][__main__.TRPOAgent.log][linesearch]: improvement: -0.019238126837760605

[2020-01-27 12:46:23.324506][__main__.TRPOAgent.log][linesearch]: improvement: -0.011506308037188662

[2020-01-27 12:46:23.355069][__main__.TRPOAgent.log][linesearch]: improvement: -0.007225517694388062

[2020-01-27 12:46:23.382628][__main__.TRPOAgent.log][linesearch]: improvement: -0.0044615105307483915

[2020-01-27 12:46:23.412815][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028215812339993374

[2020-01-27 12:46:23.442988][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017874970949460778

[2020-01-27 12:46:23.473522][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011086027944531285

[2020-01-27 12:46:23.474015][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.176202156863307e-07, Discarded policy loss value: -1.2656259468672293

[2020-01-27 12:46:24.385058][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 107.15292198009354

[2020-01-27 12:46:24.393606][__main__.TRPOAgent.log][batch_info]: Batch #10, batch length: 4500

[2020-01-27 12:46:28.985076][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.17684184 -0.         ...  0.39284924 -0.41684612
  0.02399687]

[2020-01-27 12:46:28.985690][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:29.438093][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.80244099  0.         ... -0.5882046   0.94485845
 -0.35665385], shape=(4547,), dtype=float64)

[2020-01-27 12:46:29.533614][__main__.TRPOAgent.log][linesearch]: improvement: -0.1568372211394089

[2020-01-27 12:46:29.563930][__main__.TRPOAgent.log][linesearch]: improvement: -0.12412035198219051

[2020-01-27 12:46:29.564365][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 3.6754631247788203

[2020-01-27 12:46:30.433258][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.35297063181596

[2020-01-27 12:46:30.433650][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:46:30.444194][__main__.TRPOAgent.log][batch_info]: Batch #11, batch length: 4500

[2020-01-27 12:46:35.419583][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07401088 -0.         ... -0.20645245  0.19661714
  0.00983531]

[2020-01-27 12:46:35.419987][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:35.912257][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.16625068  0.         ... -0.4655248   0.17108832
  0.29443647], shape=(4547,), dtype=float64)

[2020-01-27 12:46:36.005929][__main__.TRPOAgent.log][linesearch]: improvement: 0.05640836005380212

[2020-01-27 12:46:36.035529][__main__.TRPOAgent.log][linesearch]: improvement: -0.009448215469418209

[2020-01-27 12:46:36.064798][__main__.TRPOAgent.log][linesearch]: improvement: -0.01950740032163356

[2020-01-27 12:46:36.094603][__main__.TRPOAgent.log][linesearch]: improvement: -0.016477550225593607

[2020-01-27 12:46:36.120035][__main__.TRPOAgent.log][linesearch]: improvement: -0.011573345942245794

[2020-01-27 12:46:36.150121][__main__.TRPOAgent.log][linesearch]: improvement: -0.007768585376152881

[2020-01-27 12:46:36.179719][__main__.TRPOAgent.log][linesearch]: improvement: -0.004724797568230965

[2020-01-27 12:46:36.207811][__main__.TRPOAgent.log][linesearch]: improvement: -0.003011436551463298

[2020-01-27 12:46:36.233540][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019477689007936227

[2020-01-27 12:46:36.264359][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012253061592857328

[2020-01-27 12:46:36.264815][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.583393233016177e-07, Discarded policy loss value: -3.383064257350423

[2020-01-27 12:46:37.105093][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.30687155446056

[2020-01-27 12:46:37.113329][__main__.TRPOAgent.log][batch_info]: Batch #12, batch length: 4500

[2020-01-27 12:46:41.398814][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10554732 -0.         ...  0.15707033 -0.29317989
  0.13610956]

[2020-01-27 12:46:41.399205][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:41.808250][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.304419    0.         ...  0.36489427 -0.10282702
 -0.26206725], shape=(4547,), dtype=float64)

[2020-01-27 12:46:41.896819][__main__.TRPOAgent.log][linesearch]: improvement: -0.17930498014818497

[2020-01-27 12:46:41.920154][__main__.TRPOAgent.log][linesearch]: improvement: -0.1136941363951145

[2020-01-27 12:46:41.920596][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.45926245785845

[2020-01-27 12:46:42.723111][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.7435150049325

[2020-01-27 12:46:42.723588][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:46:42.731204][__main__.TRPOAgent.log][batch_info]: Batch #13, batch length: 4500

[2020-01-27 12:46:47.076421][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05530307 -0.         ... -0.08318315  0.20989149
 -0.12670834]

[2020-01-27 12:46:47.076803][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:47.496380][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.51631664  0.         ...  0.42147901 -0.33531731
 -0.08616171], shape=(4547,), dtype=float64)

[2020-01-27 12:46:47.579479][__main__.TRPOAgent.log][linesearch]: improvement: -0.07198604345579751

[2020-01-27 12:46:47.605089][__main__.TRPOAgent.log][linesearch]: improvement: -0.04144884887154543

[2020-01-27 12:46:47.634069][__main__.TRPOAgent.log][linesearch]: improvement: -0.02871760619015129

[2020-01-27 12:46:47.659301][__main__.TRPOAgent.log][linesearch]: improvement: -0.02146093033513541

[2020-01-27 12:46:47.683052][__main__.TRPOAgent.log][linesearch]: improvement: -0.014116701866169201

[2020-01-27 12:46:47.710094][__main__.TRPOAgent.log][linesearch]: improvement: -0.008709411624296504

[2020-01-27 12:46:47.733592][__main__.TRPOAgent.log][linesearch]: improvement: -0.0053858133927864316

[2020-01-27 12:46:47.759826][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033689061320982816

[2020-01-27 12:46:47.786362][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021146021946201188

[2020-01-27 12:46:47.814956][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013152291417073414

[2020-01-27 12:46:47.815432][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.003387255686875e-07, Discarded policy loss value: -2.295848586394334

[2020-01-27 12:46:48.634521][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.43802516387757

[2020-01-27 12:46:48.640033][__main__.TRPOAgent.log][batch_info]: Batch #14, batch length: 4500

[2020-01-27 12:46:52.782737][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.13787604 -0.         ...  0.17533878 -0.38333263
  0.20799385]

[2020-01-27 12:46:52.783179][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:53.269169][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.07933333  0.         ... -0.22682376 -0.25134802
  0.47817178], shape=(4547,), dtype=float64)

[2020-01-27 12:46:53.354179][__main__.TRPOAgent.log][linesearch]: improvement: 0.07356290314218494

[2020-01-27 12:46:53.354639][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 4.054143192300266

[2020-01-27 12:46:54.205394][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.93487452018812

[2020-01-27 12:46:54.205806][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:46:54.214536][__main__.TRPOAgent.log][batch_info]: Batch #15, batch length: 4500

[2020-01-27 12:46:58.311455][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07069139 -0.         ... -0.13234519  0.28687211
 -0.15452692]

[2020-01-27 12:46:58.311845][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:46:58.728401][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.08764514  0.         ...  0.10545058 -0.05170623
 -0.05374436], shape=(4547,), dtype=float64)

[2020-01-27 12:46:58.808526][__main__.TRPOAgent.log][linesearch]: improvement: -0.10750728710046698

[2020-01-27 12:46:58.837338][__main__.TRPOAgent.log][linesearch]: improvement: -0.06319248960446577

[2020-01-27 12:46:58.864228][__main__.TRPOAgent.log][linesearch]: improvement: -0.037594595329295544

[2020-01-27 12:46:58.889538][__main__.TRPOAgent.log][linesearch]: improvement: -0.02244399093129923

[2020-01-27 12:46:58.914980][__main__.TRPOAgent.log][linesearch]: improvement: -0.0134027823478311

[2020-01-27 12:46:58.942867][__main__.TRPOAgent.log][linesearch]: improvement: -0.008030668374183314

[2020-01-27 12:46:58.969084][__main__.TRPOAgent.log][linesearch]: improvement: -0.004812593609666571

[2020-01-27 12:46:58.998057][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028747510048088287

[2020-01-27 12:46:59.027129][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017125866990825855

[2020-01-27 12:46:59.054167][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010238290743589396

[2020-01-27 12:46:59.054611][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.692426936280389e-07, Discarded policy loss value: -3.3881683312914572

[2020-01-27 12:46:59.863468][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.64402193204042

[2020-01-27 12:46:59.868848][__main__.TRPOAgent.log][batch_info]: Batch #16, batch length: 4500

[2020-01-27 12:47:04.083738][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.22826167 -0.         ...  0.08724082 -0.49026859
  0.40302776]

[2020-01-27 12:47:04.084174][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:47:04.502714][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.26072351  0.         ...  0.56813624 -0.31612857
 -0.25200767], shape=(4547,), dtype=float64)

[2020-01-27 12:47:04.587273][__main__.TRPOAgent.log][linesearch]: improvement: -0.28748487278242685

[2020-01-27 12:47:04.614435][__main__.TRPOAgent.log][linesearch]: improvement: -0.15308136708403053

[2020-01-27 12:47:04.614897][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.655450454072203

[2020-01-27 12:47:05.377823][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.42907022473656

[2020-01-27 12:47:05.378228][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:47:05.388380][__main__.TRPOAgent.log][batch_info]: Batch #17, batch length: 4500

[2020-01-27 12:47:09.758613][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05668634 -0.         ... -0.11518399  0.15434426
 -0.03916028]

[2020-01-27 12:47:09.759009][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:47:10.209430][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.42654205  0.         ...  0.34612597 -0.22980755
 -0.11631842], shape=(4547,), dtype=float64)

[2020-01-27 12:47:10.294900][__main__.TRPOAgent.log][linesearch]: improvement: 0.0035070769689522496

[2020-01-27 12:47:10.326747][__main__.TRPOAgent.log][linesearch]: improvement: -0.008070479962533295

[2020-01-27 12:47:10.356735][__main__.TRPOAgent.log][linesearch]: improvement: -0.009042617001342235

[2020-01-27 12:47:10.392222][__main__.TRPOAgent.log][linesearch]: improvement: -0.007587298244646101

[2020-01-27 12:47:10.428678][__main__.TRPOAgent.log][linesearch]: improvement: -0.005204863890190481

[2020-01-27 12:47:10.463659][__main__.TRPOAgent.log][linesearch]: improvement: -0.00386963008991148

[2020-01-27 12:47:10.496852][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028248874301011195

[2020-01-27 12:47:10.527385][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018107706131518242

[2020-01-27 12:47:10.561087][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011442556706022788

[2020-01-27 12:47:10.591799][__main__.TRPOAgent.log][linesearch]: improvement: -0.0007539840550514487

[2020-01-27 12:47:10.592337][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.899291373746209e-07, Discarded policy loss value: -1.6647408937964168

[2020-01-27 12:47:11.409660][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 163.86581065443178

[2020-01-27 12:47:11.414823][__main__.TRPOAgent.log][batch_info]: Batch #18, batch length: 4500

[2020-01-27 12:47:15.343056][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05712666 -0.         ... -0.04001938  0.05237835
 -0.01235897]

[2020-01-27 12:47:15.343452][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:47:15.772606][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.03134625  0.         ... -0.21614504 -0.75317809
  0.96932314], shape=(4547,), dtype=float64)

[2020-01-27 12:47:15.857280][__main__.TRPOAgent.log][linesearch]: improvement: -0.09662027220939251

[2020-01-27 12:47:15.885266][__main__.TRPOAgent.log][linesearch]: improvement: -0.06526349135029053

[2020-01-27 12:47:15.911686][__main__.TRPOAgent.log][linesearch]: improvement: -0.040620095257403066

[2020-01-27 12:47:15.945749][__main__.TRPOAgent.log][linesearch]: improvement: -0.024969729602803636

[2020-01-27 12:47:15.972808][__main__.TRPOAgent.log][linesearch]: improvement: -0.016624193971052414

[2020-01-27 12:47:16.000027][__main__.TRPOAgent.log][linesearch]: improvement: -0.012203454363054922

[2020-01-27 12:47:16.026761][__main__.TRPOAgent.log][linesearch]: improvement: -0.008805981645584104

[2020-01-27 12:47:16.054694][__main__.TRPOAgent.log][linesearch]: improvement: -0.0054815461948756194

[2020-01-27 12:47:16.084484][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030292884188405367

[2020-01-27 12:47:16.114556][__main__.TRPOAgent.log][linesearch]: improvement: -0.001850412085420694

[2020-01-27 12:47:16.115411][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 1.1692108674135144e-06, Discarded policy loss value: -0.7077875146137745

[2020-01-27 12:47:16.918273][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 145.7307030494131

[2020-01-27 12:47:16.920964][__main__.TRPOAgent.log][batch_info]: Batch #19, batch length: 1524

[2020-01-27 12:47:18.357819][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02836929 -0.         ...  0.22430877 -0.01601584
 -0.20829293]

[2020-01-27 12:47:18.358184][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:47:18.606129][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.05226027  0.         ...  0.31710292  0.08154218
 -0.3986451 ], shape=(4547,), dtype=float64)

[2020-01-27 12:47:18.658976][__main__.TRPOAgent.log][linesearch]: improvement: -0.0121107927138695

[2020-01-27 12:47:18.677570][__main__.TRPOAgent.log][linesearch]: improvement: -0.03041398095191372

[2020-01-27 12:47:18.677991][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 4.870970730051487

[2020-01-27 12:47:18.981968][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.3146193423016

[2020-01-27 12:47:18.982480][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:47:18.999746][__main__.TRPOAgent.log][learning]: Episode #3

[2020-01-27 12:47:19.000123][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 3000

[2020-01-27 12:47:19.028794][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 12:47:19.029486][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 12:47:19.029375][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 12:47:19.030506][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 12:47:27.397426][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 3000

[2020-01-27 12:47:27.397980][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 3000

[2020-01-27 12:47:27.399442][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 12:47:27.400147][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 12:47:27.400048][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 12:47:27.401403][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 12:47:35.530790][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 3000

[2020-01-27 12:47:35.585083][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 3000

[2020-01-27 12:47:35.585617][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 12:47:35.586157][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 12:47:35.586255][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 12:47:35.590037][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 12:47:42.922847][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 2530

[2020-01-27 12:47:43.555085][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 3000

[2020-01-27 12:47:43.555628][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 12:47:43.556289][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 12:47:43.556204][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 12:47:43.557196][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 12:47:52.753843][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 3000

[2020-01-27 12:47:52.765994][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 3000

[2020-01-27 12:47:52.766649][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 12:47:52.767182][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 12:47:52.767258][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 12:47:52.769832][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 12:48:01.498746][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 3000

[2020-01-27 12:48:01.676018][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 3000

[2020-01-27 12:48:01.676784][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 12:48:01.677829][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 12:48:01.677964][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 12:48:01.681697][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 12:48:07.637304][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 2016

[2020-01-27 12:48:08.760967][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 3000

[2020-01-27 12:48:08.761767][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 12:48:08.762734][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 12:48:08.763770][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 12:48:08.762615][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 12:48:17.641531][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 3000

[2020-01-27 12:48:17.708234][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 3000

[2020-01-27 12:48:17.708728][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 12:48:17.709405][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 12:48:17.709303][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 12:48:17.710972][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 12:48:19.284110][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 508

[2020-01-27 12:48:22.257536][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 3000

[2020-01-27 12:48:22.258037][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 12:48:22.259012][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 12:48:22.259178][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 12:48:22.263127][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 12:48:30.396651][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 3000

[2020-01-27 12:48:30.467091][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 3000

[2020-01-27 12:48:30.467584][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 12:48:30.468551][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 12:48:30.468336][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 12:48:30.469586][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 12:48:37.838373][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 2606

[2020-01-27 12:48:38.432297][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 3000

[2020-01-27 12:48:38.432817][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 12:48:38.433339][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 12:48:38.433406][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 12:48:38.435442][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 12:48:43.998980][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1707

[2020-01-27 12:48:45.836840][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 3000

[2020-01-27 12:48:45.837460][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 12:48:45.838785][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 12:48:45.838926][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 12:48:45.844113][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 12:48:54.863561][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 3000

[2020-01-27 12:48:54.940712][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 3000

[2020-01-27 12:48:54.941409][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 12:48:54.942163][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 12:48:54.942329][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 12:48:54.944704][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 12:49:04.117928][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 3000

[2020-01-27 12:49:04.155872][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 3000

[2020-01-27 12:49:04.156384][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 12:49:04.156889][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 12:49:04.156955][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 12:49:04.159032][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 12:49:08.019125][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1290

[2020-01-27 12:49:09.403190][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 2452

[2020-01-27 12:49:09.403621][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 12:49:09.404491][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 12:49:09.404581][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 12:49:09.408779][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 12:49:12.691658][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1121

[2020-01-27 12:49:15.058127][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 3000

[2020-01-27 12:49:15.058653][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 12:49:15.080149][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 12:49:16.674846][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 12:49:16.729476][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 12:49:16.731613][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 80230, Batch size: 4500, Number of batches: 18

[2020-01-27 12:49:16.732144][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 12:49:16.792689][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 12:49:21.166806][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01854362 -0.         ...  0.0973555  -0.22665914
  0.12930364]

[2020-01-27 12:49:21.167191][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:49:21.619224][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.3439585   0.         ... -0.15442279  0.7526353
 -0.59821251], shape=(4547,), dtype=float64)

[2020-01-27 12:49:21.705964][__main__.TRPOAgent.log][linesearch]: improvement: -0.2300085364816069

[2020-01-27 12:49:21.735248][__main__.TRPOAgent.log][linesearch]: improvement: -0.15310936475590653

[2020-01-27 12:49:21.762571][__main__.TRPOAgent.log][linesearch]: improvement: -0.0943505055137055

[2020-01-27 12:49:21.788129][__main__.TRPOAgent.log][linesearch]: improvement: -0.05863033686750807

[2020-01-27 12:49:21.819197][__main__.TRPOAgent.log][linesearch]: improvement: -0.037123707911049575

[2020-01-27 12:49:21.847148][__main__.TRPOAgent.log][linesearch]: improvement: -0.022994201799083402

[2020-01-27 12:49:21.874553][__main__.TRPOAgent.log][linesearch]: improvement: -0.013942305830165758

[2020-01-27 12:49:21.902724][__main__.TRPOAgent.log][linesearch]: improvement: -0.00839846722264026

[2020-01-27 12:49:21.929697][__main__.TRPOAgent.log][linesearch]: improvement: -0.005040651733190682

[2020-01-27 12:49:21.956222][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030215484694706696

[2020-01-27 12:49:21.956682][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.906497239331537e-07, Discarded policy loss value: -2.864802618990427

[2020-01-27 12:49:22.785303][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 204.0401177855064

[2020-01-27 12:49:22.791077][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 12:49:27.132770][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05500743 -0.         ... -0.08193016  0.12691429
 -0.04498412]

[2020-01-27 12:49:27.133192][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:49:27.554129][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.047527    0.         ...  0.16922881  0.12366933
 -0.29289814], shape=(4547,), dtype=float64)

[2020-01-27 12:49:27.637542][__main__.TRPOAgent.log][linesearch]: improvement: -0.07195094172512961

[2020-01-27 12:49:27.638009][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.088140823871369

[2020-01-27 12:49:28.419187][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.38665259297272

[2020-01-27 12:49:28.419593][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:49:28.428053][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 12:49:32.132822][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02134445 -0.         ... -0.17674459 -0.01097653
  0.18772112]

[2020-01-27 12:49:32.133275][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:49:32.523864][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.38756994  0.         ...  0.22681539 -0.13131949
 -0.09549589], shape=(4547,), dtype=float64)

[2020-01-27 12:49:32.635150][__main__.TRPOAgent.log][linesearch]: improvement: -0.10027641051414293

[2020-01-27 12:49:32.663576][__main__.TRPOAgent.log][linesearch]: improvement: -0.06604980368784608

[2020-01-27 12:49:32.691848][__main__.TRPOAgent.log][linesearch]: improvement: -0.04196616043218193

[2020-01-27 12:49:32.720073][__main__.TRPOAgent.log][linesearch]: improvement: -0.02615438012081084

[2020-01-27 12:49:32.747507][__main__.TRPOAgent.log][linesearch]: improvement: -0.015967708180387286

[2020-01-27 12:49:32.776299][__main__.TRPOAgent.log][linesearch]: improvement: -0.009763788533304396

[2020-01-27 12:49:32.807306][__main__.TRPOAgent.log][linesearch]: improvement: -0.005943192587240631

[2020-01-27 12:49:32.831242][__main__.TRPOAgent.log][linesearch]: improvement: -0.003579572529707642

[2020-01-27 12:49:32.859531][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021559018231542826

[2020-01-27 12:49:32.884013][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012944442725255634

[2020-01-27 12:49:32.884891][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.303743165875567e-07, Discarded policy loss value: -1.584867156544626

[2020-01-27 12:49:33.769899][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.83395105656874

[2020-01-27 12:49:33.775053][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 12:49:37.775556][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.04886755 -0.         ...  0.26516372 -0.18774568
 -0.07741803]

[2020-01-27 12:49:37.775954][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:49:38.187783][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.19848581  0.         ... -0.54491019  0.95450252
 -0.40959233], shape=(4547,), dtype=float64)

[2020-01-27 12:49:38.276785][__main__.TRPOAgent.log][linesearch]: improvement: -0.15809408478453957

[2020-01-27 12:49:38.277347][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.534699707034705

[2020-01-27 12:49:39.051335][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.8598933341117

[2020-01-27 12:49:39.051779][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:49:39.060108][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 12:49:43.313502][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02409163 -0.         ... -0.16968968  0.2073256
 -0.03763592]

[2020-01-27 12:49:43.313902][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:49:43.742120][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.29472653  0.         ... -0.34009618  0.10901481
  0.23108137], shape=(4547,), dtype=float64)

[2020-01-27 12:49:43.823812][__main__.TRPOAgent.log][linesearch]: improvement: -0.09347764767555722

[2020-01-27 12:49:43.850535][__main__.TRPOAgent.log][linesearch]: improvement: -0.05781920252625117

[2020-01-27 12:49:43.881719][__main__.TRPOAgent.log][linesearch]: improvement: -0.03479045222299204

[2020-01-27 12:49:43.909638][__main__.TRPOAgent.log][linesearch]: improvement: -0.02098583070783233

[2020-01-27 12:49:43.940400][__main__.TRPOAgent.log][linesearch]: improvement: -0.011920857879690772

[2020-01-27 12:49:43.971328][__main__.TRPOAgent.log][linesearch]: improvement: -0.007209153316819705

[2020-01-27 12:49:43.998186][__main__.TRPOAgent.log][linesearch]: improvement: -0.004318326822692953

[2020-01-27 12:49:44.025776][__main__.TRPOAgent.log][linesearch]: improvement: -0.002570122706639477

[2020-01-27 12:49:44.051489][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015475965925069168

[2020-01-27 12:49:44.080438][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009339886430250743

[2020-01-27 12:49:44.080888][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.512874304492429e-07, Discarded policy loss value: -3.591437800236525

[2020-01-27 12:49:45.021339][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.65621789525719

[2020-01-27 12:49:45.029516][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 12:49:49.505890][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01212576 -0.         ...  0.01285727 -0.03749367
  0.0246364 ]

[2020-01-27 12:49:49.506299][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:49:49.946325][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.90140225  0.         ... -0.54155982  0.48655241
  0.05500741], shape=(4547,), dtype=float64)

[2020-01-27 12:49:50.035386][__main__.TRPOAgent.log][linesearch]: improvement: -0.07374519397273538

[2020-01-27 12:49:50.064542][__main__.TRPOAgent.log][linesearch]: improvement: -0.06109140418787673

[2020-01-27 12:49:50.065003][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 3.3671709666546876

[2020-01-27 12:49:50.874074][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.29371645563083

[2020-01-27 12:49:50.874509][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:49:50.883271][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 12:49:55.297684][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00790361 -0.         ... -0.24791432  0.69013375
 -0.44221943]

[2020-01-27 12:49:55.298122][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:49:55.732175][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.0416048   0.         ... -0.02577026 -0.1985071
  0.22427736], shape=(4547,), dtype=float64)

[2020-01-27 12:49:55.819440][__main__.TRPOAgent.log][linesearch]: improvement: -0.31851874766997756

[2020-01-27 12:49:55.849906][__main__.TRPOAgent.log][linesearch]: improvement: -0.20159763527262076

[2020-01-27 12:49:55.879629][__main__.TRPOAgent.log][linesearch]: improvement: -0.12402098696352226

[2020-01-27 12:49:55.907698][__main__.TRPOAgent.log][linesearch]: improvement: -0.07549312331923197

[2020-01-27 12:49:55.934560][__main__.TRPOAgent.log][linesearch]: improvement: -0.04557969298154685

[2020-01-27 12:49:55.963564][__main__.TRPOAgent.log][linesearch]: improvement: -0.027450338139499486

[2020-01-27 12:49:55.991437][__main__.TRPOAgent.log][linesearch]: improvement: -0.016506212975013312

[2020-01-27 12:49:56.018375][__main__.TRPOAgent.log][linesearch]: improvement: -0.009933755103668673

[2020-01-27 12:49:56.046018][__main__.TRPOAgent.log][linesearch]: improvement: -0.0059660579378624234

[2020-01-27 12:49:56.073194][__main__.TRPOAgent.log][linesearch]: improvement: -0.0035779341026316303

[2020-01-27 12:49:56.073801][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.619737327539026e-07, Discarded policy loss value: -4.74408221306521

[2020-01-27 12:49:56.901897][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 200.75281988526558

[2020-01-27 12:49:56.907789][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 12:50:01.313325][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00424165 -0.         ...  0.0576821  -0.01639171
 -0.04129039]

[2020-01-27 12:50:01.313715][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:01.748841][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.0907934   0.         ...  0.38713275 -0.31875723
 -0.06837552], shape=(4547,), dtype=float64)

[2020-01-27 12:50:01.837494][__main__.TRPOAgent.log][linesearch]: improvement: -0.08422031450221645

[2020-01-27 12:50:01.866978][__main__.TRPOAgent.log][linesearch]: improvement: -0.06133524129715151

[2020-01-27 12:50:01.893621][__main__.TRPOAgent.log][linesearch]: improvement: -0.039371639345126

[2020-01-27 12:50:01.921046][__main__.TRPOAgent.log][linesearch]: improvement: -0.023585811844216042

[2020-01-27 12:50:01.949917][__main__.TRPOAgent.log][linesearch]: improvement: -0.014438855905250847

[2020-01-27 12:50:01.979176][__main__.TRPOAgent.log][linesearch]: improvement: -0.008816423046622779

[2020-01-27 12:50:02.006579][__main__.TRPOAgent.log][linesearch]: improvement: -0.00535722983317477

[2020-01-27 12:50:02.034849][__main__.TRPOAgent.log][linesearch]: improvement: -0.003228230037280122

[2020-01-27 12:50:02.063983][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019469332355457336

[2020-01-27 12:50:02.090925][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011730136418011128

[2020-01-27 12:50:02.091433][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.896460585554195e-07, Discarded policy loss value: -1.2710366360387542

[2020-01-27 12:50:02.910208][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 108.1520519544273

[2020-01-27 12:50:02.915430][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 12:50:07.328082][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01587119 -0.         ...  0.28675627 -0.56212941
  0.27537314]

[2020-01-27 12:50:07.328488][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:07.768387][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.14315636  0.         ... -0.38219609  0.50029853
 -0.11810244], shape=(4547,), dtype=float64)

[2020-01-27 12:50:07.857711][__main__.TRPOAgent.log][linesearch]: improvement: -0.31431922588436656

[2020-01-27 12:50:07.858164][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 5.4329637704277625

[2020-01-27 12:50:08.690731][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 290.3445797664365

[2020-01-27 12:50:08.691127][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:50:08.700546][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 12:50:13.209383][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00064759 -0.         ... -0.06585656  0.24876725
 -0.18291069]

[2020-01-27 12:50:13.209784][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:13.690951][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.07925128  0.         ... -0.05099586  0.06669631
 -0.01570046], shape=(4547,), dtype=float64)

[2020-01-27 12:50:13.778249][__main__.TRPOAgent.log][linesearch]: improvement: -0.12698621798400467

[2020-01-27 12:50:13.805533][__main__.TRPOAgent.log][linesearch]: improvement: -0.07568965539649763

[2020-01-27 12:50:13.834620][__main__.TRPOAgent.log][linesearch]: improvement: -0.04440877331710258

[2020-01-27 12:50:13.859871][__main__.TRPOAgent.log][linesearch]: improvement: -0.02629201562808081

[2020-01-27 12:50:13.888068][__main__.TRPOAgent.log][linesearch]: improvement: -0.015763450070476637

[2020-01-27 12:50:13.918056][__main__.TRPOAgent.log][linesearch]: improvement: -0.009531826048866598

[2020-01-27 12:50:13.944969][__main__.TRPOAgent.log][linesearch]: improvement: -0.005654010218073324

[2020-01-27 12:50:13.970473][__main__.TRPOAgent.log][linesearch]: improvement: -0.003365125119950463

[2020-01-27 12:50:13.999972][__main__.TRPOAgent.log][linesearch]: improvement: -0.0020667049027336404

[2020-01-27 12:50:14.029147][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012078651931357598

[2020-01-27 12:50:14.029752][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.667977098325576e-07, Discarded policy loss value: -6.877979529656517

[2020-01-27 12:50:14.838093][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.1942690367022

[2020-01-27 12:50:14.843557][__main__.TRPOAgent.log][batch_info]: Batch #10, batch length: 4500

[2020-01-27 12:50:19.306150][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02206907 -0.         ...  0.10930192 -0.03467157
 -0.07463035]

[2020-01-27 12:50:19.306560][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:19.741389][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.30279104  0.         ... -0.34936545  0.42206517
 -0.07269973], shape=(4547,), dtype=float64)

[2020-01-27 12:50:19.827194][__main__.TRPOAgent.log][linesearch]: improvement: -0.09562581667525338

[2020-01-27 12:50:19.856386][__main__.TRPOAgent.log][linesearch]: improvement: -0.05900266084630634

[2020-01-27 12:50:19.856831][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.5666798607949018

[2020-01-27 12:50:20.660952][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 200.38457255396747

[2020-01-27 12:50:20.661355][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:50:20.670044][__main__.TRPOAgent.log][batch_info]: Batch #11, batch length: 4500

[2020-01-27 12:50:25.093555][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01238483 -0.         ... -0.03016194 -0.17377142
  0.20393336]

[2020-01-27 12:50:25.093965][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:25.525851][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.03896595  0.         ... -0.06276941 -0.14899737
  0.21176678], shape=(4547,), dtype=float64)

[2020-01-27 12:50:25.614088][__main__.TRPOAgent.log][linesearch]: improvement: -0.1996100331931977

[2020-01-27 12:50:25.651513][__main__.TRPOAgent.log][linesearch]: improvement: -0.10608910746047417

[2020-01-27 12:50:25.675701][__main__.TRPOAgent.log][linesearch]: improvement: -0.0587450705503092

[2020-01-27 12:50:25.703641][__main__.TRPOAgent.log][linesearch]: improvement: -0.03350997775786391

[2020-01-27 12:50:25.728973][__main__.TRPOAgent.log][linesearch]: improvement: -0.019495323907766537

[2020-01-27 12:50:25.758084][__main__.TRPOAgent.log][linesearch]: improvement: -0.011486553870550975

[2020-01-27 12:50:25.786804][__main__.TRPOAgent.log][linesearch]: improvement: -0.006818536906020745

[2020-01-27 12:50:25.811810][__main__.TRPOAgent.log][linesearch]: improvement: -0.004063917737429867

[2020-01-27 12:50:25.837032][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024288298980261747

[2020-01-27 12:50:25.868079][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014537660568770816

[2020-01-27 12:50:25.868530][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.460042165535842e-07, Discarded policy loss value: -0.7294718034629336

[2020-01-27 12:50:26.697910][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 203.9674298406109

[2020-01-27 12:50:26.704578][__main__.TRPOAgent.log][batch_info]: Batch #12, batch length: 4500

[2020-01-27 12:50:31.143102][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00680535 -0.         ...  0.09668945  0.17343678
 -0.27012623]

[2020-01-27 12:50:31.143506][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:31.576859][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.12488095  0.         ... -0.0033316  -0.31333598
  0.31666758], shape=(4547,), dtype=float64)

[2020-01-27 12:50:31.669173][__main__.TRPOAgent.log][linesearch]: improvement: -0.21242787378339756

[2020-01-27 12:50:31.695585][__main__.TRPOAgent.log][linesearch]: improvement: -0.11872071898954095

[2020-01-27 12:50:31.723315][__main__.TRPOAgent.log][linesearch]: improvement: -0.06348694488587903

[2020-01-27 12:50:31.752393][__main__.TRPOAgent.log][linesearch]: improvement: -0.03876728979406652

[2020-01-27 12:50:31.779529][__main__.TRPOAgent.log][linesearch]: improvement: -0.021852138037596447

[2020-01-27 12:50:31.807682][__main__.TRPOAgent.log][linesearch]: improvement: -0.012268908594847527

[2020-01-27 12:50:31.836393][__main__.TRPOAgent.log][linesearch]: improvement: -0.006851407177803637

[2020-01-27 12:50:31.862030][__main__.TRPOAgent.log][linesearch]: improvement: -0.004066012400818941

[2020-01-27 12:50:31.890321][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024571716315602288

[2020-01-27 12:50:31.917444][__main__.TRPOAgent.log][linesearch]: improvement: -0.001499842824224995

[2020-01-27 12:50:31.917973][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.439950436820611e-07, Discarded policy loss value: -2.4950419028502906

[2020-01-27 12:50:32.728788][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.20986303304475

[2020-01-27 12:50:32.734802][__main__.TRPOAgent.log][batch_info]: Batch #13, batch length: 4500

[2020-01-27 12:50:36.989530][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00895755 -0.         ... -0.03715592 -0.17891055
  0.21606648]

[2020-01-27 12:50:36.989944][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:37.433853][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.17150659  0.         ...  0.13434902  0.34450729
 -0.47885632], shape=(4547,), dtype=float64)

[2020-01-27 12:50:37.521934][__main__.TRPOAgent.log][linesearch]: improvement: -0.05587166259324183

[2020-01-27 12:50:37.522430][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 2.630773454538654

[2020-01-27 12:50:38.346562][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 202.34241343407822

[2020-01-27 12:50:38.346991][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:50:38.355856][__main__.TRPOAgent.log][batch_info]: Batch #14, batch length: 4500

[2020-01-27 12:50:42.449753][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.0571326  -0.         ... -0.36204664  0.86634941
 -0.50430276]

[2020-01-27 12:50:42.450128][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:42.884092][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.08122309  0.         ... -0.4527956  -0.70135098
  1.15414658], shape=(4547,), dtype=float64)

[2020-01-27 12:50:42.971736][__main__.TRPOAgent.log][linesearch]: improvement: -0.45282483268532747

[2020-01-27 12:50:42.997974][__main__.TRPOAgent.log][linesearch]: improvement: -0.23782329723166384

[2020-01-27 12:50:43.025123][__main__.TRPOAgent.log][linesearch]: improvement: -0.11715447062929218

[2020-01-27 12:50:43.051816][__main__.TRPOAgent.log][linesearch]: improvement: -0.06116369224553697

[2020-01-27 12:50:43.078919][__main__.TRPOAgent.log][linesearch]: improvement: -0.03435651406011875

[2020-01-27 12:50:43.107436][__main__.TRPOAgent.log][linesearch]: improvement: -0.02073283022875394

[2020-01-27 12:50:43.131756][__main__.TRPOAgent.log][linesearch]: improvement: -0.012594066154019146

[2020-01-27 12:50:43.156837][__main__.TRPOAgent.log][linesearch]: improvement: -0.007596720833539017

[2020-01-27 12:50:43.180415][__main__.TRPOAgent.log][linesearch]: improvement: -0.004579077983366986

[2020-01-27 12:50:43.208219][__main__.TRPOAgent.log][linesearch]: improvement: -0.0027493536238987204

[2020-01-27 12:50:43.208782][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.44049472983402e-07, Discarded policy loss value: -5.721029535392395

[2020-01-27 12:50:43.986953][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.44868498937441

[2020-01-27 12:50:43.992009][__main__.TRPOAgent.log][batch_info]: Batch #15, batch length: 4500

[2020-01-27 12:50:48.458173][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00681373 -0.         ... -0.08543217 -0.03322822
  0.1186604 ]

[2020-01-27 12:50:48.458568][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:48.860479][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.0130378   0.         ...  0.00870211  0.12141522
 -0.13011733], shape=(4547,), dtype=float64)

[2020-01-27 12:50:48.941964][__main__.TRPOAgent.log][linesearch]: improvement: -0.02623478215234143

[2020-01-27 12:50:48.942723][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.37361293439095344

[2020-01-27 12:50:49.749504][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.11222909248002

[2020-01-27 12:50:49.749929][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:50:49.758564][__main__.TRPOAgent.log][batch_info]: Batch #16, batch length: 4500

[2020-01-27 12:50:53.705502][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00097082 -0.         ...  0.02025338  0.01705332
 -0.03730669]

[2020-01-27 12:50:53.705907][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:54.120624][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.36019146  0.         ...  0.09576249 -0.14935009
  0.0535876 ], shape=(4547,), dtype=float64)

[2020-01-27 12:50:54.206383][__main__.TRPOAgent.log][linesearch]: improvement: -0.010838127784436269

[2020-01-27 12:50:54.235728][__main__.TRPOAgent.log][linesearch]: improvement: -0.01596432263742642

[2020-01-27 12:50:54.263983][__main__.TRPOAgent.log][linesearch]: improvement: -0.009753589159086973

[2020-01-27 12:50:54.293199][__main__.TRPOAgent.log][linesearch]: improvement: -0.004959540161742937

[2020-01-27 12:50:54.321006][__main__.TRPOAgent.log][linesearch]: improvement: -0.003052612587808201

[2020-01-27 12:50:54.351060][__main__.TRPOAgent.log][linesearch]: improvement: -0.002283844582888328

[2020-01-27 12:50:54.382077][__main__.TRPOAgent.log][linesearch]: improvement: -0.002593725132987168

[2020-01-27 12:50:54.411277][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023830539772276005

[2020-01-27 12:50:54.436239][__main__.TRPOAgent.log][linesearch]: improvement: -0.00190487979913756

[2020-01-27 12:50:54.463984][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012596815934230454

[2020-01-27 12:50:54.464466][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.226238455989631e-07, Discarded policy loss value: -0.9679477668398859

[2020-01-27 12:50:55.299398][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.71396524990526

[2020-01-27 12:50:55.304145][__main__.TRPOAgent.log][batch_info]: Batch #17, batch length: 3730

[2020-01-27 12:50:59.301666][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02473987 -0.         ... -0.19826512  0.20961202
 -0.01134691]

[2020-01-27 12:50:59.302108][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:50:59.737826][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.38507974  0.         ... -0.20247504  0.05557132
  0.14690372], shape=(4547,), dtype=float64)

[2020-01-27 12:50:59.830141][__main__.TRPOAgent.log][linesearch]: improvement: -0.035741003756451395

[2020-01-27 12:50:59.861103][__main__.TRPOAgent.log][linesearch]: improvement: -0.05621317233313272

[2020-01-27 12:50:59.861614][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.1943063920391372

[2020-01-27 12:51:00.648795][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 239.63194463633266

[2020-01-27 12:51:00.649383][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:51:00.673746][__main__.TRPOAgent.log][learning]: Episode #4

[2020-01-27 12:51:00.674279][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 3000

[2020-01-27 12:51:00.706944][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 12:51:00.707721][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 12:51:00.707552][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 12:51:00.708761][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 12:51:09.247511][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 3000

[2020-01-27 12:51:09.403004][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 3000

[2020-01-27 12:51:09.403542][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 12:51:09.404759][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 12:51:09.404660][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 12:51:09.406296][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 12:51:18.003470][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 3000

[2020-01-27 12:51:18.057741][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 3000

[2020-01-27 12:51:18.058316][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 12:51:18.058891][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 12:51:18.058959][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 12:51:18.061919][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 12:51:26.686888][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 3000

[2020-01-27 12:51:26.725886][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 3000

[2020-01-27 12:51:26.726431][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 12:51:26.726937][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 12:51:26.727096][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 12:51:26.729903][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 12:51:30.558177][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1356

[2020-01-27 12:51:32.397701][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 2804

[2020-01-27 12:51:32.398292][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 12:51:32.399121][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 12:51:32.399242][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 12:51:32.405017][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 12:51:41.000080][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 3000

[2020-01-27 12:51:41.040759][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 3000

[2020-01-27 12:51:41.041323][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 12:51:41.041868][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 12:51:41.042067][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 12:51:41.044152][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 12:51:49.731357][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 3000

[2020-01-27 12:51:49.835584][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 3000

[2020-01-27 12:51:49.836189][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 12:51:49.837139][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 12:51:49.837031][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 12:51:49.838536][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 12:51:58.400099][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 3000

[2020-01-27 12:51:58.511375][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 3000

[2020-01-27 12:51:58.512075][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 12:51:58.512767][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 12:51:58.512865][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 12:51:58.517061][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 12:52:06.905337][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 3000

[2020-01-27 12:52:06.956854][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 3000

[2020-01-27 12:52:06.958087][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 12:52:06.959154][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 12:52:06.959358][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 12:52:06.964672][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 12:52:15.275587][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 3000

[2020-01-27 12:52:15.345714][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 3000

[2020-01-27 12:52:15.346295][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 12:52:15.347094][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 12:52:15.347165][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 12:52:15.349729][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 12:52:19.367872][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1561

[2020-01-27 12:52:21.220909][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 3000

[2020-01-27 12:52:21.221478][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 12:52:21.222103][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 12:52:21.222508][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 12:52:21.225263][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 12:52:30.264117][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 3000

[2020-01-27 12:52:30.321003][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 3000

[2020-01-27 12:52:30.321659][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 12:52:30.322224][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 12:52:30.322429][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 12:52:30.324493][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 12:52:38.935265][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 3000

[2020-01-27 12:52:39.004916][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 3000

[2020-01-27 12:52:39.005593][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 12:52:39.006178][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 12:52:39.006248][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 12:52:39.008188][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 12:52:47.580669][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 3000

[2020-01-27 12:52:47.692732][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 3000

[2020-01-27 12:52:47.693295][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 12:52:47.693882][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 12:52:47.693974][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 12:52:47.695960][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 12:52:56.406152][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 3000

[2020-01-27 12:52:56.435389][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 3000

[2020-01-27 12:52:56.435981][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 12:52:56.437683][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 12:52:56.438408][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 12:52:56.444252][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 12:53:05.183466][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 3000

[2020-01-27 12:53:05.191981][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 3000

[2020-01-27 12:53:05.192996][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 12:53:05.220686][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 12:53:07.204382][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 12:53:07.260671][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 12:53:07.262236][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 86721, Batch size: 4500, Number of batches: 20

[2020-01-27 12:53:07.262657][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 12:53:07.332617][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 12:53:11.953991][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00607091 -0.         ... -0.07904932  0.21735253
 -0.13830321]

[2020-01-27 12:53:11.954381][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:12.387657][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.09885246  0.         ... -0.18453934  0.24438971
 -0.05985037], shape=(4547,), dtype=float64)

[2020-01-27 12:53:12.484693][__main__.TRPOAgent.log][linesearch]: improvement: 0.20862510857065342

[2020-01-27 12:53:12.515232][__main__.TRPOAgent.log][linesearch]: improvement: 0.04762753537193287

[2020-01-27 12:53:12.545153][__main__.TRPOAgent.log][linesearch]: improvement: 0.0002735290094308951

[2020-01-27 12:53:12.572195][__main__.TRPOAgent.log][linesearch]: improvement: -0.009393545763736633

[2020-01-27 12:53:12.601780][__main__.TRPOAgent.log][linesearch]: improvement: -0.008345551482388913

[2020-01-27 12:53:12.636435][__main__.TRPOAgent.log][linesearch]: improvement: -0.006618188993888552

[2020-01-27 12:53:12.667010][__main__.TRPOAgent.log][linesearch]: improvement: -0.0048213995380876895

[2020-01-27 12:53:12.697884][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032538960595003275

[2020-01-27 12:53:12.750251][__main__.TRPOAgent.log][linesearch]: improvement: -0.002068037406971257

[2020-01-27 12:53:12.782731][__main__.TRPOAgent.log][linesearch]: improvement: -0.001274026089443936

[2020-01-27 12:53:12.783230][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.876880320542055e-07, Discarded policy loss value: -4.566668593758548

[2020-01-27 12:53:13.648721][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.66764098295576

[2020-01-27 12:53:13.657350][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 12:53:17.981534][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00225853 -0.         ...  0.20864755 -0.17301761
 -0.03562993]

[2020-01-27 12:53:17.981939][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:18.433294][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.13830207  0.         ... -0.02484257  0.05097716
 -0.02613459], shape=(4547,), dtype=float64)

[2020-01-27 12:53:18.531191][__main__.TRPOAgent.log][linesearch]: improvement: 0.24712836640322378

[2020-01-27 12:53:18.564640][__main__.TRPOAgent.log][linesearch]: improvement: 0.07673335378070201

[2020-01-27 12:53:18.593426][__main__.TRPOAgent.log][linesearch]: improvement: -0.036473865456434496

[2020-01-27 12:53:18.622552][__main__.TRPOAgent.log][linesearch]: improvement: -0.04596882855146056

[2020-01-27 12:53:18.623067][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 3.1768370252500597

[2020-01-27 12:53:19.457346][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.2470447232138

[2020-01-27 12:53:19.457768][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:53:19.466633][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 12:53:23.804310][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00618717 -0.         ... -0.17204163  0.1569397
  0.01510193]

[2020-01-27 12:53:23.804702][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:24.223004][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.2135095   0.         ... -0.01574913  0.06785164
 -0.05210251], shape=(4547,), dtype=float64)

[2020-01-27 12:53:24.315348][__main__.TRPOAgent.log][linesearch]: improvement: -0.07526977838844229

[2020-01-27 12:53:24.344348][__main__.TRPOAgent.log][linesearch]: improvement: -0.04652262362513271

[2020-01-27 12:53:24.369840][__main__.TRPOAgent.log][linesearch]: improvement: -0.029659263368692956

[2020-01-27 12:53:24.401688][__main__.TRPOAgent.log][linesearch]: improvement: -0.018444861457964423

[2020-01-27 12:53:24.432521][__main__.TRPOAgent.log][linesearch]: improvement: -0.011196579517899763

[2020-01-27 12:53:24.460989][__main__.TRPOAgent.log][linesearch]: improvement: -0.006807702077929134

[2020-01-27 12:53:24.505969][__main__.TRPOAgent.log][linesearch]: improvement: -0.00412504280322723

[2020-01-27 12:53:24.534383][__main__.TRPOAgent.log][linesearch]: improvement: -0.002481721258676073

[2020-01-27 12:53:24.564248][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014929610253890946

[2020-01-27 12:53:24.591298][__main__.TRPOAgent.log][linesearch]: improvement: -0.000900364130143827

[2020-01-27 12:53:24.591816][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.303317601497718e-07, Discarded policy loss value: -0.6900488465511418

[2020-01-27 12:53:25.384045][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.70318424290343

[2020-01-27 12:53:25.389746][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 12:53:29.472769][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00856508 -0.         ...  0.2239209  -0.15650819
 -0.06741271]

[2020-01-27 12:53:29.473172][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:29.918022][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.09142958  0.         ...  0.07864167 -0.07283598
 -0.00580569], shape=(4547,), dtype=float64)

[2020-01-27 12:53:30.004511][__main__.TRPOAgent.log][linesearch]: improvement: -0.12083272366815745

[2020-01-27 12:53:30.031764][__main__.TRPOAgent.log][linesearch]: improvement: -0.07080576628419433

[2020-01-27 12:53:30.032454][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.032625980737324

[2020-01-27 12:53:30.958984][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 203.13797659219463

[2020-01-27 12:53:30.959374][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:53:30.970929][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 12:53:35.263264][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01400246 -0.         ...  0.00529273  0.12778152
 -0.13307425]

[2020-01-27 12:53:35.263687][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:35.724165][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.15312598  0.         ...  0.0068134  -0.03805413
  0.03124073], shape=(4547,), dtype=float64)

[2020-01-27 12:53:35.822044][__main__.TRPOAgent.log][linesearch]: improvement: -0.04787931281966179

[2020-01-27 12:53:35.854974][__main__.TRPOAgent.log][linesearch]: improvement: -0.03162400664267501

[2020-01-27 12:53:35.887641][__main__.TRPOAgent.log][linesearch]: improvement: -0.020334280118296466

[2020-01-27 12:53:35.920200][__main__.TRPOAgent.log][linesearch]: improvement: -0.01273282359083916

[2020-01-27 12:53:35.951446][__main__.TRPOAgent.log][linesearch]: improvement: -0.007938253621189995

[2020-01-27 12:53:35.981669][__main__.TRPOAgent.log][linesearch]: improvement: -0.004883523462049566

[2020-01-27 12:53:36.008820][__main__.TRPOAgent.log][linesearch]: improvement: -0.002962017925133953

[2020-01-27 12:53:36.043264][__main__.TRPOAgent.log][linesearch]: improvement: -0.00178798974886335

[2020-01-27 12:53:36.074047][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010760791681083126

[2020-01-27 12:53:36.106809][__main__.TRPOAgent.log][linesearch]: improvement: -0.0006477761033618079

[2020-01-27 12:53:36.107279][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.528502915484587e-07, Discarded policy loss value: -0.36883514322783717

[2020-01-27 12:53:36.944061][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.17480094274431

[2020-01-27 12:53:36.950170][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 12:53:41.455655][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01409589 -0.         ... -0.09819503  0.04109377
  0.05710126]

[2020-01-27 12:53:41.456060][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:41.905393][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.38276913  0.         ...  0.06415988 -0.04571764
 -0.01844224], shape=(4547,), dtype=float64)

[2020-01-27 12:53:41.995546][__main__.TRPOAgent.log][linesearch]: improvement: -0.024069699377885723

[2020-01-27 12:53:42.028589][__main__.TRPOAgent.log][linesearch]: improvement: -0.03314433644611681

[2020-01-27 12:53:42.060026][__main__.TRPOAgent.log][linesearch]: improvement: -0.029837600897951955

[2020-01-27 12:53:42.060482][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 2.0755060007849977

[2020-01-27 12:53:42.879616][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.05922637294333

[2020-01-27 12:53:42.880024][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:53:42.889605][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 12:53:47.324461][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01478003 -0.         ...  0.12415913 -0.14487345
  0.02071432]

[2020-01-27 12:53:47.324857][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:47.766962][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.63118976  0.         ...  0.02513621 -0.05561707
  0.03048086], shape=(4547,), dtype=float64)

[2020-01-27 12:53:47.863309][__main__.TRPOAgent.log][linesearch]: improvement: -0.13202287437674665

[2020-01-27 12:53:47.894233][__main__.TRPOAgent.log][linesearch]: improvement: -0.06510757679917578

[2020-01-27 12:53:47.924861][__main__.TRPOAgent.log][linesearch]: improvement: -0.0354059482705944

[2020-01-27 12:53:47.954729][__main__.TRPOAgent.log][linesearch]: improvement: -0.020132392951023537

[2020-01-27 12:53:47.986709][__main__.TRPOAgent.log][linesearch]: improvement: -0.011706367759711789

[2020-01-27 12:53:48.014703][__main__.TRPOAgent.log][linesearch]: improvement: -0.006894608341479369

[2020-01-27 12:53:48.044396][__main__.TRPOAgent.log][linesearch]: improvement: -0.00409185299717163

[2020-01-27 12:53:48.077968][__main__.TRPOAgent.log][linesearch]: improvement: -0.002439159256697021

[2020-01-27 12:53:48.109438][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014569872993397404

[2020-01-27 12:53:48.139680][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008714626352206434

[2020-01-27 12:53:48.140181][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.719223378513346e-07, Discarded policy loss value: -1.1922983624196561

[2020-01-27 12:53:48.971375][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.01511908156948

[2020-01-27 12:53:48.978022][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 12:53:53.441566][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01641593 -0.         ...  0.08141628 -0.1660836
  0.08466732]

[2020-01-27 12:53:53.441965][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:53.886937][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.24166755  0.         ... -0.11855038  0.02199631
  0.09655407], shape=(4547,), dtype=float64)

[2020-01-27 12:53:53.977519][__main__.TRPOAgent.log][linesearch]: improvement: -0.1345845496484519

[2020-01-27 12:53:54.009490][__main__.TRPOAgent.log][linesearch]: improvement: -0.07547000569593809

[2020-01-27 12:53:54.037661][__main__.TRPOAgent.log][linesearch]: improvement: -0.041651560394089016

[2020-01-27 12:53:54.038447][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 1.8253728171032289

[2020-01-27 12:53:54.860497][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.15880374325982

[2020-01-27 12:53:54.860895][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:53:54.869147][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 12:53:59.375250][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02128033 -0.         ...  0.07818029  0.06543893
 -0.14361923]

[2020-01-27 12:53:59.375640][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:53:59.822167][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.20302514  0.         ...  0.05004258 -0.14002179
  0.08997921], shape=(4547,), dtype=float64)

[2020-01-27 12:53:59.914318][__main__.TRPOAgent.log][linesearch]: improvement: -0.02622614202598128

[2020-01-27 12:53:59.948303][__main__.TRPOAgent.log][linesearch]: improvement: -0.01981554475370717

[2020-01-27 12:53:59.978970][__main__.TRPOAgent.log][linesearch]: improvement: -0.015326967652039558

[2020-01-27 12:54:00.010357][__main__.TRPOAgent.log][linesearch]: improvement: -0.01085927358086658

[2020-01-27 12:54:00.038916][__main__.TRPOAgent.log][linesearch]: improvement: -0.007136635911261013

[2020-01-27 12:54:00.075587][__main__.TRPOAgent.log][linesearch]: improvement: -0.0044109578132898175

[2020-01-27 12:54:00.101842][__main__.TRPOAgent.log][linesearch]: improvement: -0.0027065924567586386

[2020-01-27 12:54:00.130299][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016726927128583036

[2020-01-27 12:54:00.164103][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010246468629480088

[2020-01-27 12:54:00.196162][__main__.TRPOAgent.log][linesearch]: improvement: -0.0006191168290881066

[2020-01-27 12:54:00.196648][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.168961584319613e-07, Discarded policy loss value: -1.7304235631764775

[2020-01-27 12:54:01.031027][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 131.9777089015743

[2020-01-27 12:54:01.036702][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 12:54:05.591029][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03289522 -0.         ...  0.1259747   0.03431927
 -0.16029397]

[2020-01-27 12:54:05.591416][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:06.041460][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.03848821  0.         ... -0.12504814  0.04824277
  0.07680537], shape=(4547,), dtype=float64)

[2020-01-27 12:54:06.130053][__main__.TRPOAgent.log][linesearch]: improvement: -0.15150590293770927

[2020-01-27 12:54:06.162911][__main__.TRPOAgent.log][linesearch]: improvement: -0.07846131782213162

[2020-01-27 12:54:06.194865][__main__.TRPOAgent.log][linesearch]: improvement: -0.04280979488979808

[2020-01-27 12:54:06.220788][__main__.TRPOAgent.log][linesearch]: improvement: -0.024322317713157737

[2020-01-27 12:54:06.250147][__main__.TRPOAgent.log][linesearch]: improvement: -0.01415992728431803

[2020-01-27 12:54:06.282153][__main__.TRPOAgent.log][linesearch]: improvement: -0.008386214101105072

[2020-01-27 12:54:06.311601][__main__.TRPOAgent.log][linesearch]: improvement: -0.005004604627927733

[2020-01-27 12:54:06.342712][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030018330874095334

[2020-01-27 12:54:06.372715][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017981303407963445

[2020-01-27 12:54:06.405877][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010751579472527695

[2020-01-27 12:54:06.406628][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.499280104947298e-07, Discarded policy loss value: -2.2975757403148833

[2020-01-27 12:54:07.231629][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 178.5831818534048

[2020-01-27 12:54:07.239598][__main__.TRPOAgent.log][batch_info]: Batch #10, batch length: 4500

[2020-01-27 12:54:11.713148][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01272876 -0.         ...  0.07455156 -0.01125386
 -0.0632977 ]

[2020-01-27 12:54:11.713550][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:12.160709][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.01530398  0.         ... -0.07120956 -0.05165017
  0.12285973], shape=(4547,), dtype=float64)

[2020-01-27 12:54:12.250639][__main__.TRPOAgent.log][linesearch]: improvement: 0.02974857456699223

[2020-01-27 12:54:12.284687][__main__.TRPOAgent.log][linesearch]: improvement: 0.015559344654744178

[2020-01-27 12:54:12.314296][__main__.TRPOAgent.log][linesearch]: improvement: 0.0065143486602839695

[2020-01-27 12:54:12.314763][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.14156114029541328

[2020-01-27 12:54:13.127375][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 130.51394745244656

[2020-01-27 12:54:13.127767][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:54:13.135101][__main__.TRPOAgent.log][batch_info]: Batch #11, batch length: 4500

[2020-01-27 12:54:17.619431][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01102924 -0.         ...  0.07446659  0.06826685
 -0.14273344]

[2020-01-27 12:54:17.619848][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:18.075061][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.31997292  0.         ... -0.00555631  0.02140118
 -0.01584487], shape=(4547,), dtype=float64)

[2020-01-27 12:54:18.167680][__main__.TRPOAgent.log][linesearch]: improvement: -0.052351497400714486

[2020-01-27 12:54:18.168123][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.3798416778215618

[2020-01-27 12:54:18.983347][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 177.39003772307353

[2020-01-27 12:54:18.983753][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:54:18.990987][__main__.TRPOAgent.log][batch_info]: Batch #12, batch length: 4500

[2020-01-27 12:54:23.547577][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00948075 -0.         ...  0.14300161 -0.20052056
  0.05751894]

[2020-01-27 12:54:23.547974][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:23.993175][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.59004675  0.         ... -0.00083387  0.09951642
 -0.09868255], shape=(4547,), dtype=float64)

[2020-01-27 12:54:24.085249][__main__.TRPOAgent.log][linesearch]: improvement: -0.0941650638256053

[2020-01-27 12:54:24.116120][__main__.TRPOAgent.log][linesearch]: improvement: -0.05986515345283849

[2020-01-27 12:54:24.116560][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.0558568834181123

[2020-01-27 12:54:24.943289][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 131.80636746571116

[2020-01-27 12:54:24.943727][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:54:24.951977][__main__.TRPOAgent.log][batch_info]: Batch #13, batch length: 4500

[2020-01-27 12:54:29.352495][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.04102227 -0.         ...  0.05461071  0.06236162
 -0.11697234]

[2020-01-27 12:54:29.352900][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:29.808716][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.41037985  0.         ... -0.12678001  0.1371391
 -0.01035908], shape=(4547,), dtype=float64)

[2020-01-27 12:54:29.900294][__main__.TRPOAgent.log][linesearch]: improvement: -0.0804639729201907

[2020-01-27 12:54:29.926137][__main__.TRPOAgent.log][linesearch]: improvement: -0.04774308996461596

[2020-01-27 12:54:29.954637][__main__.TRPOAgent.log][linesearch]: improvement: -0.03131784375408164

[2020-01-27 12:54:29.984706][__main__.TRPOAgent.log][linesearch]: improvement: -0.019503786371675935

[2020-01-27 12:54:30.013229][__main__.TRPOAgent.log][linesearch]: improvement: -0.011570793378224864

[2020-01-27 12:54:30.040763][__main__.TRPOAgent.log][linesearch]: improvement: -0.006824332441067615

[2020-01-27 12:54:30.070603][__main__.TRPOAgent.log][linesearch]: improvement: -0.004207373301953865

[2020-01-27 12:54:30.101134][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025579152795659477

[2020-01-27 12:54:30.128744][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015581026064036552

[2020-01-27 12:54:30.156358][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009283453628874394

[2020-01-27 12:54:30.156814][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.785580275047661e-07, Discarded policy loss value: -0.689040680372021

[2020-01-27 12:54:30.962367][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 177.57566019694363

[2020-01-27 12:54:30.968798][__main__.TRPOAgent.log][batch_info]: Batch #14, batch length: 4500

[2020-01-27 12:54:35.422683][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03499601 -0.         ... -0.0967114   0.06774073
  0.02897067]

[2020-01-27 12:54:35.423081][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:35.868164][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.33108915  0.         ... -0.19907626  0.0923526
  0.10672366], shape=(4547,), dtype=float64)

[2020-01-27 12:54:35.962835][__main__.TRPOAgent.log][linesearch]: improvement: -0.07346986482585716

[2020-01-27 12:54:35.992517][__main__.TRPOAgent.log][linesearch]: improvement: -0.04864033020156189

[2020-01-27 12:54:35.992953][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.3496640646626754

[2020-01-27 12:54:36.806058][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.8106029580505

[2020-01-27 12:54:36.806472][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:54:36.815199][__main__.TRPOAgent.log][batch_info]: Batch #15, batch length: 4500

[2020-01-27 12:54:41.323653][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03180092 -0.         ... -0.14885159 -0.01451638
  0.16336797]

[2020-01-27 12:54:41.324080][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:41.770626][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.48157348  0.         ... -0.09002211 -0.18544727
  0.27546937], shape=(4547,), dtype=float64)

[2020-01-27 12:54:41.866749][__main__.TRPOAgent.log][linesearch]: improvement: 0.00013689018404194897

[2020-01-27 12:54:41.895471][__main__.TRPOAgent.log][linesearch]: improvement: -0.02282352066306803

[2020-01-27 12:54:41.922925][__main__.TRPOAgent.log][linesearch]: improvement: -0.021640716304073826

[2020-01-27 12:54:41.953139][__main__.TRPOAgent.log][linesearch]: improvement: -0.015440436606946717

[2020-01-27 12:54:41.978423][__main__.TRPOAgent.log][linesearch]: improvement: -0.010051000435254753

[2020-01-27 12:54:42.007849][__main__.TRPOAgent.log][linesearch]: improvement: -0.006291531732371292

[2020-01-27 12:54:42.036483][__main__.TRPOAgent.log][linesearch]: improvement: -0.003863838886590143

[2020-01-27 12:54:42.061599][__main__.TRPOAgent.log][linesearch]: improvement: -0.002349210325032436

[2020-01-27 12:54:42.091064][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014204119103072843

[2020-01-27 12:54:42.122925][__main__.TRPOAgent.log][linesearch]: improvement: -0.000856113721820817

[2020-01-27 12:54:42.123382][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.531131955965972e-07, Discarded policy loss value: -0.7365375572623709

[2020-01-27 12:54:42.937613][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.0748725762845

[2020-01-27 12:54:42.945527][__main__.TRPOAgent.log][batch_info]: Batch #16, batch length: 4500

[2020-01-27 12:54:47.371260][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.05962538 -0.         ...  0.01407917  0.09981985
 -0.11389902]

[2020-01-27 12:54:47.371650][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:47.814492][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.8311434   0.         ... -0.17429654  0.07312154
  0.10117501], shape=(4547,), dtype=float64)

[2020-01-27 12:54:47.905760][__main__.TRPOAgent.log][linesearch]: improvement: -0.03991211128133232

[2020-01-27 12:54:47.935960][__main__.TRPOAgent.log][linesearch]: improvement: -0.04639807801214202

[2020-01-27 12:54:47.963457][__main__.TRPOAgent.log][linesearch]: improvement: -0.036777255290148414

[2020-01-27 12:54:47.991538][__main__.TRPOAgent.log][linesearch]: improvement: -0.02564041529066552

[2020-01-27 12:54:48.022836][__main__.TRPOAgent.log][linesearch]: improvement: -0.016772097000262054

[2020-01-27 12:54:48.052684][__main__.TRPOAgent.log][linesearch]: improvement: -0.010587977748429012

[2020-01-27 12:54:48.078125][__main__.TRPOAgent.log][linesearch]: improvement: -0.006547515535605175

[2020-01-27 12:54:48.106572][__main__.TRPOAgent.log][linesearch]: improvement: -0.003999922018937507

[2020-01-27 12:54:48.134705][__main__.TRPOAgent.log][linesearch]: improvement: -0.002425951194811038

[2020-01-27 12:54:48.162874][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014649933102965917

[2020-01-27 12:54:48.163646][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.924445076624048e-07, Discarded policy loss value: -2.5208960310527817

[2020-01-27 12:54:48.989862][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.5817452710216

[2020-01-27 12:54:48.996455][__main__.TRPOAgent.log][batch_info]: Batch #17, batch length: 4500

[2020-01-27 12:54:53.330938][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00132302 -0.         ...  0.02249572  0.00079538
 -0.0232911 ]

[2020-01-27 12:54:53.331342][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:53.774545][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.68052863  0.         ... -0.05076958  0.32538323
 -0.27461365], shape=(4547,), dtype=float64)

[2020-01-27 12:54:53.864453][__main__.TRPOAgent.log][linesearch]: improvement: -0.08565066246684261

[2020-01-27 12:54:53.894658][__main__.TRPOAgent.log][linesearch]: improvement: -0.06018609142254949

[2020-01-27 12:54:53.923474][__main__.TRPOAgent.log][linesearch]: improvement: -0.037377515578575604

[2020-01-27 12:54:53.923945][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.7365054494958683

[2020-01-27 12:54:54.747076][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.14448401868296

[2020-01-27 12:54:54.747501][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:54:54.756948][__main__.TRPOAgent.log][batch_info]: Batch #18, batch length: 4500

[2020-01-27 12:54:59.320306][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01565989 -0.         ... -0.0366651   0.28292113
 -0.24625603]

[2020-01-27 12:54:59.320716][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:54:59.732286][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.09896054  0.         ...  0.01200676  0.1010464
 -0.11305316], shape=(4547,), dtype=float64)

[2020-01-27 12:54:59.815203][__main__.TRPOAgent.log][linesearch]: improvement: -0.09195600243775148

[2020-01-27 12:54:59.844710][__main__.TRPOAgent.log][linesearch]: improvement: -0.05716028148293284

[2020-01-27 12:54:59.869229][__main__.TRPOAgent.log][linesearch]: improvement: -0.03499705459124036

[2020-01-27 12:54:59.895384][__main__.TRPOAgent.log][linesearch]: improvement: -0.021246507849484564

[2020-01-27 12:54:59.923853][__main__.TRPOAgent.log][linesearch]: improvement: -0.012836287028594073

[2020-01-27 12:54:59.948460][__main__.TRPOAgent.log][linesearch]: improvement: -0.007733358656109246

[2020-01-27 12:54:59.978158][__main__.TRPOAgent.log][linesearch]: improvement: -0.004651334320640466

[2020-01-27 12:55:00.007219][__main__.TRPOAgent.log][linesearch]: improvement: -0.0027948639828019317

[2020-01-27 12:55:00.032097][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016783786929464029

[2020-01-27 12:55:00.061712][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010075523767674177

[2020-01-27 12:55:00.062172][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.792212083587912e-07, Discarded policy loss value: -2.004792955746851

[2020-01-27 12:55:00.886266][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.84733553511117

[2020-01-27 12:55:00.888478][__main__.TRPOAgent.log][batch_info]: Batch #19, batch length: 1221

[2020-01-27 12:55:02.023087][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07410047 -0.         ... -0.5099335  -0.40696301
  0.91689651]

[2020-01-27 12:55:02.023470][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:55:02.244560][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[  0.         -12.00797352   0.         ...  -0.75425126   4.2504836
  -3.49623233], shape=(4547,), dtype=float64)

[2020-01-27 12:55:02.299561][__main__.TRPOAgent.log][linesearch]: improvement: 0.6918860136054192

[2020-01-27 12:55:02.318401][__main__.TRPOAgent.log][linesearch]: improvement: 0.6919599669304146

[2020-01-27 12:55:02.338299][__main__.TRPOAgent.log][linesearch]: improvement: 0.7028045708035293

[2020-01-27 12:55:02.361193][__main__.TRPOAgent.log][linesearch]: improvement: 0.6113105836774473

[2020-01-27 12:55:02.381105][__main__.TRPOAgent.log][linesearch]: improvement: 0.32246945167214935

[2020-01-27 12:55:02.399884][__main__.TRPOAgent.log][linesearch]: improvement: 0.12256645689249801

[2020-01-27 12:55:02.400370][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 5, New policy loss value: 7.5016883717184175

[2020-01-27 12:55:02.663833][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 339.35220290540593

[2020-01-27 12:55:02.664298][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:55:02.688557][__main__.TRPOAgent.log][learning]: Episode #5

[2020-01-27 12:55:02.689205][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 3000

[2020-01-27 12:55:02.722360][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 12:55:02.723064][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 12:55:02.723238][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 12:55:02.727461][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 12:55:11.943930][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 3000

[2020-01-27 12:55:11.987992][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 3000

[2020-01-27 12:55:11.988623][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 12:55:11.989734][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 12:55:11.989610][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 12:55:11.990882][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 12:55:20.685619][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 3000

[2020-01-27 12:55:20.725415][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 3000

[2020-01-27 12:55:20.727430][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 12:55:20.728265][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 12:55:20.728615][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 12:55:20.730581][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 12:55:29.446432][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 3000

[2020-01-27 12:55:29.452629][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 3000

[2020-01-27 12:55:29.453375][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 12:55:29.454204][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 12:55:29.454126][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 12:55:29.455079][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 12:55:37.979551][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 3000

[2020-01-27 12:55:38.004726][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 3000

[2020-01-27 12:55:38.005322][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 12:55:38.006310][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 12:55:38.006232][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 12:55:38.007711][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 12:55:41.163555][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1069

[2020-01-27 12:55:43.227313][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 3000

[2020-01-27 12:55:43.227868][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 12:55:43.234858][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 12:55:43.234767][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 12:55:43.236103][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 12:55:50.726608][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 3000

[2020-01-27 12:55:50.738536][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 3000

[2020-01-27 12:55:50.740510][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 12:55:50.741193][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 12:55:50.741107][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 12:55:50.742217][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 12:55:58.285273][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 3000

[2020-01-27 12:55:58.291594][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 3000

[2020-01-27 12:55:58.292171][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 12:55:58.292740][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 12:55:58.292808][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 12:55:58.296529][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 12:56:06.431540][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 3000

[2020-01-27 12:56:06.454523][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 3000

[2020-01-27 12:56:06.455125][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 12:56:06.455928][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 12:56:06.455840][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 12:56:06.456904][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 12:56:14.781901][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 3000

[2020-01-27 12:56:14.875150][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 3000

[2020-01-27 12:56:14.875650][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 12:56:14.876405][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 12:56:14.876311][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 12:56:14.877448][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 12:56:22.453305][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 3000

[2020-01-27 12:56:22.495397][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 3000

[2020-01-27 12:56:22.496018][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 12:56:22.496742][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 12:56:22.496806][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 12:56:22.499268][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 12:56:29.885991][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 3000

[2020-01-27 12:56:29.955413][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 3000

[2020-01-27 12:56:29.955927][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 12:56:29.956510][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 12:56:29.956644][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 12:56:29.959494][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 12:56:37.515798][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 3000

[2020-01-27 12:56:37.521866][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 3000

[2020-01-27 12:56:37.523021][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 12:56:37.523626][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 12:56:37.523689][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 12:56:37.525605][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 12:56:46.177906][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 3000

[2020-01-27 12:56:46.235775][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 3000

[2020-01-27 12:56:46.236372][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 12:56:46.237039][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 12:56:46.236956][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 12:56:46.238612][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 12:56:53.690201][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 3000

[2020-01-27 12:56:53.694914][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 3000

[2020-01-27 12:56:53.695902][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 12:56:53.696529][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 12:56:53.696466][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 12:56:53.697771][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 12:57:01.070089][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 3000

[2020-01-27 12:57:01.101747][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 3000

[2020-01-27 12:57:01.102274][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 12:57:01.123927][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 12:57:02.945659][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 12:57:03.000882][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 12:57:03.002721][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 88069, Batch size: 4500, Number of batches: 20

[2020-01-27 12:57:03.003273][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 12:57:03.067387][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 12:57:06.871379][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.0102957  -0.         ... -0.29290648  0.5025866
 -0.20968012]

[2020-01-27 12:57:06.872056][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:07.274882][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.13459532  0.         ... -0.08691781  0.12748819
 -0.04057038], shape=(4547,), dtype=float64)

[2020-01-27 12:57:07.360649][__main__.TRPOAgent.log][linesearch]: improvement: -0.16755642073016208

[2020-01-27 12:57:07.389382][__main__.TRPOAgent.log][linesearch]: improvement: -0.0976661099834848

[2020-01-27 12:57:07.417282][__main__.TRPOAgent.log][linesearch]: improvement: -0.05680210819472009

[2020-01-27 12:57:07.443472][__main__.TRPOAgent.log][linesearch]: improvement: -0.0329281817858007

[2020-01-27 12:57:07.469918][__main__.TRPOAgent.log][linesearch]: improvement: -0.01936900941659303

[2020-01-27 12:57:07.499104][__main__.TRPOAgent.log][linesearch]: improvement: -0.011433534174617677

[2020-01-27 12:57:07.524819][__main__.TRPOAgent.log][linesearch]: improvement: -0.006818872518929986

[2020-01-27 12:57:07.550993][__main__.TRPOAgent.log][linesearch]: improvement: -0.00412277519725901

[2020-01-27 12:57:07.580495][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024947970545659004

[2020-01-27 12:57:07.609120][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015160127966247217

[2020-01-27 12:57:07.609560][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.428429185915761e-07, Discarded policy loss value: -7.251967713365376

[2020-01-27 12:57:08.434063][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.7030043083183

[2020-01-27 12:57:08.440650][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 12:57:12.824455][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00992776 -0.         ...  0.17326622 -0.34573638
  0.17247016]

[2020-01-27 12:57:12.824910][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:13.295884][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.58821424  0.         ... -0.13233674  0.18277662
 -0.05043987], shape=(4547,), dtype=float64)

[2020-01-27 12:57:13.389641][__main__.TRPOAgent.log][linesearch]: improvement: -0.06722555101073668

[2020-01-27 12:57:13.414448][__main__.TRPOAgent.log][linesearch]: improvement: -0.024292310375508386

[2020-01-27 12:57:13.443439][__main__.TRPOAgent.log][linesearch]: improvement: -0.035351293009507234

[2020-01-27 12:57:13.473793][__main__.TRPOAgent.log][linesearch]: improvement: -0.03562794486995369

[2020-01-27 12:57:13.474306][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 2.439336714500246

[2020-01-27 12:57:14.345708][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.88669571781563

[2020-01-27 12:57:14.346200][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:57:14.354231][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 12:57:18.467438][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.0015215  -0.         ... -0.01812383  0.15293904
 -0.13481521]

[2020-01-27 12:57:18.467849][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:18.870569][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.11528695  0.         ... -0.01790736  0.09042027
 -0.07251292], shape=(4547,), dtype=float64)

[2020-01-27 12:57:18.952941][__main__.TRPOAgent.log][linesearch]: improvement: -0.06936017444823639

[2020-01-27 12:57:18.983476][__main__.TRPOAgent.log][linesearch]: improvement: -0.04332788931508347

[2020-01-27 12:57:19.009786][__main__.TRPOAgent.log][linesearch]: improvement: -0.026013804576576316

[2020-01-27 12:57:19.036833][__main__.TRPOAgent.log][linesearch]: improvement: -0.01504384389044544

[2020-01-27 12:57:19.061557][__main__.TRPOAgent.log][linesearch]: improvement: -0.008360158895294845

[2020-01-27 12:57:19.093030][__main__.TRPOAgent.log][linesearch]: improvement: -0.004698710672297768

[2020-01-27 12:57:19.119368][__main__.TRPOAgent.log][linesearch]: improvement: -0.0027830448482055736

[2020-01-27 12:57:19.146005][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017177074610250642

[2020-01-27 12:57:19.174640][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010449166737296345

[2020-01-27 12:57:19.202644][__main__.TRPOAgent.log][linesearch]: improvement: -0.0006294932592827429

[2020-01-27 12:57:19.203085][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.557312759793137e-07, Discarded policy loss value: -2.2955768537538326

[2020-01-27 12:57:19.978871][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.96218700319152

[2020-01-27 12:57:19.984304][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 12:57:23.752657][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00250768 -0.         ...  0.09193109  0.02042186
 -0.11235296]

[2020-01-27 12:57:23.753053][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:24.153450][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.57164024  0.         ...  0.28919664 -1.02851927
  0.73932262], shape=(4547,), dtype=float64)

[2020-01-27 12:57:24.202649][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 12:57:28.012412][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01022417 -0.         ... -0.12551028  0.15457136
 -0.02906108]

[2020-01-27 12:57:28.012789][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:28.432188][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.27191452  0.         ...  0.7990584  -1.62073738
  0.82167898], shape=(4547,), dtype=float64)

[2020-01-27 12:57:28.513540][__main__.TRPOAgent.log][linesearch]: improvement: -0.2320477335318354

[2020-01-27 12:57:28.538577][__main__.TRPOAgent.log][linesearch]: improvement: -0.22098091006412857

[2020-01-27 12:57:28.566014][__main__.TRPOAgent.log][linesearch]: improvement: -0.1710504533326076

[2020-01-27 12:57:28.592819][__main__.TRPOAgent.log][linesearch]: improvement: -0.09943696058764495

[2020-01-27 12:57:28.620534][__main__.TRPOAgent.log][linesearch]: improvement: -0.047384751989714924

[2020-01-27 12:57:28.649342][__main__.TRPOAgent.log][linesearch]: improvement: -0.021993485810115176

[2020-01-27 12:57:28.649794][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 5, New policy loss value: 0.6666458213956652

[2020-01-27 12:57:29.399728][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.01430527316236

[2020-01-27 12:57:29.400134][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:57:29.410758][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 12:57:33.213777][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01409278 -0.         ...  0.23941307 -0.23538129
 -0.00403178]

[2020-01-27 12:57:33.214168][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:33.614605][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.1839732   0.         ...  0.09244988 -0.02253278
 -0.0699171 ], shape=(4547,), dtype=float64)

[2020-01-27 12:57:33.692547][__main__.TRPOAgent.log][linesearch]: improvement: -0.11041590136936463

[2020-01-27 12:57:33.719804][__main__.TRPOAgent.log][linesearch]: improvement: -0.06332271556871216

[2020-01-27 12:57:33.720256][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.3582606010360947

[2020-01-27 12:57:34.617197][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.2632093125862

[2020-01-27 12:57:34.617614][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:57:34.625285][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 12:57:38.589940][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01388641 -0.         ...  0.0774425   0.07043319
 -0.14787569]

[2020-01-27 12:57:38.590329][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:39.018378][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.21550287  0.         ... -0.09919219  0.02411821
  0.07507398], shape=(4547,), dtype=float64)

[2020-01-27 12:57:39.115103][__main__.TRPOAgent.log][linesearch]: improvement: -0.06515485621975037

[2020-01-27 12:57:39.142420][__main__.TRPOAgent.log][linesearch]: improvement: -0.04314059949182569

[2020-01-27 12:57:39.168869][__main__.TRPOAgent.log][linesearch]: improvement: -0.027379037208933532

[2020-01-27 12:57:39.194010][__main__.TRPOAgent.log][linesearch]: improvement: -0.016970455000377527

[2020-01-27 12:57:39.219629][__main__.TRPOAgent.log][linesearch]: improvement: -0.010378311682481378

[2020-01-27 12:57:39.247040][__main__.TRPOAgent.log][linesearch]: improvement: -0.006297602255025847

[2020-01-27 12:57:39.274389][__main__.TRPOAgent.log][linesearch]: improvement: -0.0038039811010475155

[2020-01-27 12:57:39.303613][__main__.TRPOAgent.log][linesearch]: improvement: -0.00229153795725745

[2020-01-27 12:57:39.335139][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013782159392192739

[2020-01-27 12:57:39.367008][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008281149521911502

[2020-01-27 12:57:39.367484][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.556040534275894e-07, Discarded policy loss value: -2.6290135036494124

[2020-01-27 12:57:40.231714][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.75733639116679

[2020-01-27 12:57:40.237254][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 12:57:44.707629][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00826254 -0.         ...  0.21344183 -0.25050729
  0.03706547]

[2020-01-27 12:57:44.708029][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:45.111632][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.325245    0.         ...  0.08769032 -0.06458565
 -0.02310466], shape=(4547,), dtype=float64)

[2020-01-27 12:57:45.193155][__main__.TRPOAgent.log][linesearch]: improvement: -0.05797814060672524

[2020-01-27 12:57:45.193592][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.902576190485899

[2020-01-27 12:57:45.942697][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.2500317508119

[2020-01-27 12:57:45.943089][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:57:45.954491][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 12:57:49.892327][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02849484 -0.         ... -0.16992026  0.26341429
 -0.09349403]

[2020-01-27 12:57:49.892711][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:50.300151][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.33268961  0.         ...  0.03248834  0.11852108
 -0.15100941], shape=(4547,), dtype=float64)

[2020-01-27 12:57:50.383340][__main__.TRPOAgent.log][linesearch]: improvement: -0.10811712127495254

[2020-01-27 12:57:50.409968][__main__.TRPOAgent.log][linesearch]: improvement: -0.06470917509277174

[2020-01-27 12:57:50.434843][__main__.TRPOAgent.log][linesearch]: improvement: -0.03892042632874215

[2020-01-27 12:57:50.514558][__main__.TRPOAgent.log][linesearch]: improvement: -0.023410609604279875

[2020-01-27 12:57:50.540641][__main__.TRPOAgent.log][linesearch]: improvement: -0.014071401303720421

[2020-01-27 12:57:50.566725][__main__.TRPOAgent.log][linesearch]: improvement: -0.008452569875324834

[2020-01-27 12:57:50.593696][__main__.TRPOAgent.log][linesearch]: improvement: -0.005075179427861842

[2020-01-27 12:57:50.622183][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030464437962933744

[2020-01-27 12:57:50.648752][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018283527165017244

[2020-01-27 12:57:50.675043][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010971878783891942

[2020-01-27 12:57:50.675508][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.68231068899986e-07, Discarded policy loss value: -1.7058434576213293

[2020-01-27 12:57:51.421723][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.79110878936154

[2020-01-27 12:57:51.427021][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 12:57:55.917372][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03620507 -0.         ... -0.07353014  0.0133754
  0.06015474]

[2020-01-27 12:57:55.917772][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:57:56.332408][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.96787814  0.         ... -0.02089285 -0.33677174
  0.35766458], shape=(4547,), dtype=float64)

[2020-01-27 12:57:56.428647][__main__.TRPOAgent.log][linesearch]: improvement: -0.15054311851223456

[2020-01-27 12:57:56.429158][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.4447193121657653

[2020-01-27 12:57:57.313806][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.50708827116875

[2020-01-27 12:57:57.314193][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:57:57.321544][__main__.TRPOAgent.log][batch_info]: Batch #10, batch length: 4500

[2020-01-27 12:58:01.179155][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01344746 -0.         ...  0.05941262 -0.05353482
 -0.00587779]

[2020-01-27 12:58:01.179558][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:01.596434][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.02501652  0.         ...  0.15401537  0.40488426
 -0.55889964], shape=(4547,), dtype=float64)

[2020-01-27 12:58:01.685653][__main__.TRPOAgent.log][linesearch]: improvement: -0.03699991804731245

[2020-01-27 12:58:01.711361][__main__.TRPOAgent.log][linesearch]: improvement: -0.02554591965038789

[2020-01-27 12:58:01.738030][__main__.TRPOAgent.log][linesearch]: improvement: -0.016361466153648106

[2020-01-27 12:58:01.762101][__main__.TRPOAgent.log][linesearch]: improvement: -0.010154025523376832

[2020-01-27 12:58:01.791239][__main__.TRPOAgent.log][linesearch]: improvement: -0.006206608062433991

[2020-01-27 12:58:01.819104][__main__.TRPOAgent.log][linesearch]: improvement: -0.0037635831979789328

[2020-01-27 12:58:01.842726][__main__.TRPOAgent.log][linesearch]: improvement: -0.0022720987354031344

[2020-01-27 12:58:01.870837][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013682141410497756

[2020-01-27 12:58:01.895568][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008226979924667033

[2020-01-27 12:58:01.924792][__main__.TRPOAgent.log][linesearch]: improvement: -0.0004942527580119038

[2020-01-27 12:58:01.925276][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.282575341265902e-07, Discarded policy loss value: -0.06384448402751514

[2020-01-27 12:58:02.718103][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.05453236434262

[2020-01-27 12:58:02.723768][__main__.TRPOAgent.log][batch_info]: Batch #11, batch length: 4500

[2020-01-27 12:58:06.897665][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05801823 -0.         ...  0.28297336 -0.33059796
  0.0476246 ]

[2020-01-27 12:58:06.898076][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:07.341628][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.88957932  0.         ...  0.33370156  0.14409224
 -0.4777938 ], shape=(4547,), dtype=float64)

[2020-01-27 12:58:07.430589][__main__.TRPOAgent.log][linesearch]: improvement: -0.24460780695788453

[2020-01-27 12:58:07.457123][__main__.TRPOAgent.log][linesearch]: improvement: -0.11375221191685458

[2020-01-27 12:58:07.457782][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.80260095548235

[2020-01-27 12:58:08.293607][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.59248753598996

[2020-01-27 12:58:08.294016][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:58:08.302180][__main__.TRPOAgent.log][batch_info]: Batch #12, batch length: 4500

[2020-01-27 12:58:12.501649][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.0035425  -0.         ... -0.03968478  0.17769945
 -0.13801467]

[2020-01-27 12:58:12.502052][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:12.925938][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.16760008  0.         ... -0.34659577 -0.30435071
  0.65094648], shape=(4547,), dtype=float64)

[2020-01-27 12:58:13.007995][__main__.TRPOAgent.log][linesearch]: improvement: -0.08451668845199745

[2020-01-27 12:58:13.032094][__main__.TRPOAgent.log][linesearch]: improvement: -0.054608973363014846

[2020-01-27 12:58:13.062334][__main__.TRPOAgent.log][linesearch]: improvement: -0.03380783052771241

[2020-01-27 12:58:13.088992][__main__.TRPOAgent.log][linesearch]: improvement: -0.020602967547329043

[2020-01-27 12:58:13.114315][__main__.TRPOAgent.log][linesearch]: improvement: -0.012481452007510607

[2020-01-27 12:58:13.142505][__main__.TRPOAgent.log][linesearch]: improvement: -0.0075507274260827595

[2020-01-27 12:58:13.166111][__main__.TRPOAgent.log][linesearch]: improvement: -0.004574465542750983

[2020-01-27 12:58:13.191796][__main__.TRPOAgent.log][linesearch]: improvement: -0.002782939408906504

[2020-01-27 12:58:13.215657][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017060874658691993

[2020-01-27 12:58:13.243776][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010617971849777863

[2020-01-27 12:58:13.244246][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.299573788112638e-07, Discarded policy loss value: -0.9813885652663412

[2020-01-27 12:58:14.027728][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.82820333469816

[2020-01-27 12:58:14.033096][__main__.TRPOAgent.log][batch_info]: Batch #13, batch length: 4500

[2020-01-27 12:58:18.231449][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00581356 -0.         ... -0.07363116 -0.21918325
  0.2928144 ]

[2020-01-27 12:58:18.231841][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:18.642687][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.09892812  0.         ... -0.7884552   0.05878384
  0.72967136], shape=(4547,), dtype=float64)

[2020-01-27 12:58:18.721991][__main__.TRPOAgent.log][linesearch]: improvement: 0.23111394738228075

[2020-01-27 12:58:18.751047][__main__.TRPOAgent.log][linesearch]: improvement: 0.0874886186927819

[2020-01-27 12:58:18.776946][__main__.TRPOAgent.log][linesearch]: improvement: 0.013428719788817212

[2020-01-27 12:58:18.777407][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 2.949969235273836

[2020-01-27 12:58:19.537612][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 203.0300762009969

[2020-01-27 12:58:19.538021][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:58:19.545394][__main__.TRPOAgent.log][batch_info]: Batch #14, batch length: 4500

[2020-01-27 12:58:23.383296][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00414953 -0.         ...  0.06377305 -0.02943566
 -0.03433739]

[2020-01-27 12:58:23.383705][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:23.806877][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.19108322  0.         ...  0.25935119  0.50660188
 -0.76595306], shape=(4547,), dtype=float64)

[2020-01-27 12:58:23.893647][__main__.TRPOAgent.log][linesearch]: improvement: -0.03204706290283421

[2020-01-27 12:58:23.920098][__main__.TRPOAgent.log][linesearch]: improvement: -0.02510952788037324

[2020-01-27 12:58:23.947314][__main__.TRPOAgent.log][linesearch]: improvement: -0.01735293894708745

[2020-01-27 12:58:23.972406][__main__.TRPOAgent.log][linesearch]: improvement: -0.011278646429563333

[2020-01-27 12:58:24.000144][__main__.TRPOAgent.log][linesearch]: improvement: -0.007089957129201352

[2020-01-27 12:58:24.026337][__main__.TRPOAgent.log][linesearch]: improvement: -0.004372694910164787

[2020-01-27 12:58:24.051084][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026669333238467097

[2020-01-27 12:58:24.079080][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016158829628467597

[2020-01-27 12:58:24.105770][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009752185283400383

[2020-01-27 12:58:24.133212][__main__.TRPOAgent.log][linesearch]: improvement: -0.0005871852993513116

[2020-01-27 12:58:24.133661][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.409457280489813e-07, Discarded policy loss value: -1.9865957973511719

[2020-01-27 12:58:24.914685][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.62764514596222

[2020-01-27 12:58:24.919797][__main__.TRPOAgent.log][batch_info]: Batch #15, batch length: 4500

[2020-01-27 12:58:28.735732][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03589739 -0.         ...  0.08560591  0.24040549
 -0.3260114 ]

[2020-01-27 12:58:28.736141][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:29.144821][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.99654169  0.         ...  0.60647908  0.60807548
 -1.21455456], shape=(4547,), dtype=float64)

[2020-01-27 12:58:29.229221][__main__.TRPOAgent.log][linesearch]: improvement: -0.14311155887267857

[2020-01-27 12:58:29.254143][__main__.TRPOAgent.log][linesearch]: improvement: -0.09006855848422335

[2020-01-27 12:58:29.281603][__main__.TRPOAgent.log][linesearch]: improvement: -0.05542248746898659

[2020-01-27 12:58:29.305956][__main__.TRPOAgent.log][linesearch]: improvement: -0.03371447757585555

[2020-01-27 12:58:29.334808][__main__.TRPOAgent.log][linesearch]: improvement: -0.020385749664155295

[2020-01-27 12:58:29.361564][__main__.TRPOAgent.log][linesearch]: improvement: -0.012285935532816428

[2020-01-27 12:58:29.384778][__main__.TRPOAgent.log][linesearch]: improvement: -0.0073907142297415795

[2020-01-27 12:58:29.412771][__main__.TRPOAgent.log][linesearch]: improvement: -0.0044412215220823725

[2020-01-27 12:58:29.437987][__main__.TRPOAgent.log][linesearch]: improvement: -0.002667156046795238

[2020-01-27 12:58:29.467082][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016011610923505337

[2020-01-27 12:58:29.467617][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.645056281750475e-07, Discarded policy loss value: -2.442024145233717

[2020-01-27 12:58:30.257546][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 106.37630052341899

[2020-01-27 12:58:30.265176][__main__.TRPOAgent.log][batch_info]: Batch #16, batch length: 4500

[2020-01-27 12:58:34.053276][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.06006665 -0.         ... -0.15475529 -0.18549605
  0.34025134]

[2020-01-27 12:58:34.053680][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:34.465673][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          2.42215589  0.         ... -0.26311259  2.04890001
 -1.78578742], shape=(4547,), dtype=float64)

[2020-01-27 12:58:34.547217][__main__.TRPOAgent.log][linesearch]: improvement: 0.2897648851654473

[2020-01-27 12:58:34.574181][__main__.TRPOAgent.log][linesearch]: improvement: 0.3224618091369791

[2020-01-27 12:58:34.600918][__main__.TRPOAgent.log][linesearch]: improvement: 0.26120391209961924

[2020-01-27 12:58:34.632184][__main__.TRPOAgent.log][linesearch]: improvement: 0.12421707082528344

[2020-01-27 12:58:34.632803][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 1.5444155383774447

[2020-01-27 12:58:35.426733][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 200.59475354903455

[2020-01-27 12:58:35.427149][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:58:35.434731][__main__.TRPOAgent.log][batch_info]: Batch #17, batch length: 4500

[2020-01-27 12:58:39.210012][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.08594346 -0.         ...  0.18744858  0.24228359
 -0.42973216]

[2020-01-27 12:58:39.210449][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:39.626264][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.6658741   0.         ...  0.28467377 -0.09381839
 -0.19085539], shape=(4547,), dtype=float64)

[2020-01-27 12:58:39.714977][__main__.TRPOAgent.log][linesearch]: improvement: -0.21156718323471457

[2020-01-27 12:58:39.743614][__main__.TRPOAgent.log][linesearch]: improvement: -0.13803523474031332

[2020-01-27 12:58:39.773419][__main__.TRPOAgent.log][linesearch]: improvement: -0.08724654691833988

[2020-01-27 12:58:39.802470][__main__.TRPOAgent.log][linesearch]: improvement: -0.05404094645905433

[2020-01-27 12:58:39.825364][__main__.TRPOAgent.log][linesearch]: improvement: -0.03305649141409317

[2020-01-27 12:58:39.854351][__main__.TRPOAgent.log][linesearch]: improvement: -0.02006634428406029

[2020-01-27 12:58:39.879088][__main__.TRPOAgent.log][linesearch]: improvement: -0.012124573235756664

[2020-01-27 12:58:39.903671][__main__.TRPOAgent.log][linesearch]: improvement: -0.007305496092586683

[2020-01-27 12:58:39.932176][__main__.TRPOAgent.log][linesearch]: improvement: -0.004394419673220629

[2020-01-27 12:58:39.955304][__main__.TRPOAgent.log][linesearch]: improvement: -0.002640666829116345

[2020-01-27 12:58:39.955762][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.713411334455208e-07, Discarded policy loss value: -2.639488920396119

[2020-01-27 12:58:40.729963][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 105.8923500135614

[2020-01-27 12:58:40.735408][__main__.TRPOAgent.log][batch_info]: Batch #18, batch length: 4500

[2020-01-27 12:58:44.571820][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.13411776 -0.         ... -0.0521679  -0.51573224
  0.56790014]

[2020-01-27 12:58:44.572230][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:44.998499][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          3.83847058  0.         ...  0.88066276  1.9125412
 -2.79320396], shape=(4547,), dtype=float64)

[2020-01-27 12:58:45.084224][__main__.TRPOAgent.log][linesearch]: improvement: 0.4875218686288689

[2020-01-27 12:58:45.115305][__main__.TRPOAgent.log][linesearch]: improvement: 0.16526391649897132

[2020-01-27 12:58:45.138974][__main__.TRPOAgent.log][linesearch]: improvement: 0.01259428319056255

[2020-01-27 12:58:45.139413][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 4.2655550354880765

[2020-01-27 12:58:45.910891][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 201.4650030656838

[2020-01-27 12:58:45.911300][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:58:45.916914][__main__.TRPOAgent.log][batch_info]: Batch #19, batch length: 2569

[2020-01-27 12:58:48.064970][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03698524 -0.         ...  0.07367951  0.13122315
 -0.20490267]

[2020-01-27 12:58:48.065345][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 12:58:48.362219][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          4.19692609  0.         ...  0.1791282   0.86370862
 -1.04283683], shape=(4547,), dtype=float64)

[2020-01-27 12:58:48.432089][__main__.TRPOAgent.log][linesearch]: improvement: -0.17270197697315012

[2020-01-27 12:58:48.432601][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.3101030664237613

[2020-01-27 12:58:48.876720][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 180.27349073449452

[2020-01-27 12:58:48.877165][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 12:58:48.895638][__main__.TRPOAgent.log][learning]: Episode #6

[2020-01-27 12:58:48.896234][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 3000

[2020-01-27 12:58:48.925801][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 12:58:48.926530][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 12:58:48.926757][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 12:58:48.933213][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 12:58:56.640844][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 3000

[2020-01-27 12:58:56.662200][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 3000

[2020-01-27 12:58:56.662801][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 12:58:56.663850][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 12:58:56.663988][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 12:58:56.666879][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 12:59:04.297090][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 3000

[2020-01-27 12:59:04.327360][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 3000

[2020-01-27 12:59:04.327980][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 12:59:04.328572][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 12:59:04.328644][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 12:59:04.330530][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 12:59:11.898731][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 3000

[2020-01-27 12:59:11.911475][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 3000

[2020-01-27 12:59:11.912417][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 12:59:11.913010][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 12:59:11.913104][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 12:59:11.916940][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 12:59:19.541747][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 3000

[2020-01-27 12:59:19.703189][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 3000

