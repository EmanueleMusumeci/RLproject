LOGGER started at 2020-01-27 14:28:43.586478.
Currently active debug channels:
	rollouts
	training
	batch_info
	linesearch
	learning
	thread_rollouts
[2020-01-27 14:28:43.719375][__main__.TRPOAgent.log][learning]: Episode #0

[2020-01-27 14:28:43.972867][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:28:44.026642][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:28:44.027493][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:28:44.027403][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:28:44.028882][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:28:46.867334][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1149

[2020-01-27 14:28:47.173375][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:28:47.173958][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:28:47.174610][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:28:47.174676][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:28:47.177158][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:28:49.907474][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1237

[2020-01-27 14:28:50.191938][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:28:50.192451][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:28:50.193026][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:28:50.193225][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:28:50.194165][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:28:53.485502][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:28:53.523345][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:28:53.523796][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:28:53.524283][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:28:53.524371][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:28:53.526501][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:28:56.759456][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:28:56.776692][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:28:56.777370][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:28:56.778328][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:28:56.778571][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:28:56.780588][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:28:58.492728][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 799

[2020-01-27 14:28:59.203844][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:28:59.204463][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:28:59.205078][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:28:59.205151][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:28:59.206468][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:29:02.412103][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:29:02.442975][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:29:02.443506][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:29:02.444062][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:29:02.444179][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:29:02.446758][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:29:05.635915][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:29:05.665482][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:29:05.666085][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:29:05.666689][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:29:05.666758][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:29:05.668721][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:29:08.887486][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:29:08.922970][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:29:08.923494][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:29:08.924243][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:29:08.924102][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:29:08.925242][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:29:12.134785][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:29:12.156139][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:29:12.156628][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:29:12.157164][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:29:12.157240][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:29:12.159667][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:29:15.403610][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:29:15.422294][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:29:15.422824][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:29:15.423344][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:29:15.423408][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:29:15.425063][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:29:18.598840][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:29:18.634748][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:29:18.635269][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:29:18.635837][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:29:18.636053][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:29:18.637833][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:29:21.834544][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:29:21.838924][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:29:21.839728][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:29:21.840344][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:29:21.840474][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:29:21.843098][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:29:25.107270][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:29:25.144407][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:29:25.145021][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:29:25.145583][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:29:25.145644][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:29:25.147237][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:29:28.760648][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:29:28.794886][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:29:28.795358][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:29:28.795946][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:29:28.796087][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:29:28.797879][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:29:31.987250][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:29:31.988910][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:29:31.990043][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:29:32.001714][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:29:32.467048][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:29:32.474275][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:29:32.475728][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 43685, Batch size: 4500, Number of batches: 10

[2020-01-27 14:29:32.476114][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:29:32.505630][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:29:35.845618][__main__.TRPOAgent.log][training]: policy_gradient: [-0.1108757  -0.1023145  -0.04852455 ... -0.30766928  0.90829007
 -0.60062079]

[2020-01-27 14:29:35.846003][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:29:36.228950][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 3.47775585 -1.1987433   3.33759869 ...  7.80753255 -4.74348971
 -3.06404284], shape=(4547,), dtype=float64)

[2020-01-27 14:29:36.302468][__main__.TRPOAgent.log][linesearch]: improvement: -0.7997162176971386

[2020-01-27 14:29:36.334916][__main__.TRPOAgent.log][linesearch]: improvement: -0.4122152780303736

[2020-01-27 14:29:36.362269][__main__.TRPOAgent.log][linesearch]: improvement: -0.2128086711089452

[2020-01-27 14:29:36.389808][__main__.TRPOAgent.log][linesearch]: improvement: -0.11430448501641877

[2020-01-27 14:29:36.415468][__main__.TRPOAgent.log][linesearch]: improvement: -0.06116829758843778

[2020-01-27 14:29:36.439180][__main__.TRPOAgent.log][linesearch]: improvement: -0.0362973939084128

[2020-01-27 14:29:36.464635][__main__.TRPOAgent.log][linesearch]: improvement: -0.024074805131192534

[2020-01-27 14:29:36.491519][__main__.TRPOAgent.log][linesearch]: improvement: -0.01722776881464938

[2020-01-27 14:29:36.514532][__main__.TRPOAgent.log][linesearch]: improvement: -0.01133331221862477

[2020-01-27 14:29:36.541581][__main__.TRPOAgent.log][linesearch]: improvement: -0.007928177047162421

[2020-01-27 14:29:36.542088][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.040874943174785e-07, Discarded policy loss value: -93.3844823644416

[2020-01-27 14:29:37.375689][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 313.88418718591157

[2020-01-27 14:29:37.380063][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:29:40.829770][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.02870837  0.02468892  0.01434495 ... -0.08728316 -0.19671781
  0.28400097]

[2020-01-27 14:29:40.830151][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:29:41.218636][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.68426684 -2.41187615  1.12457305 ... -0.37424714  1.11682585
 -0.74257871], shape=(4547,), dtype=float64)

[2020-01-27 14:29:41.293484][__main__.TRPOAgent.log][linesearch]: improvement: 0.053568678210274

[2020-01-27 14:29:41.317012][__main__.TRPOAgent.log][linesearch]: improvement: 0.022359407715009993

[2020-01-27 14:29:41.317451][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 4.564910148989873

[2020-01-27 14:29:42.040719][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 292.58383238542575

[2020-01-27 14:29:42.041126][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:29:42.048170][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:29:45.482764][__main__.TRPOAgent.log][training]: policy_gradient: [-0.02483773 -0.0295344  -0.         ...  0.04995409  0.14923853
 -0.19919262]

[2020-01-27 14:29:45.483164][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:29:45.871222][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.45398542 -0.69347809  0.         ... -0.7900433   1.21188896
 -0.42184566], shape=(4547,), dtype=float64)

[2020-01-27 14:29:45.950824][__main__.TRPOAgent.log][linesearch]: improvement: -0.01455867904410102

[2020-01-27 14:29:45.978549][__main__.TRPOAgent.log][linesearch]: improvement: -0.016279035465410985

[2020-01-27 14:29:46.003007][__main__.TRPOAgent.log][linesearch]: improvement: -0.015165281293051613

[2020-01-27 14:29:46.028333][__main__.TRPOAgent.log][linesearch]: improvement: -0.014868488396127866

[2020-01-27 14:29:46.053865][__main__.TRPOAgent.log][linesearch]: improvement: -0.01137344402000462

[2020-01-27 14:29:46.082863][__main__.TRPOAgent.log][linesearch]: improvement: -0.007086730770998528

[2020-01-27 14:29:46.108762][__main__.TRPOAgent.log][linesearch]: improvement: -0.00457114771754874

[2020-01-27 14:29:46.133630][__main__.TRPOAgent.log][linesearch]: improvement: -0.003483083968147227

[2020-01-27 14:29:46.163317][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026111504040162714

[2020-01-27 14:29:46.186016][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018697183934195394

[2020-01-27 14:29:46.186456][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.437774222701094e-07, Discarded policy loss value: -6.742749780466164

[2020-01-27 14:29:46.931657][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.476701505547

[2020-01-27 14:29:46.937268][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:29:50.345894][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01978567  0.02191916 -0.         ... -0.06645115 -0.09158003
  0.15803118]

[2020-01-27 14:29:50.346279][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:29:50.729467][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.55831797  0.70185659  0.         ... -0.21354297  0.8370683
 -0.62352533], shape=(4547,), dtype=float64)

[2020-01-27 14:29:50.811340][__main__.TRPOAgent.log][linesearch]: improvement: -0.08709005197047981

[2020-01-27 14:29:50.836506][__main__.TRPOAgent.log][linesearch]: improvement: -0.06038556964701325

[2020-01-27 14:29:50.863498][__main__.TRPOAgent.log][linesearch]: improvement: -0.03622997598300959

[2020-01-27 14:29:50.863982][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 3.0707716249839474

[2020-01-27 14:29:51.605444][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 266.7436918702511

[2020-01-27 14:29:51.605845][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:29:51.617204][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:29:54.974512][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00296023  0.02781827 -0.         ... -0.27245542  0.15315492
  0.1193005 ]

[2020-01-27 14:29:54.974902][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:29:55.367284][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.36418836  0.42940106  0.         ... -5.0909473   2.41162686
  2.67932044], shape=(4547,), dtype=float64)

[2020-01-27 14:29:55.442918][__main__.TRPOAgent.log][linesearch]: improvement: -0.2110506776955825

[2020-01-27 14:29:55.470230][__main__.TRPOAgent.log][linesearch]: improvement: -0.11545468195165132

[2020-01-27 14:29:55.493957][__main__.TRPOAgent.log][linesearch]: improvement: -0.057041657931014456

[2020-01-27 14:29:55.520107][__main__.TRPOAgent.log][linesearch]: improvement: -0.030573802605435407

[2020-01-27 14:29:55.548258][__main__.TRPOAgent.log][linesearch]: improvement: -0.02053587478419061

[2020-01-27 14:29:55.571903][__main__.TRPOAgent.log][linesearch]: improvement: -0.015136486415243677

[2020-01-27 14:29:55.600404][__main__.TRPOAgent.log][linesearch]: improvement: -0.011342952152352037

[2020-01-27 14:29:55.624916][__main__.TRPOAgent.log][linesearch]: improvement: -0.008125110865743412

[2020-01-27 14:29:55.654466][__main__.TRPOAgent.log][linesearch]: improvement: -0.005431908458835011

[2020-01-27 14:29:55.679100][__main__.TRPOAgent.log][linesearch]: improvement: -0.0034746001161440887

[2020-01-27 14:29:55.679582][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.126050119246143e-07, Discarded policy loss value: -3.944171333155188

[2020-01-27 14:29:56.353684][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 293.78220180077705

[2020-01-27 14:29:56.358480][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:30:00.023212][__main__.TRPOAgent.log][training]: policy_gradient: [-0.04152539 -0.0394808  -0.         ...  0.10364795  0.32813771
 -0.43178566]

[2020-01-27 14:30:00.023611][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:30:00.405163][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.71534274 -0.72641217  0.         ...  0.39215673 -2.29865974
  1.90650301], shape=(4547,), dtype=float64)

[2020-01-27 14:30:00.486108][__main__.TRPOAgent.log][linesearch]: improvement: 0.1135920204095262

[2020-01-27 14:30:00.514860][__main__.TRPOAgent.log][linesearch]: improvement: 0.04482462080063687

[2020-01-27 14:30:00.538761][__main__.TRPOAgent.log][linesearch]: improvement: 0.01573237534350369

[2020-01-27 14:30:00.539202][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 3.1463038163274684

[2020-01-27 14:30:01.263569][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 293.63252181280996

[2020-01-27 14:30:01.263964][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:30:01.271228][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:30:04.628410][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01858237  0.01408759 -0.         ... -0.01746028 -0.09048372
  0.107944  ]

[2020-01-27 14:30:04.633943][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:30:05.019003][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.25247934 -0.29328216  0.         ... -0.34124949  0.40288881
 -0.06163932], shape=(4547,), dtype=float64)

[2020-01-27 14:30:05.093604][__main__.TRPOAgent.log][linesearch]: improvement: 0.05135168818010283

[2020-01-27 14:30:05.117771][__main__.TRPOAgent.log][linesearch]: improvement: 0.018837393216660114

[2020-01-27 14:30:05.148532][__main__.TRPOAgent.log][linesearch]: improvement: 0.006161680192818819

[2020-01-27 14:30:05.172234][__main__.TRPOAgent.log][linesearch]: improvement: 0.0011116250697673635

[2020-01-27 14:30:05.202022][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017204875895988625

[2020-01-27 14:30:05.225392][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032222823763763397

[2020-01-27 14:30:05.253354][__main__.TRPOAgent.log][linesearch]: improvement: -0.0029842408959761535

[2020-01-27 14:30:05.276957][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024785011692320946

[2020-01-27 14:30:05.302597][__main__.TRPOAgent.log][linesearch]: improvement: -0.001699933677627552

[2020-01-27 14:30:05.329670][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011111269664190182

[2020-01-27 14:30:05.330241][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.51416339257016e-07, Discarded policy loss value: -1.7076868421579845

[2020-01-27 14:30:06.070953][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 295.97029317039403

[2020-01-27 14:30:06.077989][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:30:09.593489][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00487683  0.00849311 -0.         ... -0.13486611 -0.26654187
  0.40140798]

[2020-01-27 14:30:09.593874][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:30:09.981348][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-2.86942937  0.69902681  0.         ... -2.67367019 -1.5437308
  4.21740099], shape=(4547,), dtype=float64)

[2020-01-27 14:30:10.059623][__main__.TRPOAgent.log][linesearch]: improvement: -0.3568928348920346

[2020-01-27 14:30:10.088020][__main__.TRPOAgent.log][linesearch]: improvement: -0.17469496097779036

[2020-01-27 14:30:10.111576][__main__.TRPOAgent.log][linesearch]: improvement: -0.09213990318434329

[2020-01-27 14:30:10.137404][__main__.TRPOAgent.log][linesearch]: improvement: -0.058892391838902824

[2020-01-27 14:30:10.164035][__main__.TRPOAgent.log][linesearch]: improvement: -0.038848681593657286

[2020-01-27 14:30:10.189527][__main__.TRPOAgent.log][linesearch]: improvement: -0.026635068335637113

[2020-01-27 14:30:10.214891][__main__.TRPOAgent.log][linesearch]: improvement: -0.019061973444584712

[2020-01-27 14:30:10.239914][__main__.TRPOAgent.log][linesearch]: improvement: -0.012914696494478628

[2020-01-27 14:30:10.266274][__main__.TRPOAgent.log][linesearch]: improvement: -0.008112639150430923

[2020-01-27 14:30:10.289936][__main__.TRPOAgent.log][linesearch]: improvement: -0.004997810485276144

[2020-01-27 14:30:10.290377][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.537857896061405e-07, Discarded policy loss value: -2.0076338411687638

[2020-01-27 14:30:11.086329][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.8889607521046

[2020-01-27 14:30:11.091488][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:30:14.507357][__main__.TRPOAgent.log][training]: policy_gradient: [-0.0033009   0.01442911 -0.         ... -0.18284923  0.18977594
 -0.00692671]

[2020-01-27 14:30:14.507739][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:30:14.899177][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.16560442  0.15165167  0.         ... -0.15708711  0.80532381
 -0.6482367 ], shape=(4547,), dtype=float64)

[2020-01-27 14:30:14.977775][__main__.TRPOAgent.log][linesearch]: improvement: 0.11312967841487054

[2020-01-27 14:30:15.005527][__main__.TRPOAgent.log][linesearch]: improvement: 0.0571532626732317

[2020-01-27 14:30:15.029249][__main__.TRPOAgent.log][linesearch]: improvement: 0.02131863247027238

[2020-01-27 14:30:15.029915][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.27511323439066016

[2020-01-27 14:30:15.743699][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 298.2410503437718

[2020-01-27 14:30:15.744102][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:30:15.750038][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 3185

[2020-01-27 14:30:18.160519][__main__.TRPOAgent.log][training]: policy_gradient: [-0.03842933  0.05593027 -0.         ... -0.21602209  0.41457478
 -0.19855268]

[2020-01-27 14:30:18.160897][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:30:18.476340][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 1.44915084  0.27925063  0.         ... -0.73468754 -1.79953325
  2.53422079], shape=(4547,), dtype=float64)

[2020-01-27 14:30:18.543108][__main__.TRPOAgent.log][linesearch]: improvement: -0.7138123966605989

[2020-01-27 14:30:18.565881][__main__.TRPOAgent.log][linesearch]: improvement: -0.29688094414420396

[2020-01-27 14:30:18.589893][__main__.TRPOAgent.log][linesearch]: improvement: -0.13301709255579786

[2020-01-27 14:30:18.590327][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 3.9530495941935584

[2020-01-27 14:30:19.093776][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 386.7587245702288

[2020-01-27 14:30:19.094167][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:30:19.101221][__main__.TRPOAgent.log][learning]: Episode #1

[2020-01-27 14:30:19.101565][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:30:19.147774][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:30:19.148458][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:30:19.148348][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:30:19.149287][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:30:22.280018][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:30:22.378940][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:30:22.379551][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:30:22.380440][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:30:22.380508][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:30:22.382463][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:30:25.640639][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:30:25.663961][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:30:25.664592][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:30:25.665621][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:30:25.665788][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:30:25.668279][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:30:27.813455][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 988

[2020-01-27 14:30:28.315096][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:30:28.315685][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:30:28.316360][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:30:28.316478][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:30:28.319186][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:30:31.576425][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:30:31.634181][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:30:31.634664][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:30:31.635251][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:30:31.635326][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:30:31.639420][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:30:34.889927][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:30:34.927019][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:30:34.927515][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:30:34.928199][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:30:34.928379][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:30:34.931874][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:30:38.190113][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:30:38.200410][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:30:38.201092][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:30:38.201706][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:30:38.201761][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:30:38.203416][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:30:39.780207][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 674

[2020-01-27 14:30:40.545906][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:30:40.546439][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:30:40.547218][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:30:40.547279][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:30:40.549129][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:30:43.843653][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:30:43.860241][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:30:43.860714][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:30:43.861304][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:30:43.861376][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:30:43.863300][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:30:47.108562][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:30:47.110059][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:30:47.110979][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:30:47.111734][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:30:47.111818][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:30:47.113574][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:30:50.384997][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:30:50.393456][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:30:50.394086][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:30:50.394652][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:30:50.394776][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:30:50.396851][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:30:53.639097][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:30:53.645952][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:30:53.646425][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:30:53.647083][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:30:53.647148][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:30:53.648948][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:30:56.912153][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:30:56.932905][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:30:56.933370][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:30:56.934109][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:30:56.934180][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:30:56.935967][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:31:00.187959][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:31:00.261532][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:31:00.262073][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:31:00.262799][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:31:00.263000][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:31:00.265301][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:31:03.498087][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:31:03.523686][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:31:03.524300][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:31:03.525390][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:31:03.525252][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:31:03.526751][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:31:06.786914][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:31:06.866755][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:31:06.867253][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:31:06.878539][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:31:07.343598][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:31:07.368822][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:31:07.370201][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 43662, Batch size: 4500, Number of batches: 10

[2020-01-27 14:31:07.370560][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:31:07.400840][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:31:10.874804][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00732512  0.0401527  -0.         ... -0.17995159  0.11189597
  0.06805562]

[2020-01-27 14:31:10.875216][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:11.261196][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.24482429 -0.32581582  0.         ... -0.56498732  0.3619387
  0.20304862], shape=(4547,), dtype=float64)

[2020-01-27 14:31:11.339695][__main__.TRPOAgent.log][linesearch]: improvement: -0.043044015001013625

[2020-01-27 14:31:11.368065][__main__.TRPOAgent.log][linesearch]: improvement: -0.02182642646501698

[2020-01-27 14:31:11.391348][__main__.TRPOAgent.log][linesearch]: improvement: -0.011915249536273986

[2020-01-27 14:31:11.419298][__main__.TRPOAgent.log][linesearch]: improvement: -0.006943295332759503

[2020-01-27 14:31:11.445406][__main__.TRPOAgent.log][linesearch]: improvement: -0.004311051116250519

[2020-01-27 14:31:11.473776][__main__.TRPOAgent.log][linesearch]: improvement: -0.003027732762704405

[2020-01-27 14:31:11.499564][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021579188050446785

[2020-01-27 14:31:11.523694][__main__.TRPOAgent.log][linesearch]: improvement: -0.001602146206557853

[2020-01-27 14:31:11.551192][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009868962125409997

[2020-01-27 14:31:11.578780][__main__.TRPOAgent.log][linesearch]: improvement: -0.0005345461141197383

[2020-01-27 14:31:11.579325][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.483285667645062e-07, Discarded policy loss value: -1.5552850853139892

[2020-01-27 14:31:12.326612][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.1252994952493

[2020-01-27 14:31:12.331636][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:31:15.715270][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01555325 -0.00325814 -0.         ...  0.00179607 -0.0019863
  0.00019024]

[2020-01-27 14:31:15.715655][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:16.112583][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 2.47919003  0.44024228  0.         ...  0.17459973  1.2958991
 -1.47049883], shape=(4547,), dtype=float64)

[2020-01-27 14:31:16.191478][__main__.TRPOAgent.log][linesearch]: improvement: -0.05065975970764157

[2020-01-27 14:31:16.221085][__main__.TRPOAgent.log][linesearch]: improvement: -0.03519324769117581

[2020-01-27 14:31:16.246368][__main__.TRPOAgent.log][linesearch]: improvement: -0.022821756840669574

[2020-01-27 14:31:16.246839][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.03626679199233816

[2020-01-27 14:31:16.968088][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 283.1885542009599

[2020-01-27 14:31:16.968481][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:31:16.975774][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:31:20.383424][__main__.TRPOAgent.log][training]: policy_gradient: [-0.0071566   0.0231601  -0.         ... -0.18258801  0.16084044
  0.02174757]

[2020-01-27 14:31:20.383806][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:20.778312][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.11086531 -0.42221318  0.         ...  0.17246496  0.15033648
 -0.32280145], shape=(4547,), dtype=float64)

[2020-01-27 14:31:20.855363][__main__.TRPOAgent.log][linesearch]: improvement: -0.03832068081624615

[2020-01-27 14:31:20.885212][__main__.TRPOAgent.log][linesearch]: improvement: -0.022973204889400634

[2020-01-27 14:31:20.909773][__main__.TRPOAgent.log][linesearch]: improvement: -0.018832255903024314

[2020-01-27 14:31:20.936113][__main__.TRPOAgent.log][linesearch]: improvement: -0.01630538225435077

[2020-01-27 14:31:20.960353][__main__.TRPOAgent.log][linesearch]: improvement: -0.013058612606221986

[2020-01-27 14:31:20.989897][__main__.TRPOAgent.log][linesearch]: improvement: -0.00848312584347799

[2020-01-27 14:31:21.014564][__main__.TRPOAgent.log][linesearch]: improvement: -0.004690137551632195

[2020-01-27 14:31:21.039889][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026031395760500453

[2020-01-27 14:31:21.067332][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015594821217981003

[2020-01-27 14:31:21.091764][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011325997045070202

[2020-01-27 14:31:21.092208][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.43117909558112e-07, Discarded policy loss value: -0.1848078887867153

[2020-01-27 14:31:21.816933][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.3830654904345

[2020-01-27 14:31:21.821890][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:31:25.251993][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00933328 -0.02348131 -0.         ... -0.0024683  -0.19271314
  0.19518144]

[2020-01-27 14:31:25.252383][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:25.641814][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.21528877  0.47622946  0.         ... -0.24506047 -0.59024993
  0.83531041], shape=(4547,), dtype=float64)

[2020-01-27 14:31:25.721927][__main__.TRPOAgent.log][linesearch]: improvement: 0.10371791050039025

[2020-01-27 14:31:25.751802][__main__.TRPOAgent.log][linesearch]: improvement: 0.02569093739993189

[2020-01-27 14:31:25.775334][__main__.TRPOAgent.log][linesearch]: improvement: 0.004420225595640481

[2020-01-27 14:31:25.775817][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 4.4236230020468215

[2020-01-27 14:31:26.495727][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.8578831226238

[2020-01-27 14:31:26.496133][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:31:26.503146][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:31:29.906298][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01245127 -0.02253333 -0.         ... -0.00307962  0.01732056
 -0.01424094]

[2020-01-27 14:31:29.906685][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:30.295078][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.84720799  1.2225817   0.         ...  0.88875544  0.10712069
 -0.99587613], shape=(4547,), dtype=float64)

[2020-01-27 14:31:30.379429][__main__.TRPOAgent.log][linesearch]: improvement: -0.004378320111777967

[2020-01-27 14:31:30.407340][__main__.TRPOAgent.log][linesearch]: improvement: -0.0044723820918326496

[2020-01-27 14:31:30.407778][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.4200725953196078

[2020-01-27 14:31:31.108083][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.57420768423424

[2020-01-27 14:31:31.108467][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:31:31.115795][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:31:34.507276][__main__.TRPOAgent.log][training]: policy_gradient: [-0.05172514  0.09834666 -0.         ... -0.24016692  0.32181904
 -0.08165212]

[2020-01-27 14:31:34.507668][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:34.903908][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.46735876  1.38116245  0.         ...  0.24074878  0.56995873
 -0.81070751], shape=(4547,), dtype=float64)

[2020-01-27 14:31:34.981565][__main__.TRPOAgent.log][linesearch]: improvement: 0.0600114067938895

[2020-01-27 14:31:35.009751][__main__.TRPOAgent.log][linesearch]: improvement: 0.02729629622506824

[2020-01-27 14:31:35.033596][__main__.TRPOAgent.log][linesearch]: improvement: 0.009844372547565428

[2020-01-27 14:31:35.058509][__main__.TRPOAgent.log][linesearch]: improvement: 0.0016685515536623008

[2020-01-27 14:31:35.084605][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026632304197127254

[2020-01-27 14:31:35.113395][__main__.TRPOAgent.log][linesearch]: improvement: -0.00369655059094276

[2020-01-27 14:31:35.139442][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023446300372595985

[2020-01-27 14:31:35.162846][__main__.TRPOAgent.log][linesearch]: improvement: -0.0020079669490966623

[2020-01-27 14:31:35.189606][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017060387870362348

[2020-01-27 14:31:35.212809][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012238235026518218

[2020-01-27 14:31:35.213264][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.656768790445835e-07, Discarded policy loss value: -3.9448810939767127

[2020-01-27 14:31:35.957138][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.2352914027967

[2020-01-27 14:31:35.962346][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:31:39.326208][__main__.TRPOAgent.log][training]: policy_gradient: [-0.03609649  0.01744161 -0.         ... -0.1974163   0.08946641
  0.1079499 ]

[2020-01-27 14:31:39.326601][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:39.722029][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.23769889  1.03749889  0.         ...  0.24997646  0.26159075
 -0.51156722], shape=(4547,), dtype=float64)

[2020-01-27 14:31:39.808685][__main__.TRPOAgent.log][linesearch]: improvement: -0.17505794719636514

[2020-01-27 14:31:39.834389][__main__.TRPOAgent.log][linesearch]: improvement: -0.09556521777170923

[2020-01-27 14:31:39.861211][__main__.TRPOAgent.log][linesearch]: improvement: -0.05076790176695445

[2020-01-27 14:31:39.888197][__main__.TRPOAgent.log][linesearch]: improvement: -0.026944996651077524

[2020-01-27 14:31:39.911544][__main__.TRPOAgent.log][linesearch]: improvement: -0.01540236416449936

[2020-01-27 14:31:39.938168][__main__.TRPOAgent.log][linesearch]: improvement: -0.009404975049313258

[2020-01-27 14:31:39.962955][__main__.TRPOAgent.log][linesearch]: improvement: -0.005756820350148573

[2020-01-27 14:31:39.990323][__main__.TRPOAgent.log][linesearch]: improvement: -0.003714160097938901

[2020-01-27 14:31:40.013941][__main__.TRPOAgent.log][linesearch]: improvement: -0.002373836692018294

[2020-01-27 14:31:40.041462][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014958490093732735

[2020-01-27 14:31:40.041904][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.582954409767996e-07, Discarded policy loss value: -4.034957900624016

[2020-01-27 14:31:40.761655][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 292.29365649329515

[2020-01-27 14:31:40.766759][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:31:44.157430][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.03381219 -0.03760537 -0.         ...  0.13457698 -0.21132726
  0.07675028]

[2020-01-27 14:31:44.157813][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:44.550054][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.08358522  0.19811307  0.         ...  0.1939943   0.11676007
 -0.31075437], shape=(4547,), dtype=float64)

[2020-01-27 14:31:44.630087][__main__.TRPOAgent.log][linesearch]: improvement: 0.015341634262138248

[2020-01-27 14:31:44.658134][__main__.TRPOAgent.log][linesearch]: improvement: 0.0025817512498735873

[2020-01-27 14:31:44.681164][__main__.TRPOAgent.log][linesearch]: improvement: -0.007383520299789781

[2020-01-27 14:31:44.706365][__main__.TRPOAgent.log][linesearch]: improvement: -0.008411496442813582

[2020-01-27 14:31:44.729993][__main__.TRPOAgent.log][linesearch]: improvement: -0.0074385164035952744

[2020-01-27 14:31:44.758803][__main__.TRPOAgent.log][linesearch]: improvement: -0.006109325619028216

[2020-01-27 14:31:44.782755][__main__.TRPOAgent.log][linesearch]: improvement: -0.004374805075261168

[2020-01-27 14:31:44.809438][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026631200562861612

[2020-01-27 14:31:44.832450][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016542802103969856

[2020-01-27 14:31:44.860726][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010002575109909762

[2020-01-27 14:31:44.861261][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.835877793275198e-07, Discarded policy loss value: -0.46608576563837567

[2020-01-27 14:31:45.605119][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.11806423516305

[2020-01-27 14:31:45.612597][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:31:48.965195][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01216349 -0.03377857 -0.         ... -0.02745834 -0.26211581
  0.28957416]

[2020-01-27 14:31:48.965587][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:49.364358][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.49437639  0.94505801  0.         ... -0.2710351  -1.22265234
  1.49368744], shape=(4547,), dtype=float64)

[2020-01-27 14:31:49.446061][__main__.TRPOAgent.log][linesearch]: improvement: -0.41221353147075046

[2020-01-27 14:31:49.476014][__main__.TRPOAgent.log][linesearch]: improvement: -0.24019084305465688

[2020-01-27 14:31:49.498447][__main__.TRPOAgent.log][linesearch]: improvement: -0.12046739132735951

[2020-01-27 14:31:49.525478][__main__.TRPOAgent.log][linesearch]: improvement: -0.05740985708284674

[2020-01-27 14:31:49.548670][__main__.TRPOAgent.log][linesearch]: improvement: -0.029652823529243477

[2020-01-27 14:31:49.576721][__main__.TRPOAgent.log][linesearch]: improvement: -0.016027586968686336

[2020-01-27 14:31:49.600349][__main__.TRPOAgent.log][linesearch]: improvement: -0.008808240379898802

[2020-01-27 14:31:49.629574][__main__.TRPOAgent.log][linesearch]: improvement: -0.005098277705406051

[2020-01-27 14:31:49.657069][__main__.TRPOAgent.log][linesearch]: improvement: -0.002965314178508649

[2020-01-27 14:31:49.680426][__main__.TRPOAgent.log][linesearch]: improvement: -0.001748285505180025

[2020-01-27 14:31:49.680877][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.93996461936674e-07, Discarded policy loss value: -3.2970634687161646

[2020-01-27 14:31:50.381447][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.32856041347986

[2020-01-27 14:31:50.384673][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 3162

[2020-01-27 14:31:52.767960][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00446998 -0.01085873 -0.         ... -0.12734155  0.00808546
  0.11925609]

[2020-01-27 14:31:52.768322][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:31:53.074469][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.56248618  0.81642709  0.         ... -0.77498722 -0.50367864
  1.27866585], shape=(4547,), dtype=float64)

[2020-01-27 14:31:53.137758][__main__.TRPOAgent.log][linesearch]: improvement: -0.2574400472702776

[2020-01-27 14:31:53.161173][__main__.TRPOAgent.log][linesearch]: improvement: -0.11920394662029965

[2020-01-27 14:31:53.184118][__main__.TRPOAgent.log][linesearch]: improvement: -0.060963673223816794

[2020-01-27 14:31:53.205841][__main__.TRPOAgent.log][linesearch]: improvement: -0.034740403848550816

[2020-01-27 14:31:53.206337][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 1.4166070334756051

[2020-01-27 14:31:53.724743][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 392.2190785687552

[2020-01-27 14:31:53.725160][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:31:53.741955][__main__.TRPOAgent.log][learning]: Episode #2

[2020-01-27 14:31:53.742304][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:31:53.787918][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:31:53.788496][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:31:53.788431][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:31:53.799910][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:31:56.522164][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1238

[2020-01-27 14:31:56.766179][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:31:56.766701][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:31:56.767413][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:31:56.768526][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:31:56.767331][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:32:00.036096][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:32:00.046685][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:32:00.047157][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:32:00.047939][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:32:00.047773][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:32:00.048894][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:32:03.186351][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:32:03.276502][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:32:03.276986][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:32:03.277544][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:32:03.277636][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:32:03.280214][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:32:06.488586][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:32:06.566453][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:32:06.567084][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:32:06.567778][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:32:06.567716][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:32:06.569347][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:32:09.942027][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:32:09.964641][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:32:09.965166][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:32:09.965844][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:32:09.965916][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:32:09.967279][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:32:13.181421][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:32:13.179590][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:32:13.182288][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:32:13.182873][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:32:13.182941][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:32:13.184966][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:32:16.446837][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:32:16.516316][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:32:16.516794][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:32:16.517284][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:32:16.517462][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:32:16.519984][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:32:19.870665][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:32:19.888091][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:32:19.888577][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:32:19.889166][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:32:19.889238][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:32:19.891081][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:32:23.185523][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1208

[2020-01-27 14:32:23.472774][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:32:23.473314][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:32:23.473934][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:32:23.474004][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:32:23.477094][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:32:27.119888][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:32:27.123463][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:32:27.123953][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:32:27.124629][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:32:27.124750][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:32:27.128427][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:32:30.910026][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:32:30.914605][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:32:30.917452][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:32:30.918401][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:32:30.918287][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:32:30.919921][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:32:34.518366][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:32:34.535428][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:32:34.536085][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:32:34.536701][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:32:34.536766][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:32:34.538981][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:32:38.167332][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:32:38.226711][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:32:38.227311][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:32:38.227924][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:32:38.227847][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:32:38.228813][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:32:40.430076][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 874

[2020-01-27 14:32:41.094240][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:32:41.094833][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:32:41.095671][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:32:41.095744][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:32:41.098005][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:32:45.320502][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:32:45.330151][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:32:45.330692][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:32:45.342989][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:32:45.853581][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:32:45.882448][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:32:45.884255][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 43820, Batch size: 4500, Number of batches: 10

[2020-01-27 14:32:45.884866][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:32:45.920146][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:32:50.258960][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.04536475 -0.04143146 -0.         ...  0.28528287 -0.16034219
 -0.12494068]

[2020-01-27 14:32:50.259378][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:32:50.670131][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.54122103  2.42708725  0.         ...  1.08072659  0.1594958
 -1.24022239], shape=(4547,), dtype=float64)

[2020-01-27 14:32:50.753550][__main__.TRPOAgent.log][linesearch]: improvement: -0.34356369229148775

[2020-01-27 14:32:50.779250][__main__.TRPOAgent.log][linesearch]: improvement: -0.18146945064946252

[2020-01-27 14:32:50.806847][__main__.TRPOAgent.log][linesearch]: improvement: -0.0921654891804522

[2020-01-27 14:32:50.832351][__main__.TRPOAgent.log][linesearch]: improvement: -0.04707391693506224

[2020-01-27 14:32:50.857145][__main__.TRPOAgent.log][linesearch]: improvement: -0.024824398826229555

[2020-01-27 14:32:50.885757][__main__.TRPOAgent.log][linesearch]: improvement: -0.013258209036537849

[2020-01-27 14:32:50.912515][__main__.TRPOAgent.log][linesearch]: improvement: -0.006858897210650028

[2020-01-27 14:32:50.945787][__main__.TRPOAgent.log][linesearch]: improvement: -0.004072350662745805

[2020-01-27 14:32:50.973101][__main__.TRPOAgent.log][linesearch]: improvement: -0.002415544535766756

[2020-01-27 14:32:50.997832][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014278794631206182

[2020-01-27 14:32:50.998275][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.855954479228453e-07, Discarded policy loss value: -8.238653288590012

[2020-01-27 14:32:51.758544][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 290.5280018155938

[2020-01-27 14:32:51.763673][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:32:55.421021][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01443348 -0.03189028 -0.         ...  0.12955557 -0.03074573
 -0.09880984]

[2020-01-27 14:32:55.421417][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:32:55.833817][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.13307561  3.06142404  0.         ...  0.1536735   0.38986371
 -0.54353721], shape=(4547,), dtype=float64)

[2020-01-27 14:32:55.914124][__main__.TRPOAgent.log][linesearch]: improvement: -0.12400377788004446

[2020-01-27 14:32:55.943383][__main__.TRPOAgent.log][linesearch]: improvement: -0.08376725894438763

[2020-01-27 14:32:55.969111][__main__.TRPOAgent.log][linesearch]: improvement: -0.052297349124698744

[2020-01-27 14:32:55.969583][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 3.8823733001319276

[2020-01-27 14:32:56.701692][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.3251095732645

[2020-01-27 14:32:56.702100][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:32:56.712515][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:33:00.280994][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.15785752 -0.09670019 -0.         ...  0.56847963 -0.41153141
 -0.15694822]

[2020-01-27 14:33:00.281391][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:00.677608][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.39237769  1.39663435  0.         ...  0.19123374 -0.57377342
  0.38253968], shape=(4547,), dtype=float64)

[2020-01-27 14:33:00.760201][__main__.TRPOAgent.log][linesearch]: improvement: 0.08212140644853783

[2020-01-27 14:33:00.789561][__main__.TRPOAgent.log][linesearch]: improvement: 0.006416518287665873

[2020-01-27 14:33:00.813925][__main__.TRPOAgent.log][linesearch]: improvement: -0.015875183763952805

[2020-01-27 14:33:00.841916][__main__.TRPOAgent.log][linesearch]: improvement: -0.017526422142198683

[2020-01-27 14:33:00.865880][__main__.TRPOAgent.log][linesearch]: improvement: -0.014409755015590164

[2020-01-27 14:33:00.894960][__main__.TRPOAgent.log][linesearch]: improvement: -0.011074276297663666

[2020-01-27 14:33:00.921588][__main__.TRPOAgent.log][linesearch]: improvement: -0.008151569033789663

[2020-01-27 14:33:00.946151][__main__.TRPOAgent.log][linesearch]: improvement: -0.0054247167423779885

[2020-01-27 14:33:00.974078][__main__.TRPOAgent.log][linesearch]: improvement: -0.003429467296659716

[2020-01-27 14:33:00.998273][__main__.TRPOAgent.log][linesearch]: improvement: -0.0022100893364687124

[2020-01-27 14:33:00.998721][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.871514317186313e-07, Discarded policy loss value: -4.497456864766375

[2020-01-27 14:33:01.752053][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.89078253629225

[2020-01-27 14:33:01.757156][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:33:05.251894][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.0414881  -0.0127452  -0.         ...  0.11992553 -0.01797878
 -0.10194675]

[2020-01-27 14:33:05.252294][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:05.648253][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 2.46678183 -2.71587047  0.         ... -3.69140829  3.98613275
 -0.29472445], shape=(4547,), dtype=float64)

[2020-01-27 14:33:05.726216][__main__.TRPOAgent.log][linesearch]: improvement: -0.30840932579571767

[2020-01-27 14:33:05.754590][__main__.TRPOAgent.log][linesearch]: improvement: -0.3060952250543503

[2020-01-27 14:33:05.780530][__main__.TRPOAgent.log][linesearch]: improvement: -0.25790735854306246

[2020-01-27 14:33:05.806748][__main__.TRPOAgent.log][linesearch]: improvement: -0.16212683720722432

[2020-01-27 14:33:05.831891][__main__.TRPOAgent.log][linesearch]: improvement: -0.08164300109511347

[2020-01-27 14:33:05.856685][__main__.TRPOAgent.log][linesearch]: improvement: -0.03443892946083693

[2020-01-27 14:33:05.857198][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 5, New policy loss value: 0.5960861200782602

[2020-01-27 14:33:06.604875][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.3156166068564

[2020-01-27 14:33:06.605317][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:33:06.612774][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:33:10.163465][__main__.TRPOAgent.log][training]: policy_gradient: [ 3.49784221e-02 -2.87498422e-02 -0.00000000e+00 ...  1.21510658e-01
 -1.21544938e-01  3.42801395e-05]

[2020-01-27 14:33:10.163898][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:10.574156][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.19435811  1.3979143   0.         ... -0.25366604  0.42664312
 -0.17297709], shape=(4547,), dtype=float64)

[2020-01-27 14:33:10.652541][__main__.TRPOAgent.log][linesearch]: improvement: -0.09711135489751754

[2020-01-27 14:33:10.681223][__main__.TRPOAgent.log][linesearch]: improvement: -0.05103598071965787

[2020-01-27 14:33:10.705108][__main__.TRPOAgent.log][linesearch]: improvement: -0.030752717557238407

[2020-01-27 14:33:10.732285][__main__.TRPOAgent.log][linesearch]: improvement: -0.01944548642116306

[2020-01-27 14:33:10.759157][__main__.TRPOAgent.log][linesearch]: improvement: -0.01293329292454537

[2020-01-27 14:33:10.783763][__main__.TRPOAgent.log][linesearch]: improvement: -0.008943799904850946

[2020-01-27 14:33:10.810403][__main__.TRPOAgent.log][linesearch]: improvement: -0.006090232962467246

[2020-01-27 14:33:10.834493][__main__.TRPOAgent.log][linesearch]: improvement: -0.004198224951926788

[2020-01-27 14:33:10.863393][__main__.TRPOAgent.log][linesearch]: improvement: -0.002748675350925378

[2020-01-27 14:33:10.886695][__main__.TRPOAgent.log][linesearch]: improvement: -0.001796153679632928

[2020-01-27 14:33:10.887157][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.74423134603495e-07, Discarded policy loss value: -1.9007436169109415

[2020-01-27 14:33:11.639030][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 290.5014053875881

[2020-01-27 14:33:11.644317][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:33:15.054550][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00886834  0.00076112 -0.         ... -0.02028481 -0.03335281
  0.05363762]

[2020-01-27 14:33:15.054955][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:15.468211][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.03273446  0.832631    0.         ... -0.56456696  0.43740298
  0.12716397], shape=(4547,), dtype=float64)

[2020-01-27 14:33:15.546545][__main__.TRPOAgent.log][linesearch]: improvement: -0.06931424770087044

[2020-01-27 14:33:15.578382][__main__.TRPOAgent.log][linesearch]: improvement: -0.04106454044441371

[2020-01-27 14:33:15.603841][__main__.TRPOAgent.log][linesearch]: improvement: -0.024922639507566346

[2020-01-27 14:33:15.633940][__main__.TRPOAgent.log][linesearch]: improvement: -0.016602269825349958

[2020-01-27 14:33:15.662314][__main__.TRPOAgent.log][linesearch]: improvement: -0.011025490174826513

[2020-01-27 14:33:15.687441][__main__.TRPOAgent.log][linesearch]: improvement: -0.007057554534990129

[2020-01-27 14:33:15.711279][__main__.TRPOAgent.log][linesearch]: improvement: -0.004217615195756197

[2020-01-27 14:33:15.738321][__main__.TRPOAgent.log][linesearch]: improvement: -0.002318605872287882

[2020-01-27 14:33:15.764794][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014752820717856552

[2020-01-27 14:33:15.792912][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011451758427924452

[2020-01-27 14:33:15.793455][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.290742955370732e-07, Discarded policy loss value: -0.2907396154845147

[2020-01-27 14:33:16.555277][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 291.6031196235548

[2020-01-27 14:33:16.560176][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:33:20.017081][__main__.TRPOAgent.log][training]: policy_gradient: [-0.05721771  0.11470022 -0.         ... -0.13805407  0.27761319
 -0.13955913]

[2020-01-27 14:33:20.017468][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:20.423432][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.01264899 -0.05656713  0.         ... -0.24753349  0.04753078
  0.20000271], shape=(4547,), dtype=float64)

[2020-01-27 14:33:20.502530][__main__.TRPOAgent.log][linesearch]: improvement: -0.10049527902654276

[2020-01-27 14:33:20.503047][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 3.6914146171618696

[2020-01-27 14:33:21.235169][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.7176733341032

[2020-01-27 14:33:21.235568][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:33:21.245889][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:33:24.674175][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.28436404 -0.30719537 -0.         ...  0.38895842 -0.43636615
  0.04740773]

[2020-01-27 14:33:24.674561][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:25.059432][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.17822541 -0.29329504  0.         ... -0.0464324   0.00636186
  0.04007054], shape=(4547,), dtype=float64)

[2020-01-27 14:33:25.143755][__main__.TRPOAgent.log][linesearch]: improvement: -0.09257897533909887

[2020-01-27 14:33:25.169787][__main__.TRPOAgent.log][linesearch]: improvement: -0.060901291911227506

[2020-01-27 14:33:25.197728][__main__.TRPOAgent.log][linesearch]: improvement: -0.04100785975366694

[2020-01-27 14:33:25.224060][__main__.TRPOAgent.log][linesearch]: improvement: -0.02793012384760285

[2020-01-27 14:33:25.253601][__main__.TRPOAgent.log][linesearch]: improvement: -0.018405959094598856

[2020-01-27 14:33:25.282781][__main__.TRPOAgent.log][linesearch]: improvement: -0.011745255764290974

[2020-01-27 14:33:25.307870][__main__.TRPOAgent.log][linesearch]: improvement: -0.007614309325149726

[2020-01-27 14:33:25.334970][__main__.TRPOAgent.log][linesearch]: improvement: -0.0049364301318419734

[2020-01-27 14:33:25.360997][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030466543286697956

[2020-01-27 14:33:25.389432][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018388029434976971

[2020-01-27 14:33:25.390074][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.150757750421314e-07, Discarded policy loss value: -3.2933815694805273

[2020-01-27 14:33:26.134276][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.0861202168761

[2020-01-27 14:33:26.139054][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:33:29.596292][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.04240282 -0.04668838 -0.         ...  0.1598063  -0.10181477
 -0.05799153]

[2020-01-27 14:33:29.596685][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:29.994457][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.1665814   0.3048485   0.         ...  1.8694691  -0.87227537
 -0.99719374], shape=(4547,), dtype=float64)

[2020-01-27 14:33:30.081458][__main__.TRPOAgent.log][linesearch]: improvement: -0.40309995067243654

[2020-01-27 14:33:30.106054][__main__.TRPOAgent.log][linesearch]: improvement: -0.2183713401497308

[2020-01-27 14:33:30.134489][__main__.TRPOAgent.log][linesearch]: improvement: -0.10090767695838965

[2020-01-27 14:33:30.159284][__main__.TRPOAgent.log][linesearch]: improvement: -0.049325328706177674

[2020-01-27 14:33:30.188259][__main__.TRPOAgent.log][linesearch]: improvement: -0.02599425872001615

[2020-01-27 14:33:30.214307][__main__.TRPOAgent.log][linesearch]: improvement: -0.013858378574447494

[2020-01-27 14:33:30.241278][__main__.TRPOAgent.log][linesearch]: improvement: -0.007969609695437363

[2020-01-27 14:33:30.267636][__main__.TRPOAgent.log][linesearch]: improvement: -0.004678630808040651

[2020-01-27 14:33:30.291625][__main__.TRPOAgent.log][linesearch]: improvement: -0.002816660719090258

[2020-01-27 14:33:30.320034][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017247551921095727

[2020-01-27 14:33:30.320701][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.192619039955336e-07, Discarded policy loss value: -1.0715458591411173

[2020-01-27 14:33:31.056365][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.16735859349143

[2020-01-27 14:33:31.061109][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 3320

[2020-01-27 14:33:33.603187][__main__.TRPOAgent.log][training]: policy_gradient: [-0.33266363  0.29940261 -0.         ... -0.58055246  0.46737324
  0.11317922]

[2020-01-27 14:33:33.603565][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:33:33.928025][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.08790014  0.24093847  0.         ... -0.34346929  0.7657835
 -0.42231421], shape=(4547,), dtype=float64)

[2020-01-27 14:33:33.993037][__main__.TRPOAgent.log][linesearch]: improvement: -0.23739576509457772

[2020-01-27 14:33:33.993589][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 2.5985880596967728

[2020-01-27 14:33:34.513931][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 374.18149718889066

[2020-01-27 14:33:34.514317][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:33:34.533245][__main__.TRPOAgent.log][learning]: Episode #3

[2020-01-27 14:33:34.533587][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:33:34.579552][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:33:34.580121][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:33:34.580274][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:33:34.581823][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:33:38.130636][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:33:38.210156][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:33:38.210708][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:33:38.211440][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:33:38.211341][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:33:38.212498][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:33:41.562960][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:33:41.579660][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:33:41.580505][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:33:41.581273][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:33:41.581461][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:33:41.585113][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:33:45.356779][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:33:45.416842][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:33:45.417398][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:33:45.418079][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:33:45.418151][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:33:45.420690][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:33:49.375964][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:33:49.387368][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:33:49.387878][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:33:49.388392][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:33:49.388465][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:33:49.391539][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:33:52.494472][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1091

[2020-01-27 14:33:52.963624][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:33:52.964205][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:33:52.964954][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:33:52.965028][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:33:52.966966][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:33:56.269884][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:33:56.291668][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:33:56.292168][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:33:56.292624][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:33:56.292690][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:33:56.294614][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:33:59.618317][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:33:59.632916][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:33:59.633477][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:33:59.634266][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:33:59.635252][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:33:59.634181][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:34:02.951978][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:34:03.004063][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:34:03.004530][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:34:03.005083][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:34:03.005322][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:34:03.008622][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:34:06.452572][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:34:06.504037][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:34:06.504587][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:34:06.505266][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:34:06.505185][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:34:06.506292][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:34:10.185016][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:34:10.230112][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:34:10.230620][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:34:10.231156][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:34:10.231334][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:34:10.234692][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:34:13.950693][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:34:13.959283][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:34:13.959722][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:34:13.960318][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:34:13.960386][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:34:13.962072][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:34:17.503784][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:34:17.549251][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1488

[2020-01-27 14:34:17.549646][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:34:17.550105][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:34:17.550188][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:34:17.553986][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:34:20.903398][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:34:20.912130][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:34:20.912662][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:34:20.913166][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:34:20.913329][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:34:20.915623][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:34:24.176234][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:34:24.216787][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:34:24.217338][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:34:24.217960][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:34:24.218032][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:34:24.220048][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:34:27.562557][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:34:27.566638][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:34:27.567171][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:34:27.578732][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:34:28.067343][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:34:28.093321][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:34:28.094798][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 44579, Batch size: 4500, Number of batches: 10

[2020-01-27 14:34:28.095218][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:34:28.124799][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:34:31.541989][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.1688481  -0.09912671 -0.         ...  0.3212299  -0.12864859
 -0.19258131]

[2020-01-27 14:34:31.542394][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:34:31.944431][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.0939986   1.4162962   0.         ... -0.84156323 -0.30637068
  1.1479339 ], shape=(4547,), dtype=float64)

[2020-01-27 14:34:32.027583][__main__.TRPOAgent.log][linesearch]: improvement: -0.07515657252351904

[2020-01-27 14:34:32.054347][__main__.TRPOAgent.log][linesearch]: improvement: -0.0549520081734447

[2020-01-27 14:34:32.079777][__main__.TRPOAgent.log][linesearch]: improvement: -0.037967716436331234

[2020-01-27 14:34:32.106439][__main__.TRPOAgent.log][linesearch]: improvement: -0.025700549191463917

[2020-01-27 14:34:32.132129][__main__.TRPOAgent.log][linesearch]: improvement: -0.01750937195640878

[2020-01-27 14:34:32.159680][__main__.TRPOAgent.log][linesearch]: improvement: -0.012046741607341094

[2020-01-27 14:34:32.185982][__main__.TRPOAgent.log][linesearch]: improvement: -0.0076537647089143235

[2020-01-27 14:34:32.213159][__main__.TRPOAgent.log][linesearch]: improvement: -0.004663116652520838

[2020-01-27 14:34:32.240167][__main__.TRPOAgent.log][linesearch]: improvement: -0.0027917519025058013

[2020-01-27 14:34:32.267789][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016794126154544564

[2020-01-27 14:34:32.268241][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.231149155572953e-07, Discarded policy loss value: -2.1893806157333566

[2020-01-27 14:34:33.021726][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.7809041234833

[2020-01-27 14:34:33.028410][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:34:36.757813][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.28009642 -0.3193248  -0.         ...  0.05720887 -0.35764092
  0.30043204]

[2020-01-27 14:34:36.758303][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:34:37.154947][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.0432561   0.72939162  0.         ...  0.19777775 -0.80438429
  0.60660654], shape=(4547,), dtype=float64)

[2020-01-27 14:34:37.233332][__main__.TRPOAgent.log][linesearch]: improvement: -0.20473877581454358

[2020-01-27 14:34:37.262346][__main__.TRPOAgent.log][linesearch]: improvement: -0.12344893073495422

[2020-01-27 14:34:37.287199][__main__.TRPOAgent.log][linesearch]: improvement: -0.0697204967723386

[2020-01-27 14:34:37.287644][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 2.050783393396673

[2020-01-27 14:34:38.035966][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.39619890113806

[2020-01-27 14:34:38.036376][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:34:38.043782][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:34:41.463723][__main__.TRPOAgent.log][training]: policy_gradient: [-0.34208623  0.38751824 -0.         ... -0.07799893  0.31690128
 -0.23890235]

[2020-01-27 14:34:41.464115][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:34:41.856056][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.02964643  1.29077281  0.         ... -0.57935374 -0.19491236
  0.7742661 ], shape=(4547,), dtype=float64)

[2020-01-27 14:34:41.937652][__main__.TRPOAgent.log][linesearch]: improvement: -0.1676451200402176

[2020-01-27 14:34:41.938177][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 2.8354841065913123

[2020-01-27 14:34:42.633925][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.492750456847

[2020-01-27 14:34:42.634328][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:34:42.641605][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:34:46.438760][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00766887 -0.02240042 -0.         ...  0.0267168  -0.19896287
  0.17224607]

[2020-01-27 14:34:46.439172][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:34:46.825371][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.37229965  0.44764971  0.         ... -0.32140125 -0.38086489
  0.70226614], shape=(4547,), dtype=float64)

[2020-01-27 14:34:46.903833][__main__.TRPOAgent.log][linesearch]: improvement: 0.0012878473564241766

[2020-01-27 14:34:46.933177][__main__.TRPOAgent.log][linesearch]: improvement: -0.009374504183955246

[2020-01-27 14:34:46.957757][__main__.TRPOAgent.log][linesearch]: improvement: -0.010764351789924742

[2020-01-27 14:34:46.984403][__main__.TRPOAgent.log][linesearch]: improvement: -0.009630588411285945

[2020-01-27 14:34:47.008633][__main__.TRPOAgent.log][linesearch]: improvement: -0.008211056108796466

[2020-01-27 14:34:47.036716][__main__.TRPOAgent.log][linesearch]: improvement: -0.0069228779388414985

[2020-01-27 14:34:47.060295][__main__.TRPOAgent.log][linesearch]: improvement: -0.005523504480347885

[2020-01-27 14:34:47.088082][__main__.TRPOAgent.log][linesearch]: improvement: -0.004341882446731393

[2020-01-27 14:34:47.114183][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028868724486850184

[2020-01-27 14:34:47.141070][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017511241865304505

[2020-01-27 14:34:47.141528][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.692319464674829e-07, Discarded policy loss value: -2.0510954553107035

[2020-01-27 14:34:47.889837][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.1430457003206

[2020-01-27 14:34:47.895193][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:34:51.438612][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.06017338 -0.00932344 -0.         ...  0.13035486 -0.12444248
 -0.00591238]

[2020-01-27 14:34:51.439021][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:34:51.914025][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.3582791   1.06537968  0.         ... -0.02002216 -0.20711483
  0.22713699], shape=(4547,), dtype=float64)

[2020-01-27 14:34:52.004753][__main__.TRPOAgent.log][linesearch]: improvement: -0.09580132140903541

[2020-01-27 14:34:52.031158][__main__.TRPOAgent.log][linesearch]: improvement: -0.05490508204545563

[2020-01-27 14:34:52.031599][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.9891353413331753

[2020-01-27 14:34:52.778857][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.8198429442136

[2020-01-27 14:34:52.779270][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:34:52.786715][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:34:56.199723][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.03850861 -0.16012346 -0.         ... -0.18454651 -0.05178648
  0.23633299]

[2020-01-27 14:34:56.200113][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:34:56.596796][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.47172714 -0.86699596  0.         ...  0.39869464 -0.01169933
 -0.38699531], shape=(4547,), dtype=float64)

[2020-01-27 14:34:56.677483][__main__.TRPOAgent.log][linesearch]: improvement: -0.09739435508507777

[2020-01-27 14:34:56.705690][__main__.TRPOAgent.log][linesearch]: improvement: -0.06058825798662126

[2020-01-27 14:34:56.706350][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.9802713517364625

[2020-01-27 14:34:57.455593][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.10843167128456

[2020-01-27 14:34:57.456004][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:34:57.463369][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:35:00.881232][__main__.TRPOAgent.log][training]: policy_gradient: [-0.1190639   0.13042595 -0.         ... -0.20939206  0.11875003
  0.09064203]

[2020-01-27 14:35:00.881629][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:35:01.267672][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.75573985  0.69970203  0.         ... -0.2299903  -0.20165526
  0.43164556], shape=(4547,), dtype=float64)

[2020-01-27 14:35:01.345622][__main__.TRPOAgent.log][linesearch]: improvement: -0.20396798615354972

[2020-01-27 14:35:01.374721][__main__.TRPOAgent.log][linesearch]: improvement: -0.09701476929930042

[2020-01-27 14:35:01.399713][__main__.TRPOAgent.log][linesearch]: improvement: -0.043459411284584526

[2020-01-27 14:35:01.424402][__main__.TRPOAgent.log][linesearch]: improvement: -0.022551985679984732

[2020-01-27 14:35:01.454264][__main__.TRPOAgent.log][linesearch]: improvement: -0.01346519231304466

[2020-01-27 14:35:01.480171][__main__.TRPOAgent.log][linesearch]: improvement: -0.008259591051532178

[2020-01-27 14:35:01.514413][__main__.TRPOAgent.log][linesearch]: improvement: -0.005236771015968644

[2020-01-27 14:35:01.540441][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032684514375169726

[2020-01-27 14:35:01.568867][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019926475516167663

[2020-01-27 14:35:01.594673][__main__.TRPOAgent.log][linesearch]: improvement: -0.001207073604631681

[2020-01-27 14:35:01.595132][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.245430459130803e-07, Discarded policy loss value: -1.4883874961824732

[2020-01-27 14:35:02.355614][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.50628675890135

[2020-01-27 14:35:02.360782][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:35:05.798722][__main__.TRPOAgent.log][training]: policy_gradient: [-0.05203032 -0.02897891 -0.         ... -0.26838804  0.13964814
  0.1287399 ]

[2020-01-27 14:35:05.799103][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:35:06.181032][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.07789141 -2.44836337  0.         ...  1.45267973  0.35640877
 -1.80908851], shape=(4547,), dtype=float64)

[2020-01-27 14:35:06.255995][__main__.TRPOAgent.log][linesearch]: improvement: -0.05740119100256891

[2020-01-27 14:35:06.281072][__main__.TRPOAgent.log][linesearch]: improvement: -0.041045795936104135

[2020-01-27 14:35:06.309398][__main__.TRPOAgent.log][linesearch]: improvement: -0.03696062477332451

[2020-01-27 14:35:06.333660][__main__.TRPOAgent.log][linesearch]: improvement: -0.0341226909787361

[2020-01-27 14:35:06.361286][__main__.TRPOAgent.log][linesearch]: improvement: -0.026259581023038603

[2020-01-27 14:35:06.389011][__main__.TRPOAgent.log][linesearch]: improvement: -0.015985971853052172

[2020-01-27 14:35:06.413422][__main__.TRPOAgent.log][linesearch]: improvement: -0.009492976885022597

[2020-01-27 14:35:06.439065][__main__.TRPOAgent.log][linesearch]: improvement: -0.005534845539198252

[2020-01-27 14:35:06.462340][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033809442776830956

[2020-01-27 14:35:06.491184][__main__.TRPOAgent.log][linesearch]: improvement: -0.002099632529407036

[2020-01-27 14:35:06.491627][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.863275234093487e-07, Discarded policy loss value: -0.7931604390874808

[2020-01-27 14:35:07.229222][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.0832206507392

[2020-01-27 14:35:07.234248][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:35:10.576496][__main__.TRPOAgent.log][training]: policy_gradient: [-0.19371858  0.29661894 -0.         ... -0.04865853  0.22385216
 -0.17519363]

[2020-01-27 14:35:10.576896][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:35:10.960010][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.70063846 -1.32579175  0.         ...  0.35733171  0.12060749
 -0.4779392 ], shape=(4547,), dtype=float64)

[2020-01-27 14:35:11.033761][__main__.TRPOAgent.log][linesearch]: improvement: 0.014514261509686932

[2020-01-27 14:35:11.059770][__main__.TRPOAgent.log][linesearch]: improvement: -0.009340911588259893

[2020-01-27 14:35:11.084646][__main__.TRPOAgent.log][linesearch]: improvement: -0.016349873581644525

[2020-01-27 14:35:11.111637][__main__.TRPOAgent.log][linesearch]: improvement: -0.014120316691809576

[2020-01-27 14:35:11.135516][__main__.TRPOAgent.log][linesearch]: improvement: -0.019026526390831577

[2020-01-27 14:35:11.162499][__main__.TRPOAgent.log][linesearch]: improvement: -0.01757669768559711

[2020-01-27 14:35:11.191859][__main__.TRPOAgent.log][linesearch]: improvement: -0.012303798630001861

[2020-01-27 14:35:11.214395][__main__.TRPOAgent.log][linesearch]: improvement: -0.007306533980498298

[2020-01-27 14:35:11.241144][__main__.TRPOAgent.log][linesearch]: improvement: -0.0043220505986196756

[2020-01-27 14:35:11.265139][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025046972401110423

[2020-01-27 14:35:11.265584][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.966141053636615e-07, Discarded policy loss value: -2.00710983442067

[2020-01-27 14:35:12.001697][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.7810657552184

[2020-01-27 14:35:12.006550][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4079

[2020-01-27 14:35:15.098989][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.10911589 -0.13537705 -0.         ...  0.13142858 -0.15587371
  0.02444513]

[2020-01-27 14:35:15.099359][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:35:15.459475][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.10969698  0.24122422  0.         ... -0.15556899  0.01574492
  0.13982407], shape=(4547,), dtype=float64)

[2020-01-27 14:35:15.538130][__main__.TRPOAgent.log][linesearch]: improvement: 0.1778868710154704

[2020-01-27 14:35:15.561709][__main__.TRPOAgent.log][linesearch]: improvement: 0.034384476286841714

[2020-01-27 14:35:15.593425][__main__.TRPOAgent.log][linesearch]: improvement: -0.0042215842523142255

[2020-01-27 14:35:15.593873][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.2668098089065455

[2020-01-27 14:35:16.252720][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 311.5280375950015

[2020-01-27 14:35:16.253140][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:35:16.273199][__main__.TRPOAgent.log][learning]: Episode #4

[2020-01-27 14:35:16.273564][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:35:16.319396][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:35:16.320131][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:35:16.319981][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:35:16.320873][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:35:18.727875][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1093

[2020-01-27 14:35:19.154462][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:35:19.155033][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:35:19.155888][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:35:19.155940][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:35:19.157981][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:35:22.373946][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:35:22.453078][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:35:22.453569][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:35:22.454179][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:35:22.454241][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:35:22.455913][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:35:24.880546][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1114

[2020-01-27 14:35:25.300426][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:35:25.301022][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:35:25.301871][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:35:25.301938][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:35:25.306512][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:35:28.608845][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:35:28.640412][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:35:28.640992][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:35:28.641720][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:35:28.641811][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:35:28.644161][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:35:30.736929][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 944

[2020-01-27 14:35:31.228665][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:35:31.229298][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:35:31.230053][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:35:31.230121][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:35:31.232163][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:35:33.478127][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 969

[2020-01-27 14:35:34.002369][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:35:34.003020][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:35:34.003872][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:35:34.003744][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:35:34.005247][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:35:37.208896][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1451

[2020-01-27 14:35:37.283752][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:35:37.284299][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:35:37.284898][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:35:37.284828][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:35:37.286082][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:35:40.517127][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:35:40.570055][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:35:40.570600][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:35:40.571332][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:35:40.571269][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:35:40.572468][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:35:43.869719][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:35:43.898288][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:35:43.898887][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:35:43.899533][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:35:43.900417][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:35:43.899468][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:35:47.161000][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:35:47.182801][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:35:47.183410][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:35:47.184103][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:35:47.184027][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:35:47.185548][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:35:50.430523][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:35:50.470294][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:35:50.471077][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:35:50.471655][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:35:50.471731][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:35:50.473333][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:35:53.957804][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:35:54.024379][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:35:54.025154][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:35:54.025865][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:35:54.025940][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:35:54.027523][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:35:57.674217][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:35:57.687624][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:35:57.688149][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:35:57.688845][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:35:57.688911][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:35:57.692484][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:36:00.374584][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1229

[2020-01-27 14:36:00.697417][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:36:00.698073][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:36:00.698930][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:36:00.698855][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:36:00.700299][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:36:04.003480][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:36:04.016183][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:36:04.016911][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:36:04.027592][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:36:04.477190][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:36:04.505490][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:36:04.506875][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 42800, Batch size: 4500, Number of batches: 10

[2020-01-27 14:36:04.507242][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:36:04.538849][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:36:07.945652][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.04921424 -0.01475134 -0.         ...  0.27732761 -0.17397047
 -0.10335714]

[2020-01-27 14:36:07.946040][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:08.330305][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.28826234  1.04059347  0.         ... -0.0334112  -0.06794855
  0.10135974], shape=(4547,), dtype=float64)

[2020-01-27 14:36:08.409243][__main__.TRPOAgent.log][linesearch]: improvement: -0.1431744986507175

[2020-01-27 14:36:08.409731][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.19713409143262672

[2020-01-27 14:36:09.144138][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.7418941861273

[2020-01-27 14:36:09.144537][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:36:09.151770][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:36:12.578424][__main__.TRPOAgent.log][training]: policy_gradient: [-0.10148299  0.2062671  -0.         ... -0.2303886   0.13165898
  0.09872961]

[2020-01-27 14:36:12.578806][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:12.970264][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.36067688  0.23140386  0.         ...  0.31294017  0.24394347
 -0.55688364], shape=(4547,), dtype=float64)

[2020-01-27 14:36:13.047160][__main__.TRPOAgent.log][linesearch]: improvement: -0.16424057375492823

[2020-01-27 14:36:13.076034][__main__.TRPOAgent.log][linesearch]: improvement: -0.0961590602182057

[2020-01-27 14:36:13.076484][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.4172525039232423

[2020-01-27 14:36:13.788613][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.9787061146779

[2020-01-27 14:36:13.789019][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:36:13.796355][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:36:17.323328][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.08995482 -0.02165456 -0.         ...  0.08712006 -0.02989191
 -0.05722815]

[2020-01-27 14:36:17.323924][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:17.723820][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.11286895  0.07397829  0.         ...  0.09922784 -0.03388009
 -0.06534775], shape=(4547,), dtype=float64)

[2020-01-27 14:36:17.800524][__main__.TRPOAgent.log][linesearch]: improvement: -0.09118125149456685

[2020-01-27 14:36:17.829409][__main__.TRPOAgent.log][linesearch]: improvement: -0.061823006302013706

[2020-01-27 14:36:17.856636][__main__.TRPOAgent.log][linesearch]: improvement: -0.03982519788756167

[2020-01-27 14:36:17.881486][__main__.TRPOAgent.log][linesearch]: improvement: -0.024302506601356078

[2020-01-27 14:36:17.909912][__main__.TRPOAgent.log][linesearch]: improvement: -0.014433175741916104

[2020-01-27 14:36:17.935759][__main__.TRPOAgent.log][linesearch]: improvement: -0.008696625652144574

[2020-01-27 14:36:17.963822][__main__.TRPOAgent.log][linesearch]: improvement: -0.005276135535407678

[2020-01-27 14:36:17.991161][__main__.TRPOAgent.log][linesearch]: improvement: -0.003172746510264224

[2020-01-27 14:36:18.015915][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019036325771658036

[2020-01-27 14:36:18.044586][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011434242287828855

[2020-01-27 14:36:18.045055][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.150079626988307e-07, Discarded policy loss value: -0.49719430120265595

[2020-01-27 14:36:18.789996][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.74232716300173

[2020-01-27 14:36:18.795273][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:36:22.243706][__main__.TRPOAgent.log][training]: policy_gradient: [-0.34288556  0.14915349 -0.         ...  0.08852464  0.18495006
 -0.27347471]

[2020-01-27 14:36:22.244100][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:22.647598][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.57713092  1.0558918   0.         ...  0.71613911 -0.0121319
 -0.70400721], shape=(4547,), dtype=float64)

[2020-01-27 14:36:22.729579][__main__.TRPOAgent.log][linesearch]: improvement: -0.14358460622025593

[2020-01-27 14:36:22.758597][__main__.TRPOAgent.log][linesearch]: improvement: -0.08372830283728261

[2020-01-27 14:36:22.759060][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 3.270845477199299

[2020-01-27 14:36:23.509007][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.43618193535224

[2020-01-27 14:36:23.509430][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:36:23.517044][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:36:27.293794][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.34394569 -0.09628957 -0.         ...  0.38284544 -0.27645762
 -0.10638781]

[2020-01-27 14:36:27.294177][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:27.684815][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.17242576 -0.26296989  0.         ...  0.25880556 -0.06009367
 -0.19871189], shape=(4547,), dtype=float64)

[2020-01-27 14:36:27.762665][__main__.TRPOAgent.log][linesearch]: improvement: -0.14201727779557022

[2020-01-27 14:36:27.795048][__main__.TRPOAgent.log][linesearch]: improvement: -0.08925828478785897

[2020-01-27 14:36:27.818477][__main__.TRPOAgent.log][linesearch]: improvement: -0.056013115013020254

[2020-01-27 14:36:27.895220][__main__.TRPOAgent.log][linesearch]: improvement: -0.035025567638106325

[2020-01-27 14:36:27.923011][__main__.TRPOAgent.log][linesearch]: improvement: -0.02151362752761371

[2020-01-27 14:36:27.946873][__main__.TRPOAgent.log][linesearch]: improvement: -0.013253569828755751

[2020-01-27 14:36:27.974037][__main__.TRPOAgent.log][linesearch]: improvement: -0.00799209210525581

[2020-01-27 14:36:27.999206][__main__.TRPOAgent.log][linesearch]: improvement: -0.004799785632436826

[2020-01-27 14:36:28.027538][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028728077387356055

[2020-01-27 14:36:28.052796][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017155701163686365

[2020-01-27 14:36:28.053349][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.66915317609909e-07, Discarded policy loss value: -0.43815166083406226

[2020-01-27 14:36:28.793162][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.015603626241

[2020-01-27 14:36:28.798277][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:36:32.290406][__main__.TRPOAgent.log][training]: policy_gradient: [-0.1046734  -0.1221657  -0.         ... -0.32083762 -0.02121553
  0.34205315]

[2020-01-27 14:36:32.290799][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:32.688574][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.24029078 -0.85628961  0.         ... -0.37634147 -0.04782927
  0.42417074], shape=(4547,), dtype=float64)

[2020-01-27 14:36:32.771908][__main__.TRPOAgent.log][linesearch]: improvement: -0.10031620793083817

[2020-01-27 14:36:32.772365][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.2860208761273133

[2020-01-27 14:36:33.502043][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.5192492872934

[2020-01-27 14:36:33.502442][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:36:33.509869][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:36:37.077652][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.1691817   0.00591846 -0.         ...  0.1220489  -0.05123084
 -0.07081806]

[2020-01-27 14:36:37.078037][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:37.459628][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.46603658 -0.61643249  0.         ... -0.58729707 -0.02981914
  0.61711621], shape=(4547,), dtype=float64)

[2020-01-27 14:36:37.532614][__main__.TRPOAgent.log][linesearch]: improvement: -0.06401775351328254

[2020-01-27 14:36:37.554542][__main__.TRPOAgent.log][linesearch]: improvement: -0.03872548836186429

[2020-01-27 14:36:37.554982][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.5852000134312068

[2020-01-27 14:36:38.233788][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 291.55688408435685

[2020-01-27 14:36:38.234137][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:36:38.240350][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:36:41.581132][__main__.TRPOAgent.log][training]: policy_gradient: [-0.18566612  0.27768967 -0.         ... -0.10563811  0.08511017
  0.02052793]

[2020-01-27 14:36:41.581523][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:41.972968][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.16242171  0.72500739  0.         ... -0.46392755 -0.08478792
  0.54871547], shape=(4547,), dtype=float64)

[2020-01-27 14:36:42.045762][__main__.TRPOAgent.log][linesearch]: improvement: -0.05530455606291573

[2020-01-27 14:36:42.067157][__main__.TRPOAgent.log][linesearch]: improvement: -0.03163140187673008

[2020-01-27 14:36:42.090455][__main__.TRPOAgent.log][linesearch]: improvement: -0.03005247027478597

[2020-01-27 14:36:42.114015][__main__.TRPOAgent.log][linesearch]: improvement: -0.024563119476120443

[2020-01-27 14:36:42.134828][__main__.TRPOAgent.log][linesearch]: improvement: -0.016284206272050517

[2020-01-27 14:36:42.154968][__main__.TRPOAgent.log][linesearch]: improvement: -0.010156250726887084

[2020-01-27 14:36:42.176027][__main__.TRPOAgent.log][linesearch]: improvement: -0.006149139781353119

[2020-01-27 14:36:42.200342][__main__.TRPOAgent.log][linesearch]: improvement: -0.003633762998842771

[2020-01-27 14:36:42.225080][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021401378772551993

[2020-01-27 14:36:42.248396][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013031652798689564

[2020-01-27 14:36:42.248830][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.12239257501754e-07, Discarded policy loss value: -1.5329180806344775

[2020-01-27 14:36:42.986367][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 359.5924887298889

[2020-01-27 14:36:42.991057][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:36:46.371522][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.11192865 -0.09838361 -0.         ...  0.00931718 -0.03747818
  0.028161  ]

[2020-01-27 14:36:46.371903][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:46.764254][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.1898008   0.82549845  0.         ... -0.04131823  0.08143667
 -0.04011844], shape=(4547,), dtype=float64)

[2020-01-27 14:36:46.835781][__main__.TRPOAgent.log][linesearch]: improvement: 0.010128171945121944

[2020-01-27 14:36:46.857485][__main__.TRPOAgent.log][linesearch]: improvement: -0.005054316634755196

[2020-01-27 14:36:46.857945][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.7198919880386558

[2020-01-27 14:36:47.575553][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.1000052115686

[2020-01-27 14:36:47.575949][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:36:47.581094][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 2300

[2020-01-27 14:36:49.328896][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.73731157 -1.00936571 -0.         ...  0.22978146 -0.47437113
  0.24458967]

[2020-01-27 14:36:49.329264][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:36:49.590357][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.66099934 -1.05420593  0.         ...  1.00471271  0.03634331
 -1.04105602], shape=(4547,), dtype=float64)

[2020-01-27 14:36:49.648053][__main__.TRPOAgent.log][linesearch]: improvement: -0.08145957830669559

[2020-01-27 14:36:49.648483][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 3.730228425534638

[2020-01-27 14:36:50.024236][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 356.3688391135979

[2020-01-27 14:36:50.024629][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:36:50.043768][__main__.TRPOAgent.log][learning]: Episode #5

[2020-01-27 14:36:50.044274][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:36:50.083924][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:36:50.084578][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:36:50.084769][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:36:50.086767][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:36:53.333430][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:36:53.358288][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:36:53.358830][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:36:53.359527][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:36:53.359595][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:36:53.361892][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:36:56.554521][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:36:56.572875][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:36:56.573496][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:36:56.574475][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:36:56.574543][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:36:56.576150][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:36:59.288939][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1211

[2020-01-27 14:36:59.383105][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1383

[2020-01-27 14:36:59.383614][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:36:59.384401][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:36:59.384513][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:36:59.386338][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:37:00.937828][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 711

[2020-01-27 14:37:01.679391][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:37:01.679977][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:37:01.680877][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:37:01.680971][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:37:01.683453][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:37:05.992130][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:37:06.080318][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:37:06.081269][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:37:06.082266][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:37:06.082559][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:37:06.088294][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:37:09.622258][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:37:09.633881][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:37:09.634419][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:37:09.635046][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:37:09.635112][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:37:09.638168][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:37:11.424363][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 717

[2020-01-27 14:37:12.175688][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1478

[2020-01-27 14:37:12.176247][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:37:12.176910][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:37:12.176833][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:37:12.178884][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:37:13.884779][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 690

[2020-01-27 14:37:14.758126][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:37:14.758727][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:37:14.759669][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:37:14.759609][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:37:14.760803][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:37:18.106561][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:37:18.190862][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:37:18.191436][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:37:18.192185][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:37:18.192120][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:37:18.193179][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:37:19.887131][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 702

[2020-01-27 14:37:20.727856][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1498

[2020-01-27 14:37:20.728454][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:37:20.729174][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:37:20.729282][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:37:20.732850][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:37:24.208150][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1486

[2020-01-27 14:37:24.211683][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:37:24.212680][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:37:24.213469][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:37:24.214299][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:37:24.213399][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:37:26.451720][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 967

[2020-01-27 14:37:27.044097][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:37:27.044713][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:37:27.045543][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:37:27.045468][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:37:27.046685][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:37:30.484527][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:37:30.500877][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:37:30.501509][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:37:30.502434][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:37:30.502564][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:37:30.504703][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:37:33.349530][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1302

[2020-01-27 14:37:33.543827][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:37:33.544390][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:37:33.545126][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:37:33.545185][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:37:33.547283][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:37:36.724642][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:37:36.814700][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:37:36.815229][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:37:36.824725][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:37:37.154071][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:37:37.177011][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:37:37.178380][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 40645, Batch size: 4500, Number of batches: 10

[2020-01-27 14:37:37.178733][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:37:37.204853][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:37:40.651431][__main__.TRPOAgent.log][training]: policy_gradient: [-0.40571464  0.20217377 -0.         ... -0.12701053  0.27237907
 -0.14536854]

[2020-01-27 14:37:40.651834][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:37:41.040698][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.67895481 -0.80831916  0.         ...  0.73433181 -0.35594308
 -0.37838873], shape=(4547,), dtype=float64)

[2020-01-27 14:37:41.112099][__main__.TRPOAgent.log][linesearch]: improvement: -0.011573521913312401

[2020-01-27 14:37:41.145706][__main__.TRPOAgent.log][linesearch]: improvement: -0.02431698033211016

[2020-01-27 14:37:41.169397][__main__.TRPOAgent.log][linesearch]: improvement: -0.03299905977676998

[2020-01-27 14:37:41.190639][__main__.TRPOAgent.log][linesearch]: improvement: -0.027524422702281948

[2020-01-27 14:37:41.212303][__main__.TRPOAgent.log][linesearch]: improvement: -0.01857862989664949

[2020-01-27 14:37:41.234285][__main__.TRPOAgent.log][linesearch]: improvement: -0.011564051534716624

[2020-01-27 14:37:41.257229][__main__.TRPOAgent.log][linesearch]: improvement: -0.0070702365907724385

[2020-01-27 14:37:41.280101][__main__.TRPOAgent.log][linesearch]: improvement: -0.004264380035466697

[2020-01-27 14:37:41.302059][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025730098981697758

[2020-01-27 14:37:41.323764][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015370996220938071

[2020-01-27 14:37:41.324204][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.584278410650882e-07, Discarded policy loss value: -1.0786363645587353

[2020-01-27 14:37:42.053626][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.044371918937

[2020-01-27 14:37:42.058701][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:37:45.518629][__main__.TRPOAgent.log][training]: policy_gradient: [-0.29587401  0.16886006 -0.         ...  0.02530024  0.20227602
 -0.22757626]

[2020-01-27 14:37:45.519012][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:37:45.922346][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.28919462  0.37833008  0.         ...  0.51510783 -0.03913848
 -0.47596935], shape=(4547,), dtype=float64)

[2020-01-27 14:37:45.994562][__main__.TRPOAgent.log][linesearch]: improvement: -0.10378925286443622

[2020-01-27 14:37:46.016275][__main__.TRPOAgent.log][linesearch]: improvement: -0.0649586247233761

[2020-01-27 14:37:46.039416][__main__.TRPOAgent.log][linesearch]: improvement: -0.03883639476222239

[2020-01-27 14:37:46.061371][__main__.TRPOAgent.log][linesearch]: improvement: -0.023287426443349524

[2020-01-27 14:37:46.083613][__main__.TRPOAgent.log][linesearch]: improvement: -0.014092921299506622

[2020-01-27 14:37:46.104471][__main__.TRPOAgent.log][linesearch]: improvement: -0.008531940569931562

[2020-01-27 14:37:46.126528][__main__.TRPOAgent.log][linesearch]: improvement: -0.005179135022277681

[2020-01-27 14:37:46.148373][__main__.TRPOAgent.log][linesearch]: improvement: -0.003133688923861788

[2020-01-27 14:37:46.170534][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018924982965634607

[2020-01-27 14:37:46.191107][__main__.TRPOAgent.log][linesearch]: improvement: -0.001145615532396449

[2020-01-27 14:37:46.191536][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.532475787147043e-07, Discarded policy loss value: -0.33940509505160965

[2020-01-27 14:37:46.911207][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.7946644395177

[2020-01-27 14:37:46.916254][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:37:50.563096][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.13747168 -0.03055189 -0.         ...  0.01595982 -0.03485479
  0.01889498]

[2020-01-27 14:37:50.563481][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:37:50.975694][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.27805596  1.75011238  0.         ...  0.54538052 -0.13523155
 -0.41014898], shape=(4547,), dtype=float64)

[2020-01-27 14:37:51.070367][__main__.TRPOAgent.log][linesearch]: improvement: -0.051947705287098356

[2020-01-27 14:37:51.104492][__main__.TRPOAgent.log][linesearch]: improvement: -0.042533301035076465

[2020-01-27 14:37:51.138310][__main__.TRPOAgent.log][linesearch]: improvement: -0.031637458638599014

[2020-01-27 14:37:51.171422][__main__.TRPOAgent.log][linesearch]: improvement: -0.019948013014526117

[2020-01-27 14:37:51.203789][__main__.TRPOAgent.log][linesearch]: improvement: -0.01161264068399992

[2020-01-27 14:37:51.236460][__main__.TRPOAgent.log][linesearch]: improvement: -0.007053831415188838

[2020-01-27 14:37:51.269296][__main__.TRPOAgent.log][linesearch]: improvement: -0.004331987546269872

[2020-01-27 14:37:51.304007][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026790757149846023

[2020-01-27 14:37:51.334796][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016416773867365675

[2020-01-27 14:37:51.368931][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010042540912842401

[2020-01-27 14:37:51.369536][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.299324987958642e-07, Discarded policy loss value: -0.24591571148105182

[2020-01-27 14:37:52.125830][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.86700427018854

[2020-01-27 14:37:52.130810][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:37:55.749410][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.19408235 -0.13960342 -0.         ...  0.12639465 -0.33154405
  0.2051494 ]

[2020-01-27 14:37:55.749795][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:37:56.141162][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.38340099  1.05941488  0.         ... -0.26395151  0.0350305
  0.22892101], shape=(4547,), dtype=float64)

[2020-01-27 14:37:56.230852][__main__.TRPOAgent.log][linesearch]: improvement: -0.13127037608230196

[2020-01-27 14:37:56.267491][__main__.TRPOAgent.log][linesearch]: improvement: -0.0823147779028286

[2020-01-27 14:37:56.268461][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 4.954355569455594

[2020-01-27 14:37:57.009141][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 364.477525339162

[2020-01-27 14:37:57.009538][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:37:57.016861][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:38:00.576640][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.18577516 -0.09484453 -0.         ...  0.11303758 -0.2070417
  0.09400412]

[2020-01-27 14:38:00.577056][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:38:00.999067][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.85331653  1.36396086  0.         ...  0.32478235  0.1607251
 -0.48550745], shape=(4547,), dtype=float64)

[2020-01-27 14:38:01.105509][__main__.TRPOAgent.log][linesearch]: improvement: 0.2104201269604617

[2020-01-27 14:38:01.140105][__main__.TRPOAgent.log][linesearch]: improvement: 0.09741524981350291

[2020-01-27 14:38:01.177600][__main__.TRPOAgent.log][linesearch]: improvement: 0.03778970291925687

[2020-01-27 14:38:01.178084][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.5379861135223344

[2020-01-27 14:38:02.017897][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 364.27054041874504

[2020-01-27 14:38:02.018369][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:38:02.026442][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:38:05.561445][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07349292 -0.         ...  0.27052659 -0.08258382
 -0.18794276]

[2020-01-27 14:38:05.561825][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:38:05.966026][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.55968902  0.         ...  1.0064375  -0.28377327
 -0.72266423], shape=(4547,), dtype=float64)

[2020-01-27 14:38:06.048069][__main__.TRPOAgent.log][linesearch]: improvement: -0.21065043972210362

[2020-01-27 14:38:06.079857][__main__.TRPOAgent.log][linesearch]: improvement: -0.11688070161422415

[2020-01-27 14:38:06.110757][__main__.TRPOAgent.log][linesearch]: improvement: -0.06661162534688714

[2020-01-27 14:38:06.141682][__main__.TRPOAgent.log][linesearch]: improvement: -0.038880281620617296

[2020-01-27 14:38:06.174959][__main__.TRPOAgent.log][linesearch]: improvement: -0.022953431931353352

[2020-01-27 14:38:06.211459][__main__.TRPOAgent.log][linesearch]: improvement: -0.013615895154643232

[2020-01-27 14:38:06.240296][__main__.TRPOAgent.log][linesearch]: improvement: -0.008100357694586169

[2020-01-27 14:38:06.273549][__main__.TRPOAgent.log][linesearch]: improvement: -0.004840016971971339

[2020-01-27 14:38:06.306402][__main__.TRPOAgent.log][linesearch]: improvement: -0.0029024951102194496

[2020-01-27 14:38:06.340441][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017389836802903513

[2020-01-27 14:38:06.341022][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.090039885779173e-07, Discarded policy loss value: -1.5671105575612718

[2020-01-27 14:38:07.097940][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.4574691473029

[2020-01-27 14:38:07.103096][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:38:10.637349][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10408584 -0.         ...  0.0431952   0.14047354
 -0.18366873]

[2020-01-27 14:38:10.637753][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:38:11.028662][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.55051338  0.         ...  1.0143447  -0.30005186
 -0.71429284], shape=(4547,), dtype=float64)

[2020-01-27 14:38:11.120106][__main__.TRPOAgent.log][linesearch]: improvement: -0.08403709374626356

[2020-01-27 14:38:11.156681][__main__.TRPOAgent.log][linesearch]: improvement: -0.07126543144842756

[2020-01-27 14:38:11.186964][__main__.TRPOAgent.log][linesearch]: improvement: -0.051918312270325284

[2020-01-27 14:38:11.217717][__main__.TRPOAgent.log][linesearch]: improvement: -0.03367517621971006

[2020-01-27 14:38:11.252508][__main__.TRPOAgent.log][linesearch]: improvement: -0.02020391220687001

[2020-01-27 14:38:11.289093][__main__.TRPOAgent.log][linesearch]: improvement: -0.011848460450243348

[2020-01-27 14:38:11.321377][__main__.TRPOAgent.log][linesearch]: improvement: -0.007096964042633047

[2020-01-27 14:38:11.353649][__main__.TRPOAgent.log][linesearch]: improvement: -0.004235353876825165

[2020-01-27 14:38:11.385601][__main__.TRPOAgent.log][linesearch]: improvement: -0.00253686748318227

[2020-01-27 14:38:11.418509][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015173791992735275

[2020-01-27 14:38:11.419009][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.480229349915974e-07, Discarded policy loss value: -2.26442379829019

[2020-01-27 14:38:12.206767][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.37661187795874

[2020-01-27 14:38:12.211672][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:38:15.803820][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.1104483  -0.         ...  0.00417745 -0.06854438
  0.06436693]

[2020-01-27 14:38:15.804215][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:38:16.208888][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.38154487  0.         ... -0.18998434  0.0391735
  0.15081084], shape=(4547,), dtype=float64)

[2020-01-27 14:38:16.295882][__main__.TRPOAgent.log][linesearch]: improvement: -0.05522009716881704

[2020-01-27 14:38:16.330755][__main__.TRPOAgent.log][linesearch]: improvement: -0.029037718087884024

[2020-01-27 14:38:16.360548][__main__.TRPOAgent.log][linesearch]: improvement: -0.017325870903465157

[2020-01-27 14:38:16.394854][__main__.TRPOAgent.log][linesearch]: improvement: -0.012851064261387046

[2020-01-27 14:38:16.430591][__main__.TRPOAgent.log][linesearch]: improvement: -0.007951261151434963

[2020-01-27 14:38:16.455362][__main__.TRPOAgent.log][linesearch]: improvement: -0.005040099705732715

[2020-01-27 14:38:16.492608][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032146948259401675

[2020-01-27 14:38:16.527974][__main__.TRPOAgent.log][linesearch]: improvement: -0.0020599590737757634

[2020-01-27 14:38:16.561780][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012323223318968513

[2020-01-27 14:38:16.588407][__main__.TRPOAgent.log][linesearch]: improvement: -0.0007864490016420422

[2020-01-27 14:38:16.588941][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.331883693217972e-07, Discarded policy loss value: -1.2690125476904068

[2020-01-27 14:38:17.457555][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.9052826914269

[2020-01-27 14:38:17.463423][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:38:21.089336][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.0878189  -0.         ... -0.25608056 -0.0969326
  0.35301316]

[2020-01-27 14:38:21.089725][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:38:21.498766][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.35809926  0.         ...  1.82095187 -0.41339118
 -1.40756069], shape=(4547,), dtype=float64)

[2020-01-27 14:38:21.587990][__main__.TRPOAgent.log][linesearch]: improvement: -0.2653186878523247

[2020-01-27 14:38:21.623818][__main__.TRPOAgent.log][linesearch]: improvement: -0.14653481872567176

[2020-01-27 14:38:21.624426][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.23099453367477962

[2020-01-27 14:38:22.355433][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.10140279988116

[2020-01-27 14:38:22.355800][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:38:22.359019][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 145

[2020-01-27 14:38:22.516005][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07317405 -0.         ...  3.29188294  0.06005835
 -3.35194129]

[2020-01-27 14:38:22.516330][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:38:22.646867][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[  0.         -30.80622216   0.         ... -11.84233132   3.81885215
   8.02347916], shape=(4547,), dtype=float64)

[2020-01-27 14:38:22.675232][__main__.TRPOAgent.log][linesearch]: improvement: -2.0598160357790363

[2020-01-27 14:38:22.675675][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 46.4885589279652

[2020-01-27 14:38:22.741498][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 582.9870246610508

[2020-01-27 14:38:22.741894][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:38:22.761348][__main__.TRPOAgent.log][learning]: Episode #6

[2020-01-27 14:38:22.761763][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:38:22.812270][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:38:22.812925][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:38:22.812854][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:38:22.814098][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:38:25.405632][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1100

[2020-01-27 14:38:25.827186][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:38:25.827822][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:38:25.828634][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:38:25.828712][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:38:25.831107][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:38:27.263346][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 604

[2020-01-27 14:38:27.696993][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1068

[2020-01-27 14:38:27.697756][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:38:27.698441][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:38:27.698681][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:38:27.702049][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:38:31.160719][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:38:31.157681][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:38:31.161650][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:38:31.162323][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:38:31.162267][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:38:31.163257][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:38:34.089686][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1290

[2020-01-27 14:38:34.332059][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:38:34.332864][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:38:34.333515][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:38:34.333720][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:38:34.337393][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:38:37.392231][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1327

[2020-01-27 14:38:37.632045][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:38:37.633665][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:38:37.634619][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:38:37.634719][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:38:37.637512][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:38:39.145339][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 658

[2020-01-27 14:38:39.895973][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1364

[2020-01-27 14:38:39.896474][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:38:39.897143][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:38:39.897078][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:38:39.898223][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:38:43.133591][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:38:43.253380][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:38:43.253947][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:38:43.254607][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:38:43.254672][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:38:43.256721][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:38:46.383223][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1384

[2020-01-27 14:38:46.514957][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1418

[2020-01-27 14:38:46.515483][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:38:46.516325][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:38:46.516386][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:38:46.520004][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:38:48.785681][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 994

[2020-01-27 14:38:49.070603][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1238

[2020-01-27 14:38:49.071064][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:38:49.071738][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:38:49.072910][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:38:49.071653][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:38:51.522012][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1053

[2020-01-27 14:38:52.002822][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:38:52.003423][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:38:52.004217][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:38:52.004288][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:38:52.006249][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:38:54.938029][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1268

[2020-01-27 14:38:55.209688][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:38:55.210255][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:38:55.210978][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:38:55.211145][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:38:55.212392][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:38:56.594106][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 610

[2020-01-27 14:38:57.004753][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 960

[2020-01-27 14:38:57.005518][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:38:57.006293][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:38:57.006439][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:38:57.008630][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:39:00.381613][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:39:00.432584][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:39:00.433122][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:39:00.433745][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:39:00.433820][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:39:00.436047][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:39:01.876759][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 601

[2020-01-27 14:39:02.753623][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:39:02.754223][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:39:02.754848][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:39:02.754913][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:39:02.757295][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:39:05.348835][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1122

[2020-01-27 14:39:05.438889][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1178

[2020-01-27 14:39:05.439331][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:39:05.449213][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:39:05.885606][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:39:05.912657][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:39:05.914552][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 37237, Batch size: 4500, Number of batches: 9

[2020-01-27 14:39:05.914951][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:39:05.943449][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:39:09.669523][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.5543839  -0.         ... -0.26826652  0.80266695
 -0.53440042]

[2020-01-27 14:39:09.669933][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:10.092421][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -4.76082684  0.         ...  2.2876115  -1.13458257
 -1.15302893], shape=(4547,), dtype=float64)

[2020-01-27 14:39:10.184948][__main__.TRPOAgent.log][linesearch]: improvement: -0.7674802131033545

[2020-01-27 14:39:10.222380][__main__.TRPOAgent.log][linesearch]: improvement: -0.46177876057954137

[2020-01-27 14:39:10.254625][__main__.TRPOAgent.log][linesearch]: improvement: -0.27040876019662363

[2020-01-27 14:39:10.290663][__main__.TRPOAgent.log][linesearch]: improvement: -0.1626966554705831

[2020-01-27 14:39:10.324106][__main__.TRPOAgent.log][linesearch]: improvement: -0.09759738021730158

[2020-01-27 14:39:10.357284][__main__.TRPOAgent.log][linesearch]: improvement: -0.058779115877236165

[2020-01-27 14:39:10.389699][__main__.TRPOAgent.log][linesearch]: improvement: -0.035470190758665865

[2020-01-27 14:39:10.424973][__main__.TRPOAgent.log][linesearch]: improvement: -0.021355030954445908

[2020-01-27 14:39:10.459109][__main__.TRPOAgent.log][linesearch]: improvement: -0.012792183580572214

[2020-01-27 14:39:10.487293][__main__.TRPOAgent.log][linesearch]: improvement: -0.007644983175595144

[2020-01-27 14:39:10.487791][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.200248527595975e-07, Discarded policy loss value: -54.50407068847632

[2020-01-27 14:39:11.375908][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.4531485219629

[2020-01-27 14:39:11.381652][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:39:15.097361][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.24108289 -0.         ... -0.02348117 -0.11550072
  0.13898189]

[2020-01-27 14:39:15.097760][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:15.508603][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.35977052  0.         ...  0.70308191 -0.50850767
 -0.19457424], shape=(4547,), dtype=float64)

[2020-01-27 14:39:15.597686][__main__.TRPOAgent.log][linesearch]: improvement: -0.1748675563836813

[2020-01-27 14:39:15.634896][__main__.TRPOAgent.log][linesearch]: improvement: -0.10784000455273213

[2020-01-27 14:39:15.671016][__main__.TRPOAgent.log][linesearch]: improvement: -0.0666644095994614

[2020-01-27 14:39:15.702875][__main__.TRPOAgent.log][linesearch]: improvement: -0.0408030854007399

[2020-01-27 14:39:15.736610][__main__.TRPOAgent.log][linesearch]: improvement: -0.024995870520193808

[2020-01-27 14:39:15.765405][__main__.TRPOAgent.log][linesearch]: improvement: -0.015303617237450273

[2020-01-27 14:39:15.802581][__main__.TRPOAgent.log][linesearch]: improvement: -0.009328383964879339

[2020-01-27 14:39:15.834273][__main__.TRPOAgent.log][linesearch]: improvement: -0.00574673229195144

[2020-01-27 14:39:15.869720][__main__.TRPOAgent.log][linesearch]: improvement: -0.0035216556616077255

[2020-01-27 14:39:15.902493][__main__.TRPOAgent.log][linesearch]: improvement: -0.00212994465831029

[2020-01-27 14:39:15.903143][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.411676656794422e-07, Discarded policy loss value: -0.47089047712311166

[2020-01-27 14:39:16.764358][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 361.6076865101012

[2020-01-27 14:39:16.770160][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:39:20.410799][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03230629 -0.         ... -0.05326275 -0.03327342
  0.08653617]

[2020-01-27 14:39:20.411201][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:20.833533][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.06712412  0.         ...  0.15434152 -0.09318211
 -0.06115942], shape=(4547,), dtype=float64)

[2020-01-27 14:39:20.920302][__main__.TRPOAgent.log][linesearch]: improvement: -0.07493414552719457

[2020-01-27 14:39:20.952767][__main__.TRPOAgent.log][linesearch]: improvement: -0.050254493877041284

[2020-01-27 14:39:20.984839][__main__.TRPOAgent.log][linesearch]: improvement: -0.027225381772652657

[2020-01-27 14:39:21.015565][__main__.TRPOAgent.log][linesearch]: improvement: -0.014026721547999

[2020-01-27 14:39:21.047663][__main__.TRPOAgent.log][linesearch]: improvement: -0.00822640132214092

[2020-01-27 14:39:21.078754][__main__.TRPOAgent.log][linesearch]: improvement: -0.004996203254241216

[2020-01-27 14:39:21.105811][__main__.TRPOAgent.log][linesearch]: improvement: -0.003077633700111271

[2020-01-27 14:39:21.139061][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018622146689830643

[2020-01-27 14:39:21.173866][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010933378777711056

[2020-01-27 14:39:21.208741][__main__.TRPOAgent.log][linesearch]: improvement: -0.0006430420022596195

[2020-01-27 14:39:21.209377][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.5734665453015e-07, Discarded policy loss value: -2.1699294278578076

[2020-01-27 14:39:22.075146][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.7223285346124

[2020-01-27 14:39:22.083063][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:39:25.646105][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.65040096 -0.         ... -0.05322307 -0.28375985
  0.33698292]

[2020-01-27 14:39:25.646526][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:26.038664][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.96534268  0.         ... -0.37397419  0.18915708
  0.18481711], shape=(4547,), dtype=float64)

[2020-01-27 14:39:26.120961][__main__.TRPOAgent.log][linesearch]: improvement: -0.13831978915294973

[2020-01-27 14:39:26.121465][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.2507996472830079

[2020-01-27 14:39:26.889073][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.52918145357825

[2020-01-27 14:39:26.889483][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:39:26.896887][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:39:30.427179][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.09060158 -0.         ...  0.02095171 -0.18096283
  0.16001112]

[2020-01-27 14:39:30.427643][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:30.815253][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.7162076   0.         ... -0.50681799  0.22647278
  0.28034521], shape=(4547,), dtype=float64)

[2020-01-27 14:39:30.907935][__main__.TRPOAgent.log][linesearch]: improvement: -0.19009723359818342

[2020-01-27 14:39:30.945522][__main__.TRPOAgent.log][linesearch]: improvement: -0.1199900886768348

[2020-01-27 14:39:30.982150][__main__.TRPOAgent.log][linesearch]: improvement: -0.07004730998701447

[2020-01-27 14:39:31.007539][__main__.TRPOAgent.log][linesearch]: improvement: -0.039407896672353226

[2020-01-27 14:39:31.047386][__main__.TRPOAgent.log][linesearch]: improvement: -0.022612600784301717

[2020-01-27 14:39:31.074884][__main__.TRPOAgent.log][linesearch]: improvement: -0.012975958065493787

[2020-01-27 14:39:31.114747][__main__.TRPOAgent.log][linesearch]: improvement: -0.007510402622954215

[2020-01-27 14:39:31.141548][__main__.TRPOAgent.log][linesearch]: improvement: -0.004377413815306763

[2020-01-27 14:39:31.182669][__main__.TRPOAgent.log][linesearch]: improvement: -0.00261898380069292

[2020-01-27 14:39:31.218684][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015711836953697933

[2020-01-27 14:39:31.219710][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.3930684268818e-07, Discarded policy loss value: -1.1731730401596192

[2020-01-27 14:39:32.099260][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.5553814310549

[2020-01-27 14:39:32.105254][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:39:35.705086][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.28362649 -0.         ...  0.08295819 -0.07557945
 -0.00737874]

[2020-01-27 14:39:35.705465][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:36.098942][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.75149657  0.         ...  0.70984117 -0.17840851
 -0.53143266], shape=(4547,), dtype=float64)

[2020-01-27 14:39:36.184069][__main__.TRPOAgent.log][linesearch]: improvement: -0.17292413705007492

[2020-01-27 14:39:36.219751][__main__.TRPOAgent.log][linesearch]: improvement: -0.1056795979365921

[2020-01-27 14:39:36.220276][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.193162350338777

[2020-01-27 14:39:36.990124][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.96531273878554

[2020-01-27 14:39:36.990545][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:39:37.002407][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:39:40.599713][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.13767619 -0.         ...  0.05879078 -0.07237956
  0.01358878]

[2020-01-27 14:39:40.600095][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:40.998731][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.32652231  0.         ...  0.35653422 -0.1607255
 -0.19580872], shape=(4547,), dtype=float64)

[2020-01-27 14:39:41.087140][__main__.TRPOAgent.log][linesearch]: improvement: -0.05462307927764298

[2020-01-27 14:39:41.123408][__main__.TRPOAgent.log][linesearch]: improvement: -0.053321156895185684

[2020-01-27 14:39:41.123898][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.9078494178827983

[2020-01-27 14:39:41.875730][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 365.83115764932694

[2020-01-27 14:39:41.876123][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:39:41.883244][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:39:45.504654][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01545468 -0.         ...  0.08794997 -0.02449328
 -0.06345669]

[2020-01-27 14:39:45.505061][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:45.892111][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.56556471  0.         ... -0.22596488  0.07938026
  0.14658462], shape=(4547,), dtype=float64)

[2020-01-27 14:39:45.972871][__main__.TRPOAgent.log][linesearch]: improvement: -0.1297319193091236

[2020-01-27 14:39:46.006955][__main__.TRPOAgent.log][linesearch]: improvement: -0.07619719521000867

[2020-01-27 14:39:46.037270][__main__.TRPOAgent.log][linesearch]: improvement: -0.044413396282429574

[2020-01-27 14:39:46.076153][__main__.TRPOAgent.log][linesearch]: improvement: -0.025752864157030375

[2020-01-27 14:39:46.112759][__main__.TRPOAgent.log][linesearch]: improvement: -0.015077216334838273

[2020-01-27 14:39:46.144298][__main__.TRPOAgent.log][linesearch]: improvement: -0.008979995204083657

[2020-01-27 14:39:46.175702][__main__.TRPOAgent.log][linesearch]: improvement: -0.005345265178147707

[2020-01-27 14:39:46.203096][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032325488986102435

[2020-01-27 14:39:46.237423][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019353509053006701

[2020-01-27 14:39:46.266984][__main__.TRPOAgent.log][linesearch]: improvement: -0.001152191949807424

[2020-01-27 14:39:46.267458][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.540484095704789e-07, Discarded policy loss value: -0.4582322038768547

[2020-01-27 14:39:47.001284][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 363.7354877451749

[2020-01-27 14:39:47.003272][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 1237

[2020-01-27 14:39:47.997821][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          1.82383584 -0.         ...  0.35175125 -0.70688205
  0.35513079]

[2020-01-27 14:39:47.998158][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:39:48.206628][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          2.58249246  0.         ... -6.25626349  3.46373537
  2.79252812], shape=(4547,), dtype=float64)

[2020-01-27 14:39:48.255271][__main__.TRPOAgent.log][linesearch]: improvement: -0.976144050502602

[2020-01-27 14:39:48.255731][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.9675400297429282

[2020-01-27 14:39:48.499763][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 334.21684658651526

[2020-01-27 14:39:48.500177][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:39:48.518581][__main__.TRPOAgent.log][learning]: Episode #7

[2020-01-27 14:39:48.519004][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:39:48.570510][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:39:48.571384][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:39:48.571135][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:39:48.572873][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:39:49.835231][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 580

[2020-01-27 14:39:50.370892][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 990

[2020-01-27 14:39:50.371501][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:39:50.372399][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:39:50.373220][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:39:50.372327][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:39:51.996504][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 735

[2020-01-27 14:39:52.419935][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1104

[2020-01-27 14:39:52.420547][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:39:52.421526][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:39:52.421596][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:39:52.423457][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:39:53.733203][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 596

[2020-01-27 14:39:53.962258][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 772

[2020-01-27 14:39:53.962873][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:39:53.963632][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:39:53.963745][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:39:53.967284][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:39:54.983803][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 438

[2020-01-27 14:39:55.783083][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1164

[2020-01-27 14:39:55.783682][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:39:55.784335][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:39:55.784467][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:39:55.786855][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:39:58.039376][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1004

[2020-01-27 14:39:58.537244][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:39:58.537792][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:39:58.538632][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:39:58.538841][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:39:58.540652][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:40:00.095133][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 682

[2020-01-27 14:40:00.268766][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 809

[2020-01-27 14:40:00.269222][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:40:00.270033][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:40:00.269958][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:40:00.271643][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:40:03.718441][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:40:03.752726][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:40:03.753694][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:40:03.754672][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:40:03.754546][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:40:03.756060][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:40:04.925809][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 505

[2020-01-27 14:40:05.586408][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1141

[2020-01-27 14:40:05.587064][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:40:05.588048][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:40:05.587949][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:40:05.588922][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:40:07.020995][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 641

[2020-01-27 14:40:07.242949][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 796

[2020-01-27 14:40:07.243449][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:40:07.244265][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:40:07.244115][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:40:07.245251][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:40:08.095579][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 364

[2020-01-27 14:40:08.818095][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1105

[2020-01-27 14:40:08.818565][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:40:08.819364][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:40:08.819465][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:40:08.824204][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:40:10.062184][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 540

[2020-01-27 14:40:10.374398][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 822

[2020-01-27 14:40:10.374839][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:40:10.375488][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:40:10.375568][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:40:10.377974][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:40:11.471296][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 464

[2020-01-27 14:40:12.192754][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1184

[2020-01-27 14:40:12.193361][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:40:12.194120][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:40:12.194173][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:40:12.197518][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:40:14.112070][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 894

[2020-01-27 14:40:14.519229][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1202

[2020-01-27 14:40:14.519756][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:40:14.520647][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:40:14.520717][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:40:14.524494][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:40:15.915212][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 572

[2020-01-27 14:40:16.070515][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 722

[2020-01-27 14:40:16.070934][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:40:16.071446][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:40:16.071508][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:40:16.073399][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:40:16.945882][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 363

[2020-01-27 14:40:17.550884][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 986

[2020-01-27 14:40:17.551493][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:40:17.561223][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:40:17.786313][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:40:17.818264][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:40:17.820908][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 25675, Batch size: 4500, Number of batches: 6

[2020-01-27 14:40:17.821586][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:40:17.850201][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:40:21.429186][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.17796832 -0.         ...  0.2592513  -0.00216325
 -0.25708806]

[2020-01-27 14:40:21.429644][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:40:21.828574][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.21812164  0.         ...  0.53499343 -0.37002076
 -0.16497268], shape=(4547,), dtype=float64)

[2020-01-27 14:40:21.920049][__main__.TRPOAgent.log][linesearch]: improvement: -0.19876998298812687

[2020-01-27 14:40:21.954727][__main__.TRPOAgent.log][linesearch]: improvement: -0.11639813732387605

[2020-01-27 14:40:21.955306][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 6.46488595763876

[2020-01-27 14:40:22.725153][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 425.83049557117124

[2020-01-27 14:40:22.725550][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:40:22.732934][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:40:26.361269][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.38629032 -0.         ... -0.34213937  0.2192731
  0.12286626]

[2020-01-27 14:40:26.361655][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:40:26.746322][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.09097009  0.         ...  0.42872847 -0.17938081
 -0.24934765], shape=(4547,), dtype=float64)

[2020-01-27 14:40:26.835683][__main__.TRPOAgent.log][linesearch]: improvement: -0.09875685707909732

[2020-01-27 14:40:26.871287][__main__.TRPOAgent.log][linesearch]: improvement: -0.08904766190615643

[2020-01-27 14:40:26.871878][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.1886361820370863

[2020-01-27 14:40:27.609758][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 486.1410527793188

[2020-01-27 14:40:27.610161][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:40:27.617496][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:40:31.309184][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -1.19738229 -0.         ... -0.06873848 -0.05621321
  0.12495169]

[2020-01-27 14:40:31.309571][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:40:31.737157][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.79987164  0.         ...  0.62299857 -0.52153743
 -0.10146115], shape=(4547,), dtype=float64)

[2020-01-27 14:40:31.821149][__main__.TRPOAgent.log][linesearch]: improvement: -0.2261743197813395

[2020-01-27 14:40:31.853863][__main__.TRPOAgent.log][linesearch]: improvement: -0.1430606093090332

[2020-01-27 14:40:31.885918][__main__.TRPOAgent.log][linesearch]: improvement: -0.08897412256968895

[2020-01-27 14:40:31.914797][__main__.TRPOAgent.log][linesearch]: improvement: -0.05385809330168945

[2020-01-27 14:40:31.948291][__main__.TRPOAgent.log][linesearch]: improvement: -0.0326307331056066

[2020-01-27 14:40:31.975349][__main__.TRPOAgent.log][linesearch]: improvement: -0.019751471383419705

[2020-01-27 14:40:32.011366][__main__.TRPOAgent.log][linesearch]: improvement: -0.01191319886877018

[2020-01-27 14:40:32.044899][__main__.TRPOAgent.log][linesearch]: improvement: -0.0071915313852781715

[2020-01-27 14:40:32.076997][__main__.TRPOAgent.log][linesearch]: improvement: -0.004340635468596865

[2020-01-27 14:40:32.109709][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026038471603411217

[2020-01-27 14:40:32.110276][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.469628992488582e-07, Discarded policy loss value: -3.142015685022273

[2020-01-27 14:40:32.940299][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 432.63464462025587

[2020-01-27 14:40:32.946083][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:40:36.556497][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.42664708 -0.         ... -0.07186815  0.00103851
  0.07082964]

[2020-01-27 14:40:36.556881][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:40:37.003310][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.11110889  0.         ...  0.3069226  -0.24637497
 -0.06054763], shape=(4547,), dtype=float64)

[2020-01-27 14:40:37.110234][__main__.TRPOAgent.log][linesearch]: improvement: -0.18741441939492276

[2020-01-27 14:40:37.150877][__main__.TRPOAgent.log][linesearch]: improvement: -0.10992574198056637

[2020-01-27 14:40:37.178990][__main__.TRPOAgent.log][linesearch]: improvement: -0.06428004781473184

[2020-01-27 14:40:37.221502][__main__.TRPOAgent.log][linesearch]: improvement: -0.03797375940667447

[2020-01-27 14:40:37.256279][__main__.TRPOAgent.log][linesearch]: improvement: -0.02259090389749563

[2020-01-27 14:40:37.291297][__main__.TRPOAgent.log][linesearch]: improvement: -0.013452925750567779

[2020-01-27 14:40:37.328169][__main__.TRPOAgent.log][linesearch]: improvement: -0.008030315676068156

[2020-01-27 14:40:37.363657][__main__.TRPOAgent.log][linesearch]: improvement: -0.004806833015613243

[2020-01-27 14:40:37.390644][__main__.TRPOAgent.log][linesearch]: improvement: -0.002882073677415775

[2020-01-27 14:40:37.428742][__main__.TRPOAgent.log][linesearch]: improvement: -0.001728835391660688

[2020-01-27 14:40:37.429275][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.226083874805501e-07, Discarded policy loss value: -5.450052086236911

[2020-01-27 14:40:38.307958][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 363.4298467862452

[2020-01-27 14:40:38.313901][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:40:41.944576][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.99221463 -0.         ...  0.20626747 -0.40078063
  0.19451316]

[2020-01-27 14:40:41.944964][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:40:42.336828][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.42917935  0.         ... -0.02993087 -0.00647795
  0.03640882], shape=(4547,), dtype=float64)

[2020-01-27 14:40:42.418261][__main__.TRPOAgent.log][linesearch]: improvement: -0.15029163818279212

[2020-01-27 14:40:42.418754][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 4.423008086137532

[2020-01-27 14:40:43.174942][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 490.3439950866521

[2020-01-27 14:40:43.175354][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:40:43.181511][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 3175

[2020-01-27 14:40:45.722829][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.41516195 -0.         ... -0.19043546 -0.02033402
  0.21076948]

[2020-01-27 14:40:45.723255][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:40:46.060118][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.68641389  0.         ... -1.03403829  0.65532409
  0.37871421], shape=(4547,), dtype=float64)

[2020-01-27 14:40:46.138717][__main__.TRPOAgent.log][linesearch]: improvement: -0.23688121681525587

[2020-01-27 14:40:46.171215][__main__.TRPOAgent.log][linesearch]: improvement: -0.13378527419414765

[2020-01-27 14:40:46.171923][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.6160268341897728

[2020-01-27 14:40:46.724765][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 469.81363057978825

[2020-01-27 14:40:46.725185][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:40:46.745676][__main__.TRPOAgent.log][learning]: Episode #8

[2020-01-27 14:40:46.746058][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:40:46.796623][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:40:46.797211][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:40:46.797576][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:40:46.799381][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:40:48.507834][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 728

[2020-01-27 14:40:49.047263][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1217

[2020-01-27 14:40:49.047699][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:40:49.048440][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:40:49.048525][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:40:49.051735][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:40:51.391850][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 943

[2020-01-27 14:40:51.470911][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1002

[2020-01-27 14:40:51.471383][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:40:51.472216][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:40:51.472283][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:40:51.474388][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:40:52.938502][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 642

[2020-01-27 14:40:53.203322][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 894

[2020-01-27 14:40:53.203967][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:40:53.204674][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:40:53.204780][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:40:53.206843][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:40:55.078225][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 816

[2020-01-27 14:40:55.275406][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 981

[2020-01-27 14:40:55.275840][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:40:55.276679][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:40:55.276549][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:40:55.277792][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:40:56.513322][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 520

[2020-01-27 14:40:57.130988][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1059

[2020-01-27 14:40:57.131670][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:40:57.132351][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:40:57.132419][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:40:57.135357][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:40:58.945867][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 786

[2020-01-27 14:40:58.966000][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 816

[2020-01-27 14:40:58.966474][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:40:58.967182][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:40:58.967279][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:40:58.969786][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:41:00.935116][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 817

[2020-01-27 14:41:01.273082][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1112

[2020-01-27 14:41:01.273506][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:41:01.274213][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:41:01.274273][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:41:01.277993][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:41:02.797154][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 669

[2020-01-27 14:41:03.338252][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1189

[2020-01-27 14:41:03.338951][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:41:03.339602][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:41:03.339731][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:41:03.341776][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:41:05.592144][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 968

[2020-01-27 14:41:05.661127][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1045

[2020-01-27 14:41:05.661750][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:41:05.662731][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:41:05.662896][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:41:05.665439][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:41:07.033470][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 565

[2020-01-27 14:41:07.328954][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 813

[2020-01-27 14:41:07.329592][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:41:07.330325][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:41:07.330387][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:41:07.334015][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:41:09.506650][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 931

[2020-01-27 14:41:09.624494][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1048

[2020-01-27 14:41:09.625009][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:41:09.625831][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:41:09.625922][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:41:09.633370][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:41:12.597821][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1301

[2020-01-27 14:41:12.742740][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1410

[2020-01-27 14:41:12.743187][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:41:12.743697][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:41:12.743766][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:41:12.746025][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:41:15.001093][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 932

[2020-01-27 14:41:15.144381][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1124

[2020-01-27 14:41:15.144850][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:41:15.145496][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:41:15.146431][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:41:15.145434][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:41:17.124546][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 872

[2020-01-27 14:41:17.277335][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 941

[2020-01-27 14:41:17.277744][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:41:17.278279][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:41:17.278344][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:41:17.280853][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:41:18.246395][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 432

[2020-01-27 14:41:19.089107][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1244

[2020-01-27 14:41:19.089527][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:41:19.097242][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:41:19.378086][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:41:19.402295][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:41:19.403772][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 27817, Batch size: 4500, Number of batches: 7

[2020-01-27 14:41:19.404157][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:41:19.431723][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:41:22.993943][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          1.05058054 -0.         ...  0.0618518   0.23937082
 -0.30122262]

[2020-01-27 14:41:22.994351][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:41:23.390022][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.56831169  0.         ...  2.17516566 -1.19645745
 -0.9787082 ], shape=(4547,), dtype=float64)

[2020-01-27 14:41:23.475870][__main__.TRPOAgent.log][linesearch]: improvement: -0.2138154116391951

[2020-01-27 14:41:23.512492][__main__.TRPOAgent.log][linesearch]: improvement: -0.13199684344103868

[2020-01-27 14:41:23.547047][__main__.TRPOAgent.log][linesearch]: improvement: -0.07297069737791073

[2020-01-27 14:41:23.580253][__main__.TRPOAgent.log][linesearch]: improvement: -0.04274406832272126

[2020-01-27 14:41:23.607734][__main__.TRPOAgent.log][linesearch]: improvement: -0.027428587822652006

[2020-01-27 14:41:23.648192][__main__.TRPOAgent.log][linesearch]: improvement: -0.015740279139269786

[2020-01-27 14:41:23.676166][__main__.TRPOAgent.log][linesearch]: improvement: -0.008886262847900506

[2020-01-27 14:41:23.711667][__main__.TRPOAgent.log][linesearch]: improvement: -0.004870729858593159

[2020-01-27 14:41:23.752509][__main__.TRPOAgent.log][linesearch]: improvement: -0.002418579587116554

[2020-01-27 14:41:23.782265][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012463503349099403

[2020-01-27 14:41:23.782865][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 1.226335123811761e-06, Discarded policy loss value: -0.6397038490368772

[2020-01-27 14:41:24.657988][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 428.37948648637206

[2020-01-27 14:41:24.666931][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:41:28.306661][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.37267297 -0.         ...  0.17199543 -0.27972558
  0.10773015]

[2020-01-27 14:41:28.307042][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:41:28.704214][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.19477103  0.         ... -0.2998632   0.2125202
  0.087343  ], shape=(4547,), dtype=float64)

[2020-01-27 14:41:28.794468][__main__.TRPOAgent.log][linesearch]: improvement: 0.02508239583553107

[2020-01-27 14:41:28.829005][__main__.TRPOAgent.log][linesearch]: improvement: -0.012329809807127168

[2020-01-27 14:41:28.857113][__main__.TRPOAgent.log][linesearch]: improvement: -0.01706183419029439

[2020-01-27 14:41:28.893000][__main__.TRPOAgent.log][linesearch]: improvement: -0.013228735013025128

[2020-01-27 14:41:28.925685][__main__.TRPOAgent.log][linesearch]: improvement: -0.009687669840838131

[2020-01-27 14:41:28.960388][__main__.TRPOAgent.log][linesearch]: improvement: -0.007154437943135861

[2020-01-27 14:41:28.988043][__main__.TRPOAgent.log][linesearch]: improvement: -0.005447836309081966

[2020-01-27 14:41:29.023203][__main__.TRPOAgent.log][linesearch]: improvement: -0.003865685882325165

[2020-01-27 14:41:29.057535][__main__.TRPOAgent.log][linesearch]: improvement: -0.002548237133142006

[2020-01-27 14:41:29.091129][__main__.TRPOAgent.log][linesearch]: improvement: -0.001547922506651478

[2020-01-27 14:41:29.091626][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.517902945956722e-07, Discarded policy loss value: -1.497882820319975

[2020-01-27 14:41:29.923037][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.2737393200534

[2020-01-27 14:41:29.930594][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:41:33.499961][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.5610974  -0.         ...  0.01340387 -0.33793755
  0.32453368]

[2020-01-27 14:41:33.500368][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:41:33.897074][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.03978278  0.         ... -0.16122321  0.15893361
  0.0022896 ], shape=(4547,), dtype=float64)

[2020-01-27 14:41:33.986046][__main__.TRPOAgent.log][linesearch]: improvement: -0.24896517664262063

[2020-01-27 14:41:34.022440][__main__.TRPOAgent.log][linesearch]: improvement: -0.13119512079230455

[2020-01-27 14:41:34.023050][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 3.898358164579473

[2020-01-27 14:41:34.786016][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 425.13499936544036

[2020-01-27 14:41:34.786437][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:41:34.797628][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:41:38.420021][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.44279508 -0.         ...  0.18171211 -0.19395235
  0.01224023]

[2020-01-27 14:41:38.420405][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:41:38.819477][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.01619455  0.         ... -1.71568101  0.71049164
  1.00518937], shape=(4547,), dtype=float64)

[2020-01-27 14:41:38.910798][__main__.TRPOAgent.log][linesearch]: improvement: -0.045187122941555025

[2020-01-27 14:41:38.950017][__main__.TRPOAgent.log][linesearch]: improvement: -0.02343770029749237

[2020-01-27 14:41:38.977733][__main__.TRPOAgent.log][linesearch]: improvement: -0.016142875850076788

[2020-01-27 14:41:39.012655][__main__.TRPOAgent.log][linesearch]: improvement: -0.012608154851684983

[2020-01-27 14:41:39.039972][__main__.TRPOAgent.log][linesearch]: improvement: -0.009213894001039336

[2020-01-27 14:41:39.076169][__main__.TRPOAgent.log][linesearch]: improvement: -0.006259987570259362

[2020-01-27 14:41:39.111887][__main__.TRPOAgent.log][linesearch]: improvement: -0.004032052941323805

[2020-01-27 14:41:39.142338][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025323948290845166

[2020-01-27 14:41:39.177882][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015164395659175156

[2020-01-27 14:41:39.209252][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009409178592143519

[2020-01-27 14:41:39.209894][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.121078442694788e-07, Discarded policy loss value: -2.751651315681804

[2020-01-27 14:41:40.031908][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 429.1226629959439

[2020-01-27 14:41:40.038048][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:41:43.738440][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          1.21415626 -0.         ... -0.35447684  0.68050201
 -0.32602518]

[2020-01-27 14:41:43.738848][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:41:44.140811][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.41542342  0.         ...  1.59005349 -0.67288149
 -0.917172  ], shape=(4547,), dtype=float64)

[2020-01-27 14:41:44.235425][__main__.TRPOAgent.log][linesearch]: improvement: -0.33244744535815185

[2020-01-27 14:41:44.271708][__main__.TRPOAgent.log][linesearch]: improvement: -0.1951227439865897

[2020-01-27 14:41:44.307578][__main__.TRPOAgent.log][linesearch]: improvement: -0.10821189976138035

[2020-01-27 14:41:44.308202][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 4.057608086711609

[2020-01-27 14:41:45.072239][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 488.3226230844945

[2020-01-27 14:41:45.072649][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:41:45.084327][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:41:48.684240][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.69502975 -0.         ... -0.20084924 -0.22085535
  0.42170459]

[2020-01-27 14:41:48.684652][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:41:49.107178][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.15627352  0.         ... -0.1505637   0.07379265
  0.07677105], shape=(4547,), dtype=float64)

[2020-01-27 14:41:49.203693][__main__.TRPOAgent.log][linesearch]: improvement: -0.0657696591740713

[2020-01-27 14:41:49.245873][__main__.TRPOAgent.log][linesearch]: improvement: -0.04081183691025547

[2020-01-27 14:41:49.283413][__main__.TRPOAgent.log][linesearch]: improvement: -0.025940076170453352

[2020-01-27 14:41:49.310854][__main__.TRPOAgent.log][linesearch]: improvement: -0.01705274815419422

[2020-01-27 14:41:49.353374][__main__.TRPOAgent.log][linesearch]: improvement: -0.011724493664653224

[2020-01-27 14:41:49.384578][__main__.TRPOAgent.log][linesearch]: improvement: -0.008823114573723423

[2020-01-27 14:41:49.425141][__main__.TRPOAgent.log][linesearch]: improvement: -0.005882457715169487

[2020-01-27 14:41:49.463016][__main__.TRPOAgent.log][linesearch]: improvement: -0.003967456827442817

[2020-01-27 14:41:49.499303][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026288024625502615

[2020-01-27 14:41:49.528010][__main__.TRPOAgent.log][linesearch]: improvement: -0.001669918695843542

[2020-01-27 14:41:49.528585][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.394060918612942e-07, Discarded policy loss value: -7.0221188863662585

[2020-01-27 14:41:50.355025][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.96063515824335

[2020-01-27 14:41:50.356801][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 817

[2020-01-27 14:41:51.046951][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          1.07484421 -0.         ... -0.8672441   0.16851063
  0.69873347]

[2020-01-27 14:41:51.047293][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:41:51.233333][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[  0.           0.60694419   0.         ... -12.17464452   5.4815009
   6.69314362], shape=(4547,), dtype=float64)

[2020-01-27 14:41:51.276330][__main__.TRPOAgent.log][linesearch]: improvement: -0.3837030637729093

[2020-01-27 14:41:51.296497][__main__.TRPOAgent.log][linesearch]: improvement: -0.22888884037953083

[2020-01-27 14:41:51.296969][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 7.735436882265914

[2020-01-27 14:41:51.479676][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 738.5959376380049

[2020-01-27 14:41:51.480084][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:41:51.503824][__main__.TRPOAgent.log][learning]: Episode #9

[2020-01-27 14:41:51.504369][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:41:51.560857][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:41:51.561468][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:41:51.561592][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:41:51.563900][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:41:53.720676][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 900

[2020-01-27 14:41:53.721063][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 849

[2020-01-27 14:41:53.721785][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:41:53.722238][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:41:53.722299][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:41:53.723918][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:41:55.552202][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 782

[2020-01-27 14:41:56.314420][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:41:56.314988][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:41:56.316219][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:41:56.316139][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:41:56.317330][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:41:57.531642][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 544

[2020-01-27 14:41:57.948570][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 885

[2020-01-27 14:41:57.949105][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:41:57.949725][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:41:57.949778][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:41:57.951222][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:41:58.874519][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 386

[2020-01-27 14:41:59.179284][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 667

[2020-01-27 14:41:59.179944][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:41:59.180852][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:41:59.180912][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:41:59.182987][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:42:00.760391][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 673

[2020-01-27 14:42:01.117986][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 975

[2020-01-27 14:42:01.118404][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:42:01.119032][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:42:01.118960][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:42:01.120621][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:42:02.714175][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 689

[2020-01-27 14:42:02.879731][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 812

[2020-01-27 14:42:02.880177][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:42:02.880844][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:42:02.880773][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:42:02.883019][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:42:03.996759][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 433

[2020-01-27 14:42:04.831948][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1344

[2020-01-27 14:42:04.832590][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:42:04.833180][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:42:04.833234][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:42:04.835207][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:42:06.425760][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 695

[2020-01-27 14:42:06.584621][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 826

[2020-01-27 14:42:06.585139][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:42:06.585835][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:42:06.585769][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:42:06.587231][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:42:08.002563][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 562

[2020-01-27 14:42:08.477071][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 998

[2020-01-27 14:42:08.477446][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:42:08.478111][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:42:08.478232][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:42:08.481718][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:42:09.737880][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 548

[2020-01-27 14:42:09.994318][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 798

[2020-01-27 14:42:09.994888][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:42:09.995502][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:42:09.995564][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:42:09.997429][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:42:11.225898][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 542

[2020-01-27 14:42:11.564262][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 858

[2020-01-27 14:42:11.564858][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:42:11.565673][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:42:11.565557][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:42:11.566695][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:42:12.574077][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 458

[2020-01-27 14:42:12.832153][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 651

[2020-01-27 14:42:12.832719][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:42:12.833488][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:42:12.833418][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:42:12.834911][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:42:13.836904][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 364

[2020-01-27 14:42:13.943904][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 466

[2020-01-27 14:42:13.944377][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:42:13.944954][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:42:13.945035][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:42:13.947454][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:42:15.534681][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 690

[2020-01-27 14:42:16.220444][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1363

[2020-01-27 14:42:16.221066][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:42:16.221644][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:42:16.221716][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:42:16.223452][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:42:17.351197][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 480

[2020-01-27 14:42:17.638048][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 758

[2020-01-27 14:42:17.638834][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:42:17.644680][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:42:17.806727][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:42:17.832115][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:42:17.834535][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 22496, Batch size: 4500, Number of batches: 5

[2020-01-27 14:42:17.835199][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:42:17.856143][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:42:21.436789][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -1.51118591 -0.         ... -0.39300123 -0.07043733
  0.46343856]

[2020-01-27 14:42:21.437207][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:42:21.835400][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.39134256  0.         ... -0.24437792  0.12481
  0.11956791], shape=(4547,), dtype=float64)

[2020-01-27 14:42:21.925124][__main__.TRPOAgent.log][linesearch]: improvement: -0.21185897456766156

[2020-01-27 14:42:21.961230][__main__.TRPOAgent.log][linesearch]: improvement: -0.13809767614709756

[2020-01-27 14:42:21.997007][__main__.TRPOAgent.log][linesearch]: improvement: -0.08928147423308275

[2020-01-27 14:42:22.033374][__main__.TRPOAgent.log][linesearch]: improvement: -0.05664579459044461

[2020-01-27 14:42:22.073039][__main__.TRPOAgent.log][linesearch]: improvement: -0.034617044007475783

[2020-01-27 14:42:22.101145][__main__.TRPOAgent.log][linesearch]: improvement: -0.020944519330674538

[2020-01-27 14:42:22.138324][__main__.TRPOAgent.log][linesearch]: improvement: -0.012731918473424031

[2020-01-27 14:42:22.171391][__main__.TRPOAgent.log][linesearch]: improvement: -0.007682475941752642

[2020-01-27 14:42:22.205700][__main__.TRPOAgent.log][linesearch]: improvement: -0.004626243501874683

[2020-01-27 14:42:22.239951][__main__.TRPOAgent.log][linesearch]: improvement: -0.0027863862661696714

[2020-01-27 14:42:22.240473][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.461971289049001e-07, Discarded policy loss value: -8.332932026250774

[2020-01-27 14:42:23.018936][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 483.63751499696457

[2020-01-27 14:42:23.024719][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:42:26.602062][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.35772447 -0.         ... -0.08979984 -0.1402542
  0.23005405]

[2020-01-27 14:42:26.602460][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:42:26.998149][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.02083745  0.         ... -0.00560604 -0.04195801
  0.04756405], shape=(4547,), dtype=float64)

[2020-01-27 14:42:27.080812][__main__.TRPOAgent.log][linesearch]: improvement: -0.09543710606205025

[2020-01-27 14:42:27.114480][__main__.TRPOAgent.log][linesearch]: improvement: -0.07278402664259964

[2020-01-27 14:42:27.115029][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.494237595393279

[2020-01-27 14:42:27.845816][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 536.1846501729501

[2020-01-27 14:42:27.846214][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:42:27.853487][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:42:31.479561][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10978325 -0.         ...  0.07590011 -0.01747493
 -0.05842517]

[2020-01-27 14:42:31.479974][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:42:31.901207][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.91714998  0.         ... -0.14256752 -0.01696299
  0.15953051], shape=(4547,), dtype=float64)

[2020-01-27 14:42:31.985445][__main__.TRPOAgent.log][linesearch]: improvement: -0.12285709502814068

[2020-01-27 14:42:32.019676][__main__.TRPOAgent.log][linesearch]: improvement: -0.0786643804531284

[2020-01-27 14:42:32.050685][__main__.TRPOAgent.log][linesearch]: improvement: -0.04860483363974932

[2020-01-27 14:42:32.081279][__main__.TRPOAgent.log][linesearch]: improvement: -0.0296842060671052

[2020-01-27 14:42:32.116068][__main__.TRPOAgent.log][linesearch]: improvement: -0.018130772742651402

[2020-01-27 14:42:32.153296][__main__.TRPOAgent.log][linesearch]: improvement: -0.011041194176204527

[2020-01-27 14:42:32.184223][__main__.TRPOAgent.log][linesearch]: improvement: -0.006673616016193762

[2020-01-27 14:42:32.224370][__main__.TRPOAgent.log][linesearch]: improvement: -0.00402389450966556

[2020-01-27 14:42:32.259497][__main__.TRPOAgent.log][linesearch]: improvement: -0.002426793529905602

[2020-01-27 14:42:32.287088][__main__.TRPOAgent.log][linesearch]: improvement: -0.001464011820027089

[2020-01-27 14:42:32.287736][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.702595049304206e-07, Discarded policy loss value: -4.6303455254600445

[2020-01-27 14:42:33.052004][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 457.0395274803457

[2020-01-27 14:42:33.057365][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:42:36.597042][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.26025463 -0.         ... -0.14264781  0.02038338
  0.12226443]

[2020-01-27 14:42:36.597431][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:42:37.032394][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.97124969  0.         ... -0.0073794  -0.07915147
  0.08653087], shape=(4547,), dtype=float64)

[2020-01-27 14:42:37.137644][__main__.TRPOAgent.log][linesearch]: improvement: -0.050560192663167136

[2020-01-27 14:42:37.178181][__main__.TRPOAgent.log][linesearch]: improvement: -0.05094876351981803

[2020-01-27 14:42:37.178700][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.7099857460236736

[2020-01-27 14:42:38.057150][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 514.9163604253262

[2020-01-27 14:42:38.057610][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:42:38.065810][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4496

[2020-01-27 14:42:41.638580][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.87798428 -0.         ...  0.16615904  0.13684902
 -0.30300807]

[2020-01-27 14:42:41.638962][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:42:42.032261][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.38376786  0.         ...  0.21830346 -0.27932064
  0.06101718], shape=(4547,), dtype=float64)

[2020-01-27 14:42:42.122239][__main__.TRPOAgent.log][linesearch]: improvement: -0.1711324275719266

[2020-01-27 14:42:42.159121][__main__.TRPOAgent.log][linesearch]: improvement: -0.10209466821316826

[2020-01-27 14:42:42.192520][__main__.TRPOAgent.log][linesearch]: improvement: -0.06251304429197879

[2020-01-27 14:42:42.226652][__main__.TRPOAgent.log][linesearch]: improvement: -0.038294638331275443

[2020-01-27 14:42:42.259484][__main__.TRPOAgent.log][linesearch]: improvement: -0.02320814211222344

[2020-01-27 14:42:42.293545][__main__.TRPOAgent.log][linesearch]: improvement: -0.013910753046000424

[2020-01-27 14:42:42.326092][__main__.TRPOAgent.log][linesearch]: improvement: -0.008227943111351999

[2020-01-27 14:42:42.358950][__main__.TRPOAgent.log][linesearch]: improvement: -0.0049339321804166

[2020-01-27 14:42:42.393703][__main__.TRPOAgent.log][linesearch]: improvement: -0.00296495833604582

[2020-01-27 14:42:42.423949][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017671956807915334

[2020-01-27 14:42:42.424540][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.297607277024338e-07, Discarded policy loss value: -0.8112769503263051

[2020-01-27 14:42:43.297595][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 429.3744613646488

[2020-01-27 14:42:43.324348][__main__.TRPOAgent.log][learning]: Episode #10

[2020-01-27 14:42:43.324738][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:42:43.386120][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:42:43.386734][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:42:43.386915][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:42:43.388691][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:42:46.003900][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1137

[2020-01-27 14:42:46.098667][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1181

[2020-01-27 14:42:46.099037][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:42:46.099924][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:42:46.099779][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:42:46.100859][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:42:47.533748][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 636

[2020-01-27 14:42:48.419531][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:42:48.420127][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:42:48.420689][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:42:48.420786][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:42:48.426554][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:42:51.904857][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1472

[2020-01-27 14:42:51.915947][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:42:51.916510][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:42:51.917209][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:42:51.917422][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:42:51.921033][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:42:53.539053][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 686

[2020-01-27 14:42:53.723777][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 805

[2020-01-27 14:42:53.724205][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:42:53.724921][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:42:53.725017][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:42:53.728758][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:42:54.905080][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 514

[2020-01-27 14:42:55.696049][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1230

[2020-01-27 14:42:55.696568][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:42:55.697196][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:42:55.697129][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:42:55.698342][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:42:57.681650][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 825

[2020-01-27 14:42:58.086924][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1268

[2020-01-27 14:42:58.087304][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:42:58.087789][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:42:58.087846][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:42:58.089815][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:43:00.063715][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 830

[2020-01-27 14:43:00.262688][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 997

[2020-01-27 14:43:00.263303][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:43:00.264047][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:43:00.264298][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:43:00.265897][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:43:01.907358][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 745

[2020-01-27 14:43:01.931432][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 674

[2020-01-27 14:43:01.931920][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:43:01.932608][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:43:01.932684][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:43:01.937009][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:43:04.181936][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 957

[2020-01-27 14:43:04.553987][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1317

[2020-01-27 14:43:04.554607][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:43:04.555543][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:43:04.555612][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:43:04.557904][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:43:06.687366][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 898

[2020-01-27 14:43:07.252881][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:43:07.253566][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:43:07.254455][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:43:07.254684][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:43:07.256988][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:43:09.069443][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 744

[2020-01-27 14:43:09.320449][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1004

[2020-01-27 14:43:09.321141][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:43:09.321967][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:43:09.322053][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:43:09.323675][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:43:10.862124][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 686

[2020-01-27 14:43:10.936783][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 715

[2020-01-27 14:43:10.937200][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:43:10.937991][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:43:10.937931][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:43:10.939141][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:43:12.563813][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 704

[2020-01-27 14:43:12.633938][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 741

[2020-01-27 14:43:12.634394][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:43:12.634916][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:43:12.634992][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:43:12.637735][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:43:13.817989][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 481

[2020-01-27 14:43:14.054217][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 687

[2020-01-27 14:43:14.054826][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:43:14.055470][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:43:14.055525][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:43:14.057230][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:43:16.298777][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1016

[2020-01-27 14:43:16.573319][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1208

[2020-01-27 14:43:16.573979][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:43:16.581531][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:43:16.868739][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:43:16.889697][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:43:16.893417][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 28658, Batch size: 4500, Number of batches: 7

[2020-01-27 14:43:16.893842][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:43:16.920683][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:43:20.582442][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.13110312 -0.         ...  0.09874347 -0.30157955
  0.20283608]

[2020-01-27 14:43:20.582844][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:43:20.985188][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.17315523  0.         ...  0.08210139 -0.16642462
  0.08432323], shape=(4547,), dtype=float64)

[2020-01-27 14:43:21.074521][__main__.TRPOAgent.log][linesearch]: improvement: -0.025052324047566965

[2020-01-27 14:43:21.074978][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.6206873366519434

[2020-01-27 14:43:21.869262][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 429.76166037996677

[2020-01-27 14:43:21.869651][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:43:21.877110][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:43:25.536415][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.77645179 -0.         ...  0.12948582  0.18606918
 -0.315555  ]

[2020-01-27 14:43:25.536836][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:43:25.957385][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.50844389  0.         ... -0.27465494  0.28172422
 -0.00706928], shape=(4547,), dtype=float64)

[2020-01-27 14:43:26.048502][__main__.TRPOAgent.log][linesearch]: improvement: -0.1384542991986284

[2020-01-27 14:43:26.049050][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 3.1028637316371186

[2020-01-27 14:43:26.787566][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 486.9793075949793

[2020-01-27 14:43:26.787963][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:43:26.798444][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:43:30.395963][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          2.27112653 -0.         ...  0.20921812  0.34425732
 -0.55347545]

[2020-01-27 14:43:30.396342][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:43:30.798920][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.67292031  0.         ... -0.2184188   0.07690048
  0.14151832], shape=(4547,), dtype=float64)

[2020-01-27 14:43:30.892246][__main__.TRPOAgent.log][linesearch]: improvement: -0.39439663218425824

[2020-01-27 14:43:30.936052][__main__.TRPOAgent.log][linesearch]: improvement: -0.23321737362611117

[2020-01-27 14:43:30.972596][__main__.TRPOAgent.log][linesearch]: improvement: -0.13635404974188692

[2020-01-27 14:43:31.007367][__main__.TRPOAgent.log][linesearch]: improvement: -0.07890478290812553

[2020-01-27 14:43:31.040559][__main__.TRPOAgent.log][linesearch]: improvement: -0.045209334161883774

[2020-01-27 14:43:31.084685][__main__.TRPOAgent.log][linesearch]: improvement: -0.026378835747226326

[2020-01-27 14:43:31.117072][__main__.TRPOAgent.log][linesearch]: improvement: -0.015572884001184839

[2020-01-27 14:43:31.157232][__main__.TRPOAgent.log][linesearch]: improvement: -0.009259163486248667

[2020-01-27 14:43:31.186570][__main__.TRPOAgent.log][linesearch]: improvement: -0.0055297026721152776

[2020-01-27 14:43:31.226996][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033098939614317757

[2020-01-27 14:43:31.227516][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.856597318349368e-07, Discarded policy loss value: -5.429819473881962

[2020-01-27 14:43:32.001288][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 361.6655342618765

[2020-01-27 14:43:32.006538][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:43:35.620952][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.52987048 -0.         ... -0.02262783  0.0297932
 -0.00716537]

[2020-01-27 14:43:35.621362][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:43:36.035952][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.5549431   0.         ...  0.36864463 -0.18725885
 -0.18138578], shape=(4547,), dtype=float64)

[2020-01-27 14:43:36.119722][__main__.TRPOAgent.log][linesearch]: improvement: -0.13839833610150176

[2020-01-27 14:43:36.156675][__main__.TRPOAgent.log][linesearch]: improvement: -0.08955267043695692

[2020-01-27 14:43:36.157253][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.469013851343894

[2020-01-27 14:43:36.921274][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 428.450570254672

[2020-01-27 14:43:36.921693][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:43:36.930023][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:43:40.497705][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.27467697 -0.         ...  0.01633952  0.22796982
 -0.24430934]

[2020-01-27 14:43:40.498084][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:43:40.903595][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.00000000e+00 -3.25153671e-01  0.00000000e+00 ... -3.97829563e-01
  3.97990261e-01 -1.60697716e-04], shape=(4547,), dtype=float64)

[2020-01-27 14:43:40.988531][__main__.TRPOAgent.log][linesearch]: improvement: -0.19874582224418536

[2020-01-27 14:43:41.025408][__main__.TRPOAgent.log][linesearch]: improvement: -0.11839361845439011

[2020-01-27 14:43:41.060375][__main__.TRPOAgent.log][linesearch]: improvement: -0.06827945013829595

[2020-01-27 14:43:41.088479][__main__.TRPOAgent.log][linesearch]: improvement: -0.0399348651904452

[2020-01-27 14:43:41.125590][__main__.TRPOAgent.log][linesearch]: improvement: -0.023499240628419127

[2020-01-27 14:43:41.161333][__main__.TRPOAgent.log][linesearch]: improvement: -0.013942217024715653

[2020-01-27 14:43:41.187137][__main__.TRPOAgent.log][linesearch]: improvement: -0.008317042283923826

[2020-01-27 14:43:41.224214][__main__.TRPOAgent.log][linesearch]: improvement: -0.004978799012006263

[2020-01-27 14:43:41.259278][__main__.TRPOAgent.log][linesearch]: improvement: -0.0029849698519544

[2020-01-27 14:43:41.294837][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017898797813633571

[2020-01-27 14:43:41.295348][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.203620451019436e-07, Discarded policy loss value: -2.4895174708281633

[2020-01-27 14:43:42.122327][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 359.73743988799976

[2020-01-27 14:43:42.130459][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:43:45.773552][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.57528872 -0.         ...  0.15905936 -0.42584042
  0.26678106]

[2020-01-27 14:43:45.773938][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:43:46.181410][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.89486418  0.         ... -0.3379322   0.31644583
  0.02148637], shape=(4547,), dtype=float64)

[2020-01-27 14:43:46.272361][__main__.TRPOAgent.log][linesearch]: improvement: -0.11082424009849845

[2020-01-27 14:43:46.272891][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 2.6582103982243788

[2020-01-27 14:43:47.040402][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 364.18030905869966

[2020-01-27 14:43:47.040808][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:43:47.045509][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 1658

[2020-01-27 14:43:48.377550][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.1771216  -0.         ...  0.28111212 -0.44539754
  0.16428542]

[2020-01-27 14:43:48.377893][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:43:48.623363][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.26463225  0.         ...  0.09690304 -0.00210693
 -0.09479611], shape=(4547,), dtype=float64)

[2020-01-27 14:43:48.687632][__main__.TRPOAgent.log][linesearch]: improvement: -0.42945045145160865

[2020-01-27 14:43:48.688179][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 4.617266414005781

[2020-01-27 14:43:48.999727][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 454.09099837502396

[2020-01-27 14:43:49.000136][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:43:49.020920][__main__.TRPOAgent.log][learning]: Episode #11

[2020-01-27 14:43:49.021466][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:43:49.080057][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:43:49.080725][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:43:49.080655][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:43:49.081779][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:43:50.234812][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 453

[2020-01-27 14:43:51.005407][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1187

[2020-01-27 14:43:51.006155][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:43:51.007254][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:43:51.007328][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:43:51.011110][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:43:53.515160][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1106

[2020-01-27 14:43:53.873340][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1472

[2020-01-27 14:43:53.873792][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:43:53.874361][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:43:53.874305][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:43:53.875480][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:43:55.635805][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 693

[2020-01-27 14:43:56.383443][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:43:56.384044][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:43:56.385143][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:43:56.385296][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:43:56.387249][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:43:57.815531][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 606

[2020-01-27 14:43:57.964880][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 751

[2020-01-27 14:43:57.965420][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:43:57.966085][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:43:57.966275][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:43:57.970089][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:43:59.736358][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 752

[2020-01-27 14:43:59.806579][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 823

[2020-01-27 14:43:59.806990][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:43:59.807489][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:43:59.807543][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:43:59.809121][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:44:01.144988][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 553

[2020-01-27 14:44:01.340098][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 769

[2020-01-27 14:44:01.340608][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:44:01.341136][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:44:01.341229][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:44:01.343373][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:44:03.611439][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1004

[2020-01-27 14:44:03.815451][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1165

[2020-01-27 14:44:03.816021][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:44:03.816870][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:44:03.816968][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:44:03.820648][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:44:07.248046][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1463

[2020-01-27 14:44:07.297616][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:44:07.298547][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:44:07.299330][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:44:07.299253][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:44:07.300948][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:44:08.595186][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 559

[2020-01-27 14:44:09.583374][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:44:09.584009][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:44:09.585039][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:44:09.585109][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:44:09.587225][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:44:11.600267][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 901

[2020-01-27 14:44:11.731259][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 974

[2020-01-27 14:44:11.731712][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:44:11.732264][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:44:11.732410][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:44:11.736310][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:44:13.317658][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 691

[2020-01-27 14:44:13.866364][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1187

[2020-01-27 14:44:13.867012][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:44:13.867638][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:44:13.867688][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:44:13.871388][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:44:16.492842][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1135

[2020-01-27 14:44:16.566691][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1225

[2020-01-27 14:44:16.567197][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:44:16.567766][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:44:16.567834][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:44:16.570068][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:44:17.600370][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 457

[2020-01-27 14:44:17.704801][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 526

[2020-01-27 14:44:17.705323][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:44:17.706000][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:44:17.706075][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:44:17.708008][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:44:19.711173][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 836

[2020-01-27 14:44:20.208383][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1328

[2020-01-27 14:44:20.209074][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:44:20.209754][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:44:20.209855][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:44:20.211856][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:44:22.298062][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 916

[2020-01-27 14:44:22.923615][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:44:22.924666][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:44:22.932479][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:44:23.229398][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:44:23.251664][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:44:23.253194][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 29532, Batch size: 4500, Number of batches: 7

[2020-01-27 14:44:23.253601][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:44:23.279249][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:44:26.917558][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.53351702 -0.         ... -0.06718145  0.27699157
 -0.20981011]

[2020-01-27 14:44:26.917952][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:44:27.311428][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.45752423  0.         ... -0.24940555  0.17112524
  0.07828032], shape=(4547,), dtype=float64)

[2020-01-27 14:44:27.395652][__main__.TRPOAgent.log][linesearch]: improvement: -0.13285909316348743

[2020-01-27 14:44:27.430308][__main__.TRPOAgent.log][linesearch]: improvement: -0.08309402183895997

[2020-01-27 14:44:27.466402][__main__.TRPOAgent.log][linesearch]: improvement: -0.05067748010339912

[2020-01-27 14:44:27.502452][__main__.TRPOAgent.log][linesearch]: improvement: -0.030703576727771242

[2020-01-27 14:44:27.535970][__main__.TRPOAgent.log][linesearch]: improvement: -0.018514789812877908

[2020-01-27 14:44:27.570271][__main__.TRPOAgent.log][linesearch]: improvement: -0.011141116450711586

[2020-01-27 14:44:27.602614][__main__.TRPOAgent.log][linesearch]: improvement: -0.006690976607974797

[2020-01-27 14:44:27.638534][__main__.TRPOAgent.log][linesearch]: improvement: -0.004017334384671223

[2020-01-27 14:44:27.668472][__main__.TRPOAgent.log][linesearch]: improvement: -0.002411538248451528

[2020-01-27 14:44:27.707691][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014472377414040949

[2020-01-27 14:44:27.708373][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.453870849321143e-07, Discarded policy loss value: -4.282002494145133

[2020-01-27 14:44:28.580394][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 294.48046382703274

[2020-01-27 14:44:28.588261][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:44:32.256725][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.09864547 -0.         ...  0.07096574  0.04088802
 -0.11185376]

[2020-01-27 14:44:32.257140][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:44:32.656239][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.19497946  0.         ... -0.07658345 -0.02164597
  0.09822942], shape=(4547,), dtype=float64)

[2020-01-27 14:44:32.745468][__main__.TRPOAgent.log][linesearch]: improvement: -0.14037259514905198

[2020-01-27 14:44:32.780637][__main__.TRPOAgent.log][linesearch]: improvement: -0.08498036465296277

[2020-01-27 14:44:32.781194][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 5.7442962850173505

[2020-01-27 14:44:33.618686][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 425.46166156316485

[2020-01-27 14:44:33.619119][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:44:33.633298][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:44:37.265394][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.13882389 -0.         ... -0.09696524  0.09649246
  0.00047278]

[2020-01-27 14:44:37.265806][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:44:37.706022][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.68437123  0.         ...  0.08888767 -0.14956388
  0.0606762 ], shape=(4547,), dtype=float64)

[2020-01-27 14:44:37.802414][__main__.TRPOAgent.log][linesearch]: improvement: -0.1707408959005745

[2020-01-27 14:44:37.837547][__main__.TRPOAgent.log][linesearch]: improvement: -0.10110783484409946

[2020-01-27 14:44:37.871738][__main__.TRPOAgent.log][linesearch]: improvement: -0.05985395256766135

[2020-01-27 14:44:37.903177][__main__.TRPOAgent.log][linesearch]: improvement: -0.03555341295995662

[2020-01-27 14:44:37.935407][__main__.TRPOAgent.log][linesearch]: improvement: -0.02118855954422827

[2020-01-27 14:44:37.968030][__main__.TRPOAgent.log][linesearch]: improvement: -0.012658472914788277

[2020-01-27 14:44:38.000488][__main__.TRPOAgent.log][linesearch]: improvement: -0.007574880607310597

[2020-01-27 14:44:38.031447][__main__.TRPOAgent.log][linesearch]: improvement: -0.004537429304871887

[2020-01-27 14:44:38.063383][__main__.TRPOAgent.log][linesearch]: improvement: -0.002719725263927497

[2020-01-27 14:44:38.095057][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016308481447597778

[2020-01-27 14:44:38.095678][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.506668704378764e-07, Discarded policy loss value: -1.2539069857482308

[2020-01-27 14:44:38.927430][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 429.7850726795801

[2020-01-27 14:44:38.936047][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:44:42.553532][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.76018264 -0.         ...  0.17051738 -0.29277731
  0.12225994]

[2020-01-27 14:44:42.553913][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:44:42.973200][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.37514935  0.         ... -0.45945287  0.42806638
  0.03138649], shape=(4547,), dtype=float64)

[2020-01-27 14:44:43.087392][__main__.TRPOAgent.log][linesearch]: improvement: -0.15117034576052946

[2020-01-27 14:44:43.087891][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.780101540756883

[2020-01-27 14:44:43.884273][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 363.93795995519014

[2020-01-27 14:44:43.884689][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:44:43.892403][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:44:47.489850][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.08229623 -0.         ...  0.2518099  -0.33262029
  0.0808104 ]

[2020-01-27 14:44:47.490257][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:44:47.891469][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.39922028  0.         ...  0.05499901  0.00583104
 -0.06083005], shape=(4547,), dtype=float64)

[2020-01-27 14:44:47.985026][__main__.TRPOAgent.log][linesearch]: improvement: -0.1500004439737319

[2020-01-27 14:44:47.985528][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 4.548457744190238

[2020-01-27 14:44:48.730848][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 485.84374448840515

[2020-01-27 14:44:48.731246][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:44:48.738318][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:44:52.400224][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.14804453 -0.         ...  0.13072244  0.16629368
 -0.29701611]

[2020-01-27 14:44:52.400617][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:44:52.819598][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.57855695  0.         ... -0.21947995  0.29074575
 -0.07126581], shape=(4547,), dtype=float64)

[2020-01-27 14:44:52.904018][__main__.TRPOAgent.log][linesearch]: improvement: -0.20175366006068973

[2020-01-27 14:44:52.938512][__main__.TRPOAgent.log][linesearch]: improvement: -0.12617566258286939

[2020-01-27 14:44:52.967637][__main__.TRPOAgent.log][linesearch]: improvement: -0.07719932438075006

[2020-01-27 14:44:53.000070][__main__.TRPOAgent.log][linesearch]: improvement: -0.04677246880130781

[2020-01-27 14:44:53.031783][__main__.TRPOAgent.log][linesearch]: improvement: -0.028178973849374955

[2020-01-27 14:44:53.064349][__main__.TRPOAgent.log][linesearch]: improvement: -0.0169900189061174

[2020-01-27 14:44:53.096029][__main__.TRPOAgent.log][linesearch]: improvement: -0.010248326706658872

[2020-01-27 14:44:53.123968][__main__.TRPOAgent.log][linesearch]: improvement: -0.006166169445174674

[2020-01-27 14:44:53.158191][__main__.TRPOAgent.log][linesearch]: improvement: -0.0037058014403044126

[2020-01-27 14:44:53.194929][__main__.TRPOAgent.log][linesearch]: improvement: -0.0022250495905948853

[2020-01-27 14:44:53.195564][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.327563665981424e-07, Discarded policy loss value: -4.128865457829012

[2020-01-27 14:44:54.059733][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.6488503347753

[2020-01-27 14:44:54.063752][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 2532

[2020-01-27 14:44:56.201055][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.41027746 -0.         ... -0.25843317 -0.00555062
  0.26398379]

[2020-01-27 14:44:56.201421][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:44:56.500376][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.57226661  0.         ...  0.58263956 -0.82562225
  0.24298269], shape=(4547,), dtype=float64)

[2020-01-27 14:44:56.564792][__main__.TRPOAgent.log][linesearch]: improvement: -0.1818805599778477

[2020-01-27 14:44:56.591209][__main__.TRPOAgent.log][linesearch]: improvement: -0.12837989935177774

[2020-01-27 14:44:56.591665][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 6.6583306007112

[2020-01-27 14:44:57.041459][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 449.29316899803297

[2020-01-27 14:44:57.041876][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:44:57.061768][__main__.TRPOAgent.log][learning]: Episode #12

[2020-01-27 14:44:57.062180][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:44:57.112444][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:44:57.113012][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:44:57.113240][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:44:57.114427][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:45:00.539003][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1397

[2020-01-27 14:45:00.545668][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:45:00.549931][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:45:00.551186][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:45:00.550977][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:45:00.552682][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:45:03.667459][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1358

[2020-01-27 14:45:03.935515][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:45:03.936079][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:45:03.936943][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:45:03.937013][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:45:03.939020][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:45:06.047632][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 896

[2020-01-27 14:45:06.678860][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:45:06.680823][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:45:06.681352][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:45:06.681437][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:45:06.683356][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:45:10.046589][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:45:10.129085][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:45:10.129637][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:45:10.130150][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:45:10.130210][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:45:10.132177][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:45:13.568015][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:45:13.611941][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:45:13.612620][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:45:13.613283][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:45:13.613347][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:45:13.615422][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:45:16.860554][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1395

[2020-01-27 14:45:16.956160][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:45:16.956726][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:45:16.957301][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:45:16.957382][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:45:16.961129][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:45:18.690980][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 737

[2020-01-27 14:45:19.247617][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1267

[2020-01-27 14:45:19.248048][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:45:19.248901][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:45:19.248837][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:45:19.249944][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:45:22.061981][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1154

[2020-01-27 14:45:22.341302][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:45:22.341881][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:45:22.342513][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:45:22.342569][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:45:22.344178][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:45:23.998311][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 724

[2020-01-27 14:45:24.780272][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:45:24.781022][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:45:24.781615][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:45:24.781697][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:45:24.786535][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:45:27.704925][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1192

[2020-01-27 14:45:27.812201][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1349

[2020-01-27 14:45:27.812599][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:45:27.813314][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:45:27.813238][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:45:27.814418][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:45:29.306899][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 633

[2020-01-27 14:45:29.695575][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1029

[2020-01-27 14:45:29.696229][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:45:29.696904][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:45:29.696969][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:45:29.699457][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:45:33.190686][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:45:33.222167][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:45:33.222935][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:45:33.223979][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:45:33.223857][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:45:33.225037][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:45:36.673483][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:45:36.688008][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:45:36.688694][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:45:36.689416][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:45:36.689522][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:45:36.694913][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:45:38.973146][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 965

[2020-01-27 14:45:39.499309][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:45:39.499841][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:45:39.500568][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:45:39.500496][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:45:39.501626][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:45:41.186777][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 715

[2020-01-27 14:45:41.409716][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 884

[2020-01-27 14:45:41.410369][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:45:41.420117][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:45:41.882164][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:45:41.905367][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:45:41.906842][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 38195, Batch size: 4500, Number of batches: 9

[2020-01-27 14:45:41.907287][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:45:41.938357][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:45:45.464810][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.20704167 -0.         ... -0.02826079  0.11294929
 -0.0846885 ]

[2020-01-27 14:45:45.465213][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:45:45.887879][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.28278812  0.         ...  0.10225181 -0.12439975
  0.02214794], shape=(4547,), dtype=float64)

[2020-01-27 14:45:45.979423][__main__.TRPOAgent.log][linesearch]: improvement: -0.0841731337179854

[2020-01-27 14:45:46.014645][__main__.TRPOAgent.log][linesearch]: improvement: -0.05606004499994599

[2020-01-27 14:45:46.056279][__main__.TRPOAgent.log][linesearch]: improvement: -0.03536914286855675

[2020-01-27 14:45:46.091850][__main__.TRPOAgent.log][linesearch]: improvement: -0.021896190028821483

[2020-01-27 14:45:46.117992][__main__.TRPOAgent.log][linesearch]: improvement: -0.013397826653226197

[2020-01-27 14:45:46.155189][__main__.TRPOAgent.log][linesearch]: improvement: -0.008131829547536551

[2020-01-27 14:45:46.190163][__main__.TRPOAgent.log][linesearch]: improvement: -0.0049230442169672095

[2020-01-27 14:45:46.225527][__main__.TRPOAgent.log][linesearch]: improvement: -0.002970882136998032

[2020-01-27 14:45:46.261375][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017903415499316289

[2020-01-27 14:45:46.292919][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010783623286183541

[2020-01-27 14:45:46.293546][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.052918690509564e-07, Discarded policy loss value: -2.211410464586208

[2020-01-27 14:45:47.215836][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.21844609423437

[2020-01-27 14:45:47.223757][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:45:50.832678][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.44410663 -0.         ... -0.21490246  0.06102415
  0.1538783 ]

[2020-01-27 14:45:50.833217][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:45:51.246656][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.24804632  0.         ... -0.003733   -0.0433778
  0.04711079], shape=(4547,), dtype=float64)

[2020-01-27 14:45:51.331011][__main__.TRPOAgent.log][linesearch]: improvement: -0.11669080875030335

[2020-01-27 14:45:51.364588][__main__.TRPOAgent.log][linesearch]: improvement: -0.06769869865444722

[2020-01-27 14:45:51.398255][__main__.TRPOAgent.log][linesearch]: improvement: -0.04008707742474238

[2020-01-27 14:45:51.430004][__main__.TRPOAgent.log][linesearch]: improvement: -0.023892651767311257

[2020-01-27 14:45:51.470437][__main__.TRPOAgent.log][linesearch]: improvement: -0.014330740666292474

[2020-01-27 14:45:51.505245][__main__.TRPOAgent.log][linesearch]: improvement: -0.008591732517589268

[2020-01-27 14:45:51.538272][__main__.TRPOAgent.log][linesearch]: improvement: -0.005151256213832056

[2020-01-27 14:45:51.569865][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030889750897342694

[2020-01-27 14:45:51.605285][__main__.TRPOAgent.log][linesearch]: improvement: -0.001852124910558306

[2020-01-27 14:45:51.637112][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011107205585259017

[2020-01-27 14:45:51.637767][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.345567726798597e-07, Discarded policy loss value: -2.334867889466295

[2020-01-27 14:45:52.401984][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.76508830290487

[2020-01-27 14:45:52.407619][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:45:56.060550][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.26896749 -0.         ...  0.29856445 -0.32920217
  0.03063772]

[2020-01-27 14:45:56.060990][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:45:56.470244][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.03134206  0.         ...  0.32036392 -0.45462081
  0.13425689], shape=(4547,), dtype=float64)

[2020-01-27 14:45:56.570011][__main__.TRPOAgent.log][linesearch]: improvement: -0.09568069443185534

[2020-01-27 14:45:56.605835][__main__.TRPOAgent.log][linesearch]: improvement: -0.07967118562816067

[2020-01-27 14:45:56.606455][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 2.4070784815973867

[2020-01-27 14:45:57.370819][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.4223562134036

[2020-01-27 14:45:57.371231][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:45:57.382060][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:46:00.932887][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.13448769 -0.         ... -0.16720131 -0.02372586
  0.19092717]

[2020-01-27 14:46:00.933326][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:46:01.382391][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.21335018  0.         ...  0.73110387 -0.77033141
  0.03922753], shape=(4547,), dtype=float64)

[2020-01-27 14:46:01.473218][__main__.TRPOAgent.log][linesearch]: improvement: -0.07925567447902493

[2020-01-27 14:46:01.512602][__main__.TRPOAgent.log][linesearch]: improvement: -0.06315587218430463

[2020-01-27 14:46:01.537736][__main__.TRPOAgent.log][linesearch]: improvement: -0.04344083597677795

[2020-01-27 14:46:01.578090][__main__.TRPOAgent.log][linesearch]: improvement: -0.028075763248521346

[2020-01-27 14:46:01.619912][__main__.TRPOAgent.log][linesearch]: improvement: -0.01759103061357936

[2020-01-27 14:46:01.660415][__main__.TRPOAgent.log][linesearch]: improvement: -0.010822136205875799

[2020-01-27 14:46:01.695924][__main__.TRPOAgent.log][linesearch]: improvement: -0.006601657831194929

[2020-01-27 14:46:01.727860][__main__.TRPOAgent.log][linesearch]: improvement: -0.004011356349861606

[2020-01-27 14:46:01.765142][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024505023358385447

[2020-01-27 14:46:01.791120][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014974420188387239

[2020-01-27 14:46:01.791676][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.938653183240552e-07, Discarded policy loss value: -1.3079879579265516

[2020-01-27 14:46:02.563130][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.32834184227727

[2020-01-27 14:46:02.568539][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:46:06.126557][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.63674036 -0.         ...  0.08932297 -0.21115344
  0.12183048]

[2020-01-27 14:46:06.126952][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:46:06.550054][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.25469134  0.         ...  1.29957791 -1.12746508
 -0.17211282], shape=(4547,), dtype=float64)

[2020-01-27 14:46:06.637620][__main__.TRPOAgent.log][linesearch]: improvement: -0.06441891259420984

[2020-01-27 14:46:06.673798][__main__.TRPOAgent.log][linesearch]: improvement: -0.08465683695732795

[2020-01-27 14:46:06.709766][__main__.TRPOAgent.log][linesearch]: improvement: -0.0625888568924331

[2020-01-27 14:46:06.743250][__main__.TRPOAgent.log][linesearch]: improvement: -0.04092165070235265

[2020-01-27 14:46:06.779296][__main__.TRPOAgent.log][linesearch]: improvement: -0.025383902529029445

[2020-01-27 14:46:06.807134][__main__.TRPOAgent.log][linesearch]: improvement: -0.015338075480931135

[2020-01-27 14:46:06.845198][__main__.TRPOAgent.log][linesearch]: improvement: -0.0092069924335485

[2020-01-27 14:46:06.894524][__main__.TRPOAgent.log][linesearch]: improvement: -0.005551801653438826

[2020-01-27 14:46:06.929197][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033329695564625617

[2020-01-27 14:46:06.965951][__main__.TRPOAgent.log][linesearch]: improvement: -0.002001426796330863

[2020-01-27 14:46:06.967227][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.751386815964722e-07, Discarded policy loss value: -1.517331277391224

[2020-01-27 14:46:07.894812][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 310.94607862412454

[2020-01-27 14:46:07.900623][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:46:11.455708][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.26300864 -0.         ...  0.17056308 -0.17750572
  0.00694264]

[2020-01-27 14:46:11.456112][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:46:11.872317][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.15136499  0.         ...  0.17600143 -0.08720403
 -0.08879741], shape=(4547,), dtype=float64)

[2020-01-27 14:46:11.957848][__main__.TRPOAgent.log][linesearch]: improvement: -0.1567349490690323

[2020-01-27 14:46:11.989590][__main__.TRPOAgent.log][linesearch]: improvement: -0.09165641429001736

[2020-01-27 14:46:12.022346][__main__.TRPOAgent.log][linesearch]: improvement: -0.05437385694355967

[2020-01-27 14:46:12.053102][__main__.TRPOAgent.log][linesearch]: improvement: -0.03229017008528734

[2020-01-27 14:46:12.085961][__main__.TRPOAgent.log][linesearch]: improvement: -0.019097823876760067

[2020-01-27 14:46:12.120424][__main__.TRPOAgent.log][linesearch]: improvement: -0.011357244047469406

[2020-01-27 14:46:12.153981][__main__.TRPOAgent.log][linesearch]: improvement: -0.006802736280512978

[2020-01-27 14:46:12.183828][__main__.TRPOAgent.log][linesearch]: improvement: -0.004091088821679301

[2020-01-27 14:46:12.213570][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024623883227887777

[2020-01-27 14:46:12.248772][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014792071125270256

[2020-01-27 14:46:12.249377][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.699007477958839e-07, Discarded policy loss value: -0.6787208950626582

[2020-01-27 14:46:13.064168][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.08293121679117

[2020-01-27 14:46:13.070201][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:46:16.636767][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.08399921 -0.         ...  0.04576449 -0.04759036
  0.00182587]

[2020-01-27 14:46:16.637188][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:46:17.039479][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.25028127  0.         ... -0.51742953  0.49465961
  0.02276992], shape=(4547,), dtype=float64)

[2020-01-27 14:46:17.123569][__main__.TRPOAgent.log][linesearch]: improvement: -0.09089381875852554

[2020-01-27 14:46:17.158056][__main__.TRPOAgent.log][linesearch]: improvement: -0.059120340860699505

[2020-01-27 14:46:17.158602][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.961085597313122

[2020-01-27 14:46:17.929281][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.9303169305897

[2020-01-27 14:46:17.929688][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:46:17.937096][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:46:21.613314][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.54710832 -0.         ...  0.06737842 -0.08578697
  0.01840855]

[2020-01-27 14:46:21.613721][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:46:22.011413][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.97354256  0.         ... -0.21574466 -0.05689017
  0.27263482], shape=(4547,), dtype=float64)

[2020-01-27 14:46:22.108820][__main__.TRPOAgent.log][linesearch]: improvement: -0.40484117901068717

[2020-01-27 14:46:22.145450][__main__.TRPOAgent.log][linesearch]: improvement: -0.16590487364364304

[2020-01-27 14:46:22.179964][__main__.TRPOAgent.log][linesearch]: improvement: -0.06598292732174371

[2020-01-27 14:46:22.180463][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.7381745040590172

[2020-01-27 14:46:22.941902][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 338.83999249643915

[2020-01-27 14:46:22.942309][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:46:22.947692][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 2195

[2020-01-27 14:46:24.678224][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01689889 -0.         ... -0.19879937 -0.28621526
  0.48501463]

[2020-01-27 14:46:24.678596][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:46:24.952888][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.61791615  0.         ... -0.17314977  0.06812202
  0.10502775], shape=(4547,), dtype=float64)

[2020-01-27 14:46:25.015982][__main__.TRPOAgent.log][linesearch]: improvement: -0.4304686539147822

[2020-01-27 14:46:25.016440][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 3.982953066602651

[2020-01-27 14:46:25.421066][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 369.52694546992274

[2020-01-27 14:46:25.421485][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:46:25.447043][__main__.TRPOAgent.log][learning]: Episode #13

[2020-01-27 14:46:25.447598][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:46:25.504984][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:46:25.505555][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:46:25.506449][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:46:25.508645][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:46:27.431995][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 759

[2020-01-27 14:46:28.162992][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:46:28.163573][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:46:28.164220][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:46:28.164464][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:46:28.166175][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:46:29.513764][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 599

[2020-01-27 14:46:30.386553][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:46:30.388880][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:46:30.389754][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:46:30.389680][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:46:30.390777][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:46:32.756672][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1009

[2020-01-27 14:46:32.876433][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1077

[2020-01-27 14:46:32.876843][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:46:32.877566][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:46:32.877640][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:46:32.879709][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:46:34.122748][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 539

[2020-01-27 14:46:35.048710][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:46:35.049369][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:46:35.056490][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:46:35.056239][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:46:35.057772][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:46:37.810439][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1181

[2020-01-27 14:46:38.167820][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:46:38.168777][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:46:38.169459][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:46:38.169549][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:46:38.170672][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:46:40.942950][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1237

[2020-01-27 14:46:41.202795][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:46:41.203554][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:46:41.204486][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:46:41.204427][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:46:41.205696][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:46:43.871784][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1165

[2020-01-27 14:46:44.124324][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1356

[2020-01-27 14:46:44.124986][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:46:44.125868][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:46:44.125939][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:46:44.129655][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:46:46.036993][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 852

[2020-01-27 14:46:46.309183][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1065

[2020-01-27 14:46:46.309832][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:46:46.310527][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:46:46.310605][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:46:46.312372][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:46:48.523874][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 926

[2020-01-27 14:46:48.607150][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1014

[2020-01-27 14:46:48.607553][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:46:48.608247][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:46:48.608167][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:46:48.610297][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:46:50.193552][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 709

[2020-01-27 14:46:50.734802][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1198

[2020-01-27 14:46:50.735353][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:46:50.736010][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:46:50.736163][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:46:50.738539][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:46:53.904945][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1337

[2020-01-27 14:46:54.057100][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:46:54.058151][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:46:54.058790][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:46:54.058920][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:46:54.060426][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:46:57.266040][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1381

[2020-01-27 14:46:57.453648][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:46:57.454189][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:46:57.455139][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:46:57.455210][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:46:57.457908][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:47:00.286565][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1256

[2020-01-27 14:47:00.552765][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:47:00.553684][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:47:00.554701][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:47:00.554486][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:47:00.555844][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:47:02.109987][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 658

[2020-01-27 14:47:02.840969][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1312

[2020-01-27 14:47:02.841445][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:47:02.842085][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:47:02.842237][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:47:02.844588][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:47:04.166394][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 576

[2020-01-27 14:47:05.103713][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:47:05.104342][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:47:05.113347][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:47:05.492087][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:47:05.518903][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:47:05.520351][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 34706, Batch size: 4500, Number of batches: 8

[2020-01-27 14:47:05.520780][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:47:05.550219][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:47:09.151948][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.38065692 -0.         ... -0.35351617  0.48867111
 -0.13515495]

[2020-01-27 14:47:09.152368][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:09.549788][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.2966064   0.         ...  0.04164202 -0.04288542
  0.00124341], shape=(4547,), dtype=float64)

[2020-01-27 14:47:09.636840][__main__.TRPOAgent.log][linesearch]: improvement: -0.16772665445540524

[2020-01-27 14:47:09.637359][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.3545065650526016

[2020-01-27 14:47:10.509964][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 362.1304816534583

[2020-01-27 14:47:10.510433][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:47:10.527288][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:47:14.146770][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.24623443 -0.         ... -0.13897675  0.19236939
 -0.05339264]

[2020-01-27 14:47:14.147171][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:14.545046][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.41035155  0.         ... -0.25279613  0.28209153
 -0.0292954 ], shape=(4547,), dtype=float64)

[2020-01-27 14:47:14.640853][__main__.TRPOAgent.log][linesearch]: improvement: -0.10561309732155388

[2020-01-27 14:47:14.677879][__main__.TRPOAgent.log][linesearch]: improvement: -0.07852563085010633

[2020-01-27 14:47:14.711787][__main__.TRPOAgent.log][linesearch]: improvement: -0.05139907922270259

[2020-01-27 14:47:14.741400][__main__.TRPOAgent.log][linesearch]: improvement: -0.03152468829733435

[2020-01-27 14:47:14.776196][__main__.TRPOAgent.log][linesearch]: improvement: -0.019084038256800717

[2020-01-27 14:47:14.807612][__main__.TRPOAgent.log][linesearch]: improvement: -0.011446662833992871

[2020-01-27 14:47:14.847343][__main__.TRPOAgent.log][linesearch]: improvement: -0.006811862551658887

[2020-01-27 14:47:14.879243][__main__.TRPOAgent.log][linesearch]: improvement: -0.004116144107311204

[2020-01-27 14:47:14.917793][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024625393406716434

[2020-01-27 14:47:14.949750][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014728553705203318

[2020-01-27 14:47:14.950379][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.335687606099588e-07, Discarded policy loss value: -2.187484006223841

[2020-01-27 14:47:15.719810][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.16558506036176

[2020-01-27 14:47:15.725187][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:47:19.338137][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.07795441 -0.         ... -0.18061171  0.0408399
  0.13977181]

[2020-01-27 14:47:19.338552][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:19.767247][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.0830901   0.         ... -0.42451922  0.54235171
 -0.11783248], shape=(4547,), dtype=float64)

[2020-01-27 14:47:19.857454][__main__.TRPOAgent.log][linesearch]: improvement: -0.05230024854190707

[2020-01-27 14:47:19.891789][__main__.TRPOAgent.log][linesearch]: improvement: -0.0465977906051489

[2020-01-27 14:47:19.927835][__main__.TRPOAgent.log][linesearch]: improvement: -0.040072196847974695

[2020-01-27 14:47:19.958578][__main__.TRPOAgent.log][linesearch]: improvement: -0.03058941579344787

[2020-01-27 14:47:19.991859][__main__.TRPOAgent.log][linesearch]: improvement: -0.021945160400575814

[2020-01-27 14:47:20.022324][__main__.TRPOAgent.log][linesearch]: improvement: -0.013907286381392936

[2020-01-27 14:47:20.054886][__main__.TRPOAgent.log][linesearch]: improvement: -0.008505236694822838

[2020-01-27 14:47:20.085793][__main__.TRPOAgent.log][linesearch]: improvement: -0.005155386567638809

[2020-01-27 14:47:20.120651][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030992166999170845

[2020-01-27 14:47:20.147481][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018702224462550099

[2020-01-27 14:47:20.147984][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.295261361458754e-07, Discarded policy loss value: -1.7038683356648474

[2020-01-27 14:47:20.923725][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.3544298053842

[2020-01-27 14:47:20.929189][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:47:24.493491][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.34162971 -0.         ...  0.26164781 -0.26283379
  0.00118599]

[2020-01-27 14:47:24.493891][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:24.895147][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.96525552  0.         ...  0.01816355 -0.08592893
  0.06776539], shape=(4547,), dtype=float64)

[2020-01-27 14:47:24.994279][__main__.TRPOAgent.log][linesearch]: improvement: -0.19969101535622702

[2020-01-27 14:47:24.994881][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 4.37935174422308

[2020-01-27 14:47:25.805813][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 430.24370667781216

[2020-01-27 14:47:25.806229][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:47:25.813800][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:47:29.417888][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.12217262 -0.         ... -0.09819033  0.25199561
 -0.15380528]

[2020-01-27 14:47:29.418295][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:29.811659][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.00000000e+00  2.15448821e+00  0.00000000e+00 ... -2.00643401e-01
  1.05780483e-03  1.99585597e-01], shape=(4547,), dtype=float64)

[2020-01-27 14:47:29.908145][__main__.TRPOAgent.log][linesearch]: improvement: -0.20807134656219062

[2020-01-27 14:47:29.944951][__main__.TRPOAgent.log][linesearch]: improvement: -0.13845559706013333

[2020-01-27 14:47:29.981142][__main__.TRPOAgent.log][linesearch]: improvement: -0.08840905661877496

[2020-01-27 14:47:30.013744][__main__.TRPOAgent.log][linesearch]: improvement: -0.05265571200933117

[2020-01-27 14:47:30.046162][__main__.TRPOAgent.log][linesearch]: improvement: -0.030775101020780404

[2020-01-27 14:47:30.081548][__main__.TRPOAgent.log][linesearch]: improvement: -0.017781315313025292

[2020-01-27 14:47:30.112956][__main__.TRPOAgent.log][linesearch]: improvement: -0.01040425556458624

[2020-01-27 14:47:30.149438][__main__.TRPOAgent.log][linesearch]: improvement: -0.0061982640799951305

[2020-01-27 14:47:30.181853][__main__.TRPOAgent.log][linesearch]: improvement: -0.0036628060284096264

[2020-01-27 14:47:30.215262][__main__.TRPOAgent.log][linesearch]: improvement: -0.002167317593398721

[2020-01-27 14:47:30.215872][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.387520756175812e-07, Discarded policy loss value: -2.112955133306926

[2020-01-27 14:47:30.980575][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 364.11091511627944

[2020-01-27 14:47:30.985883][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:47:34.642227][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.04049502 -0.         ... -0.24055303 -0.04715496
  0.28770799]

[2020-01-27 14:47:34.642626][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:35.037004][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.6517296   0.         ... -0.40726127  0.11930235
  0.28795892], shape=(4547,), dtype=float64)

[2020-01-27 14:47:35.129273][__main__.TRPOAgent.log][linesearch]: improvement: -0.15065112241101186

[2020-01-27 14:47:35.165909][__main__.TRPOAgent.log][linesearch]: improvement: -0.0847889224467524

[2020-01-27 14:47:35.195848][__main__.TRPOAgent.log][linesearch]: improvement: -0.048619591912844484

[2020-01-27 14:47:35.230575][__main__.TRPOAgent.log][linesearch]: improvement: -0.02899244618730723

[2020-01-27 14:47:35.265979][__main__.TRPOAgent.log][linesearch]: improvement: -0.01776972840198443

[2020-01-27 14:47:35.302622][__main__.TRPOAgent.log][linesearch]: improvement: -0.011219489071311983

[2020-01-27 14:47:35.328203][__main__.TRPOAgent.log][linesearch]: improvement: -0.007354818840375366

[2020-01-27 14:47:35.365987][__main__.TRPOAgent.log][linesearch]: improvement: -0.005056791498051716

[2020-01-27 14:47:35.403298][__main__.TRPOAgent.log][linesearch]: improvement: -0.0031413704552081256

[2020-01-27 14:47:35.428955][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018669146223828648

[2020-01-27 14:47:35.429485][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.591332583455791e-07, Discarded policy loss value: -4.138920434779438

[2020-01-27 14:47:36.217635][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.450958763744

[2020-01-27 14:47:36.224248][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:47:39.906479][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00045429 -0.32357565 -0.         ...  0.18155049 -0.09917762
 -0.08237287]

[2020-01-27 14:47:39.906856][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:40.314723][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.09353972 -3.3540191   0.         ...  0.64828015 -0.0617057
 -0.58657445], shape=(4547,), dtype=float64)

[2020-01-27 14:47:40.401147][__main__.TRPOAgent.log][linesearch]: improvement: -0.3377860096315177

[2020-01-27 14:47:40.438841][__main__.TRPOAgent.log][linesearch]: improvement: -0.22949484690339084

[2020-01-27 14:47:40.471601][__main__.TRPOAgent.log][linesearch]: improvement: -0.1559911000049481

[2020-01-27 14:47:40.506568][__main__.TRPOAgent.log][linesearch]: improvement: -0.08317844479498726

[2020-01-27 14:47:40.507051][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 3.5251613265094055

[2020-01-27 14:47:41.258277][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 428.62287398433864

[2020-01-27 14:47:41.258684][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:47:41.267850][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 3206

[2020-01-27 14:47:43.861790][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.51994673 -0.         ... -0.30061172  0.01807176
  0.28253996]

[2020-01-27 14:47:43.862179][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:47:44.184876][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.32025774  0.         ...  0.11146244  0.16631556
 -0.277778  ], shape=(4547,), dtype=float64)

[2020-01-27 14:47:44.263908][__main__.TRPOAgent.log][linesearch]: improvement: -0.07849059429656835

[2020-01-27 14:47:44.293856][__main__.TRPOAgent.log][linesearch]: improvement: -0.056557160610234636

[2020-01-27 14:47:44.323217][__main__.TRPOAgent.log][linesearch]: improvement: -0.04934443927690357

[2020-01-27 14:47:44.351749][__main__.TRPOAgent.log][linesearch]: improvement: -0.03873813106894319

[2020-01-27 14:47:44.380661][__main__.TRPOAgent.log][linesearch]: improvement: -0.02606473818506494

[2020-01-27 14:47:44.410287][__main__.TRPOAgent.log][linesearch]: improvement: -0.016531725120350504

[2020-01-27 14:47:44.438814][__main__.TRPOAgent.log][linesearch]: improvement: -0.009899229819788413

[2020-01-27 14:47:44.466219][__main__.TRPOAgent.log][linesearch]: improvement: -0.005916667842607026

[2020-01-27 14:47:44.492787][__main__.TRPOAgent.log][linesearch]: improvement: -0.0035718287462978004

[2020-01-27 14:47:44.519911][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021439073110380846

[2020-01-27 14:47:44.520357][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.25145713358353e-07, Discarded policy loss value: -2.0449058934361704

[2020-01-27 14:47:45.073959][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 379.013943957212

[2020-01-27 14:47:45.089013][__main__.TRPOAgent.log][learning]: Episode #14

[2020-01-27 14:47:45.089491][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:47:45.137538][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:47:45.138551][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:47:45.138148][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:47:45.140287][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:47:47.219881][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 865

[2020-01-27 14:47:47.959059][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:47:47.959699][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:47:47.960928][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:47:47.960990][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:47:47.963683][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:47:51.385837][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:47:51.426997][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:47:51.427773][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:47:51.428516][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:47:51.428600][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:47:51.430972][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:47:54.905634][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:47:54.915285][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:47:54.915808][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:47:54.916653][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:47:54.916747][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:47:54.920006][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:47:58.366168][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:47:58.456739][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:47:58.457261][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:47:58.457875][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:47:58.457808][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:47:58.458779][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:48:01.895319][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:48:01.942413][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:48:01.943031][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:48:01.943629][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:48:01.943768][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:48:01.945695][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:48:05.224218][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1399

[2020-01-27 14:48:05.372777][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:48:05.373896][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:48:05.374655][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:48:05.374393][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:48:05.375553][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:48:08.870121][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:48:08.883017][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1473

[2020-01-27 14:48:08.883637][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:48:08.884260][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:48:08.884373][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:48:08.886391][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:48:12.196973][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:48:12.253889][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:48:12.254441][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:48:12.255230][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:48:12.255149][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:48:12.256422][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:48:15.712880][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:48:15.760140][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:48:15.760718][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:48:15.761360][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:48:15.761424][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:48:15.763377][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:48:19.132610][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:48:19.237893][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:48:19.238454][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:48:19.238990][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:48:19.239069][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:48:19.242648][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:48:22.655857][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:48:22.663521][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:48:22.664185][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:48:22.664757][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:48:22.664831][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:48:22.667413][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:48:26.118984][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:48:26.139111][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:48:26.139786][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:48:26.140412][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:48:26.140493][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:48:26.142210][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:48:29.560228][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:48:29.601649][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:48:29.602230][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:48:29.602747][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:48:29.602809][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:48:29.604722][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:48:33.032040][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:48:33.067375][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:48:33.068067][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:48:33.068904][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:48:33.069012][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:48:33.071268][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:48:36.460527][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:48:36.506280][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:48:36.506794][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:48:36.522037][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:48:37.130753][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:48:37.161739][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:48:37.163519][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 44237, Batch size: 4500, Number of batches: 10

[2020-01-27 14:48:37.164088][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:48:37.202184][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:48:40.814373][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02424103 -0.         ... -0.10345638  0.10385771
 -0.00040133]

[2020-01-27 14:48:40.814752][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:48:41.216428][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.27694295  0.         ... -0.59086413  0.82666276
 -0.23579863], shape=(4547,), dtype=float64)

[2020-01-27 14:48:41.306948][__main__.TRPOAgent.log][linesearch]: improvement: -0.17630606394758308

[2020-01-27 14:48:41.342767][__main__.TRPOAgent.log][linesearch]: improvement: -0.11555698490012922

[2020-01-27 14:48:41.376903][__main__.TRPOAgent.log][linesearch]: improvement: -0.07614156272402584

[2020-01-27 14:48:41.412210][__main__.TRPOAgent.log][linesearch]: improvement: -0.04474620028127685

[2020-01-27 14:48:41.447375][__main__.TRPOAgent.log][linesearch]: improvement: -0.02691713930919226

[2020-01-27 14:48:41.480743][__main__.TRPOAgent.log][linesearch]: improvement: -0.015909120207921568

[2020-01-27 14:48:41.514959][__main__.TRPOAgent.log][linesearch]: improvement: -0.009460805284628115

[2020-01-27 14:48:41.545456][__main__.TRPOAgent.log][linesearch]: improvement: -0.005600681106103789

[2020-01-27 14:48:41.578674][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033081347090804236

[2020-01-27 14:48:41.610743][__main__.TRPOAgent.log][linesearch]: improvement: -0.001963845534402875

[2020-01-27 14:48:41.611343][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.038627417203278e-07, Discarded policy loss value: -3.856984780140087

[2020-01-27 14:48:42.489647][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.44470349231904

[2020-01-27 14:48:42.495880][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:48:46.163547][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.24768226 -0.         ...  0.06926927 -0.01449404
 -0.05477522]

[2020-01-27 14:48:46.163947][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:48:46.562356][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.13237731  0.         ... -0.07670187  0.17899117
 -0.10228929], shape=(4547,), dtype=float64)

[2020-01-27 14:48:46.648112][__main__.TRPOAgent.log][linesearch]: improvement: -0.10944744052169086

[2020-01-27 14:48:46.680334][__main__.TRPOAgent.log][linesearch]: improvement: -0.06420601467045206

[2020-01-27 14:48:46.680939][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.9822320993064608

[2020-01-27 14:48:47.423712][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.6338296532682

[2020-01-27 14:48:47.424131][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:48:47.431768][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:48:51.061280][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.73500885 -0.         ...  0.29583082 -0.02465603
 -0.27117479]

[2020-01-27 14:48:51.061670][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:48:51.463569][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.58057615  0.         ...  0.48375171 -0.74193912
  0.25818741], shape=(4547,), dtype=float64)

[2020-01-27 14:48:51.553816][__main__.TRPOAgent.log][linesearch]: improvement: -0.1537598794607321

[2020-01-27 14:48:51.589936][__main__.TRPOAgent.log][linesearch]: improvement: -0.08718471102775882

[2020-01-27 14:48:51.590476][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.9781779037143424

[2020-01-27 14:48:52.361213][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.41112949206706

[2020-01-27 14:48:52.361628][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:48:52.369308][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:48:56.005467][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.3383204  -0.         ... -0.15608166  0.0224816
  0.13360006]

[2020-01-27 14:48:56.005872][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:48:56.413534][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.30699968  0.         ... -0.07349217 -0.10241785
  0.17591002], shape=(4547,), dtype=float64)

[2020-01-27 14:48:56.499369][__main__.TRPOAgent.log][linesearch]: improvement: -0.12864441304609875

[2020-01-27 14:48:56.531899][__main__.TRPOAgent.log][linesearch]: improvement: -0.07430395452985206

[2020-01-27 14:48:56.532483][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.8647333038890682

[2020-01-27 14:48:57.294419][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.1115426086954

[2020-01-27 14:48:57.294834][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:48:57.302520][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:49:00.945289][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.47550282 -0.         ... -0.14623838  0.11968388
  0.0265545 ]

[2020-01-27 14:49:00.945734][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:49:01.376864][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.68039021  0.         ...  0.25292546 -0.01087755
 -0.24204791], shape=(4547,), dtype=float64)

[2020-01-27 14:49:01.467623][__main__.TRPOAgent.log][linesearch]: improvement: -0.08165823855144516

[2020-01-27 14:49:01.468107][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.6076833010408572

[2020-01-27 14:49:02.264789][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.67602343238116

[2020-01-27 14:49:02.265224][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:49:02.273300][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:49:05.876925][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03842751 -0.         ...  0.10410107 -0.15828275
  0.05418167]

[2020-01-27 14:49:05.877345][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:49:06.280273][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.80119601  0.         ...  0.32530312 -0.56946839
  0.24416526], shape=(4547,), dtype=float64)

[2020-01-27 14:49:06.364656][__main__.TRPOAgent.log][linesearch]: improvement: -0.09315158269140023

[2020-01-27 14:49:06.365153][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.8315126322330072

[2020-01-27 14:49:07.154730][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.9683642017516

[2020-01-27 14:49:07.155150][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:49:07.164467][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:49:10.752794][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.06053128 -0.         ...  0.05219109  0.0074759
 -0.05966699]

[2020-01-27 14:49:10.753202][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:49:11.152376][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.07691127  0.         ...  0.02098426  0.01584797
 -0.03683223], shape=(4547,), dtype=float64)

[2020-01-27 14:49:11.239744][__main__.TRPOAgent.log][linesearch]: improvement: -0.15191852046639465

[2020-01-27 14:49:11.277889][__main__.TRPOAgent.log][linesearch]: improvement: -0.09440400700905305

[2020-01-27 14:49:11.308810][__main__.TRPOAgent.log][linesearch]: improvement: -0.05809057177769561

[2020-01-27 14:49:11.345178][__main__.TRPOAgent.log][linesearch]: improvement: -0.036043732906286374

[2020-01-27 14:49:11.377136][__main__.TRPOAgent.log][linesearch]: improvement: -0.02209545262202295

[2020-01-27 14:49:11.418522][__main__.TRPOAgent.log][linesearch]: improvement: -0.013618467926797773

[2020-01-27 14:49:11.451997][__main__.TRPOAgent.log][linesearch]: improvement: -0.008273090691638174

[2020-01-27 14:49:11.486412][__main__.TRPOAgent.log][linesearch]: improvement: -0.005047750199510981

[2020-01-27 14:49:11.517756][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030630681133324567

[2020-01-27 14:49:11.552962][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018374594516504805

[2020-01-27 14:49:11.553491][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.653433553749522e-07, Discarded policy loss value: -0.8090333545336522

[2020-01-27 14:49:12.367702][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.1459411697692

[2020-01-27 14:49:12.373087][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:49:16.010261][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.04075466 -0.         ... -0.1386217   0.15577045
 -0.01714874]

[2020-01-27 14:49:16.010803][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:49:16.412236][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.00165655  0.         ...  0.22941153 -0.38406733
  0.15465581], shape=(4547,), dtype=float64)

[2020-01-27 14:49:16.505883][__main__.TRPOAgent.log][linesearch]: improvement: -0.1474590941281515

[2020-01-27 14:49:16.540835][__main__.TRPOAgent.log][linesearch]: improvement: -0.095883117277485

[2020-01-27 14:49:16.541446][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.4308940343277552

[2020-01-27 14:49:17.292777][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.37108144017185

[2020-01-27 14:49:17.293188][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:49:17.300659][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:49:20.951254][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.18682544 -0.         ...  0.09794105 -0.11849845
  0.02055739]

[2020-01-27 14:49:20.951665][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:49:21.342591][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          3.52538256  0.         ... -0.15801202  0.05916197
  0.09885005], shape=(4547,), dtype=float64)

[2020-01-27 14:49:21.432823][__main__.TRPOAgent.log][linesearch]: improvement: -0.19606581811720547

[2020-01-27 14:49:21.467867][__main__.TRPOAgent.log][linesearch]: improvement: -0.13994397990668528

[2020-01-27 14:49:21.499707][__main__.TRPOAgent.log][linesearch]: improvement: -0.09253098237926105

[2020-01-27 14:49:21.534738][__main__.TRPOAgent.log][linesearch]: improvement: -0.05890066438390873

[2020-01-27 14:49:21.568027][__main__.TRPOAgent.log][linesearch]: improvement: -0.03693758919282075

[2020-01-27 14:49:21.601230][__main__.TRPOAgent.log][linesearch]: improvement: -0.023137049365512752

[2020-01-27 14:49:21.638173][__main__.TRPOAgent.log][linesearch]: improvement: -0.014635023122989033

[2020-01-27 14:49:21.676444][__main__.TRPOAgent.log][linesearch]: improvement: -0.009346204168057426

[2020-01-27 14:49:21.710328][__main__.TRPOAgent.log][linesearch]: improvement: -0.005884557183171157

[2020-01-27 14:49:21.742746][__main__.TRPOAgent.log][linesearch]: improvement: -0.0036042750916402966

[2020-01-27 14:49:21.743250][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.920326578000066e-07, Discarded policy loss value: -2.7962542859377373

[2020-01-27 14:49:22.529878][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.3151782781409

[2020-01-27 14:49:22.534254][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 3737

[2020-01-27 14:49:25.575233][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.78019054 -0.         ... -0.22478518 -0.37329898
  0.59808416]

[2020-01-27 14:49:25.575663][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:49:25.941599][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -5.19780214  0.         ... -3.26591288  1.42797746
  1.83793542], shape=(4547,), dtype=float64)

[2020-01-27 14:49:26.013992][__main__.TRPOAgent.log][linesearch]: improvement: -11.56395469908266

[2020-01-27 14:49:26.045032][__main__.TRPOAgent.log][linesearch]: improvement: -4.897622410602391

[2020-01-27 14:49:26.074431][__main__.TRPOAgent.log][linesearch]: improvement: -1.794188083652421

[2020-01-27 14:49:26.103408][__main__.TRPOAgent.log][linesearch]: improvement: -0.7082081892096255

[2020-01-27 14:49:26.132703][__main__.TRPOAgent.log][linesearch]: improvement: -0.2857015992337113

[2020-01-27 14:49:26.133366][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 4, New policy loss value: 1.3396275549392558

[2020-01-27 14:49:26.771148][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 334.95490660632913

[2020-01-27 14:49:26.771563][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:49:26.791375][__main__.TRPOAgent.log][learning]: Episode #15

[2020-01-27 14:49:26.791764][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:49:26.848165][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:49:26.850727][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:49:26.848991][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:49:26.852057][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:49:30.569401][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:49:30.575532][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:49:30.576121][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:49:30.577549][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:49:30.577611][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:49:30.580775][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:49:34.024178][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:49:34.082793][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:49:34.083384][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:49:34.084029][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:49:34.084095][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:49:34.086119][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:49:37.529923][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:49:37.549082][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:49:37.550582][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:49:37.551395][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:49:37.551522][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:49:37.556914][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:49:41.046253][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:49:41.055984][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:49:41.057221][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:49:41.057982][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:49:41.057912][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:49:41.060769][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:49:44.490311][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:49:44.619963][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:49:44.620480][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:49:44.621282][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:49:44.621350][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:49:44.623540][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:49:48.037940][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:49:48.098916][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:49:48.099723][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:49:48.100350][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:49:48.100415][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:49:48.102920][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:49:51.626935][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:49:51.643530][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:49:51.644717][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:49:51.645464][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:49:51.645654][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:49:51.648033][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:49:55.177885][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:49:55.180725][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:49:55.182082][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:49:55.182954][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:49:55.183008][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:49:55.184128][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:49:58.574508][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:49:58.661514][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:49:58.662214][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:49:58.662818][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:49:58.662884][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:49:58.664968][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:50:02.150619][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:50:02.239538][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:50:02.240113][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:50:02.240727][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:50:02.240831][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:50:02.243628][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:50:05.691632][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:50:05.741484][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:50:05.742066][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:50:05.742603][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:50:05.742671][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:50:05.746402][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:50:09.083799][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:50:09.170559][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:50:09.171167][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:50:09.171957][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:50:09.171695][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:50:09.172885][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:50:12.525758][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:50:12.627310][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:50:12.628130][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:50:12.629046][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:50:12.629761][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:50:12.628948][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:50:16.047723][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:50:16.138855][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:50:16.139313][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:50:16.139807][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:50:16.139874][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:50:16.141712][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:50:19.652361][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:50:19.660059][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:50:19.660614][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:50:19.673630][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:50:20.301292][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:50:20.332699][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:50:20.334117][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:50:20.334491][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:50:20.367266][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:50:23.926239][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.1063897  -0.         ... -0.20174303  0.15916695
  0.04257608]

[2020-01-27 14:50:23.926624][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:50:24.324701][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.53670894  0.         ... -0.02221967  0.15741727
 -0.1351976 ], shape=(4547,), dtype=float64)

[2020-01-27 14:50:24.421626][__main__.TRPOAgent.log][linesearch]: improvement: -0.1283848841513313

[2020-01-27 14:50:24.459170][__main__.TRPOAgent.log][linesearch]: improvement: -0.07630166645765524

[2020-01-27 14:50:24.494206][__main__.TRPOAgent.log][linesearch]: improvement: -0.045279839768614494

[2020-01-27 14:50:24.530310][__main__.TRPOAgent.log][linesearch]: improvement: -0.027712179085091826

[2020-01-27 14:50:24.557131][__main__.TRPOAgent.log][linesearch]: improvement: -0.017093514623760964

[2020-01-27 14:50:24.598308][__main__.TRPOAgent.log][linesearch]: improvement: -0.01024017923760745

[2020-01-27 14:50:24.634406][__main__.TRPOAgent.log][linesearch]: improvement: -0.00657045896630537

[2020-01-27 14:50:24.663636][__main__.TRPOAgent.log][linesearch]: improvement: -0.004136324873637243

[2020-01-27 14:50:24.708361][__main__.TRPOAgent.log][linesearch]: improvement: -0.002544090794375009

[2020-01-27 14:50:24.743852][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015322640678450128

[2020-01-27 14:50:24.744360][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.351486986522557e-07, Discarded policy loss value: -2.585090291911035

[2020-01-27 14:50:25.539957][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.67789340345473

[2020-01-27 14:50:25.545464][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:50:29.090046][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.29266769 -0.         ...  0.22628932 -0.35435593
  0.12806661]

[2020-01-27 14:50:29.090428][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:50:29.521029][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.08096947  0.         ...  0.3973993  -0.60325152
  0.20585222], shape=(4547,), dtype=float64)

[2020-01-27 14:50:29.608445][__main__.TRPOAgent.log][linesearch]: improvement: -0.2489153272663239

[2020-01-27 14:50:29.647080][__main__.TRPOAgent.log][linesearch]: improvement: -0.13551624427137843

[2020-01-27 14:50:29.677943][__main__.TRPOAgent.log][linesearch]: improvement: -0.07817642538188779

[2020-01-27 14:50:29.715378][__main__.TRPOAgent.log][linesearch]: improvement: -0.04553242668107671

[2020-01-27 14:50:29.745100][__main__.TRPOAgent.log][linesearch]: improvement: -0.026790088195534245

[2020-01-27 14:50:29.778622][__main__.TRPOAgent.log][linesearch]: improvement: -0.01502606932117101

[2020-01-27 14:50:29.809283][__main__.TRPOAgent.log][linesearch]: improvement: -0.00874843889222443

[2020-01-27 14:50:29.844943][__main__.TRPOAgent.log][linesearch]: improvement: -0.005054329098707955

[2020-01-27 14:50:29.878965][__main__.TRPOAgent.log][linesearch]: improvement: -0.0029755183423915144

[2020-01-27 14:50:29.912621][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017531170104858607

[2020-01-27 14:50:29.913154][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.359568470720805e-07, Discarded policy loss value: -0.48284187254937105

[2020-01-27 14:50:30.792071][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.90943815177496

[2020-01-27 14:50:30.798274][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:50:34.399480][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.41538537 -0.         ... -0.02561298 -0.20880551
  0.2344185 ]

[2020-01-27 14:50:34.399906][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:50:34.837674][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.28177699  0.         ...  0.31794076 -0.01861123
 -0.29932953], shape=(4547,), dtype=float64)

[2020-01-27 14:50:34.922923][__main__.TRPOAgent.log][linesearch]: improvement: -0.1988777977319125

[2020-01-27 14:50:34.923412][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.8252121228910898

[2020-01-27 14:50:35.701965][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.9809625891583

[2020-01-27 14:50:35.702374][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:50:35.710179][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:50:39.303972][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.38579943 -0.         ...  0.16216632  0.08789227
 -0.25005859]

[2020-01-27 14:50:39.304366][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:50:39.719627][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.14202463  0.         ... -0.71861013  0.54310483
  0.17550529], shape=(4547,), dtype=float64)

[2020-01-27 14:50:39.812374][__main__.TRPOAgent.log][linesearch]: improvement: -0.18561634255041104

[2020-01-27 14:50:39.850433][__main__.TRPOAgent.log][linesearch]: improvement: -0.10720187894925726

[2020-01-27 14:50:39.851052][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.4299990232837665

[2020-01-27 14:50:40.625041][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.380341382293

[2020-01-27 14:50:40.625703][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:50:40.637462][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:50:44.189299][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.35756185 -0.         ...  0.03313261 -0.21505684
  0.18192424]

[2020-01-27 14:50:44.189722][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:50:44.596910][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.13735925  0.         ... -0.20484857  0.2059203
 -0.00107173], shape=(4547,), dtype=float64)

[2020-01-27 14:50:44.688784][__main__.TRPOAgent.log][linesearch]: improvement: -0.11520130734250089

[2020-01-27 14:50:44.724825][__main__.TRPOAgent.log][linesearch]: improvement: -0.0732368756929963

[2020-01-27 14:50:44.755319][__main__.TRPOAgent.log][linesearch]: improvement: -0.044959598627162145

[2020-01-27 14:50:44.784386][__main__.TRPOAgent.log][linesearch]: improvement: -0.027343096149352264

[2020-01-27 14:50:44.821294][__main__.TRPOAgent.log][linesearch]: improvement: -0.016595312732624934

[2020-01-27 14:50:44.856678][__main__.TRPOAgent.log][linesearch]: improvement: -0.010033775644661347

[2020-01-27 14:50:44.891237][__main__.TRPOAgent.log][linesearch]: improvement: -0.006059134692072421

[2020-01-27 14:50:44.928192][__main__.TRPOAgent.log][linesearch]: improvement: -0.0036404338065326725

[2020-01-27 14:50:44.965602][__main__.TRPOAgent.log][linesearch]: improvement: -0.002185984453593859

[2020-01-27 14:50:44.998355][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013127962892649503

[2020-01-27 14:50:44.998873][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.299323538957104e-07, Discarded policy loss value: -0.7621992409590117

[2020-01-27 14:50:45.873230][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.9427762365

[2020-01-27 14:50:45.879255][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:50:49.450391][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01954654 -0.         ... -0.31330821  0.26689981
  0.0464084 ]

[2020-01-27 14:50:49.450794][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:50:49.883980][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.06942728  0.         ... -0.17077619  0.26872123
 -0.09794505], shape=(4547,), dtype=float64)

[2020-01-27 14:50:49.977241][__main__.TRPOAgent.log][linesearch]: improvement: -0.21716377880781845

[2020-01-27 14:50:50.014509][__main__.TRPOAgent.log][linesearch]: improvement: -0.14179596838881414

[2020-01-27 14:50:50.051732][__main__.TRPOAgent.log][linesearch]: improvement: -0.08893140225662055

[2020-01-27 14:50:50.075996][__main__.TRPOAgent.log][linesearch]: improvement: -0.05458928729853296

[2020-01-27 14:50:50.112221][__main__.TRPOAgent.log][linesearch]: improvement: -0.033148333490894855

[2020-01-27 14:50:50.150476][__main__.TRPOAgent.log][linesearch]: improvement: -0.02001908913005246

[2020-01-27 14:50:50.180205][__main__.TRPOAgent.log][linesearch]: improvement: -0.01205493210941233

[2020-01-27 14:50:50.214112][__main__.TRPOAgent.log][linesearch]: improvement: -0.007247943795356981

[2020-01-27 14:50:50.245631][__main__.TRPOAgent.log][linesearch]: improvement: -0.004354173785660942

[2020-01-27 14:50:50.281536][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026145052875796715

[2020-01-27 14:50:50.282112][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.56321244204416e-07, Discarded policy loss value: -1.290721234575661

[2020-01-27 14:50:51.088268][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.85194747453943

[2020-01-27 14:50:51.093694][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:50:54.660628][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03284486 -0.         ... -0.01126284  0.00798995
  0.00327289]

[2020-01-27 14:50:54.661029][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:50:55.118712][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.67353725  0.         ...  0.16524226 -0.29341675
  0.12817449], shape=(4547,), dtype=float64)

[2020-01-27 14:50:55.217828][__main__.TRPOAgent.log][linesearch]: improvement: -0.0784867848526897

[2020-01-27 14:50:55.252853][__main__.TRPOAgent.log][linesearch]: improvement: -0.04834360826598541

[2020-01-27 14:50:55.253388][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.23887277797858447

[2020-01-27 14:50:56.105261][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.42344137257754

[2020-01-27 14:50:56.105703][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:50:56.118107][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:50:59.611806][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.08308314 -0.         ... -0.11392629  0.06730814
  0.04661815]

[2020-01-27 14:50:59.612184][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:51:00.031597][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.88940277  0.         ...  0.13971555  0.01625478
 -0.15597033], shape=(4547,), dtype=float64)

[2020-01-27 14:51:00.126754][__main__.TRPOAgent.log][linesearch]: improvement: -0.6288481545556669

[2020-01-27 14:51:00.127210][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.01722023072986667

[2020-01-27 14:51:00.899526][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.34800369760853

[2020-01-27 14:51:00.899952][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:51:00.911951][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:51:04.482008][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.37066027 -0.         ...  0.2475732  -0.09117957
 -0.15639363]

[2020-01-27 14:51:04.482424][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:51:04.882387][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.94846873  0.         ... -0.67315851  0.50575048
  0.16740803], shape=(4547,), dtype=float64)

[2020-01-27 14:51:04.974232][__main__.TRPOAgent.log][linesearch]: improvement: -0.13037966429492942

[2020-01-27 14:51:05.008125][__main__.TRPOAgent.log][linesearch]: improvement: -0.07728238056578218

[2020-01-27 14:51:05.034247][__main__.TRPOAgent.log][linesearch]: improvement: -0.045971888826748186

[2020-01-27 14:51:05.069292][__main__.TRPOAgent.log][linesearch]: improvement: -0.027408602518802394

[2020-01-27 14:51:05.101972][__main__.TRPOAgent.log][linesearch]: improvement: -0.01639286594185918

[2020-01-27 14:51:05.135967][__main__.TRPOAgent.log][linesearch]: improvement: -0.009825420444889654

[2020-01-27 14:51:05.170391][__main__.TRPOAgent.log][linesearch]: improvement: -0.005893343562364928

[2020-01-27 14:51:05.205197][__main__.TRPOAgent.log][linesearch]: improvement: -0.0035364801326961404

[2020-01-27 14:51:05.240364][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021214155889477393

[2020-01-27 14:51:05.277844][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012724249522210185

[2020-01-27 14:51:05.278561][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.588154365301233e-07, Discarded policy loss value: -0.9343024869185942

[2020-01-27 14:51:06.147411][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.2290784775116

[2020-01-27 14:51:06.153464][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:51:09.731101][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10094512 -0.         ...  0.05392813  0.03765187
 -0.09158   ]

[2020-01-27 14:51:09.731504][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:51:10.143439][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.12023992  0.         ... -0.03894192 -0.0727684
  0.11171032], shape=(4547,), dtype=float64)

[2020-01-27 14:51:10.232038][__main__.TRPOAgent.log][linesearch]: improvement: -0.11276615849493044

[2020-01-27 14:51:10.263793][__main__.TRPOAgent.log][linesearch]: improvement: -0.07797678633981175

[2020-01-27 14:51:10.301165][__main__.TRPOAgent.log][linesearch]: improvement: -0.05060209312435249

[2020-01-27 14:51:10.339485][__main__.TRPOAgent.log][linesearch]: improvement: -0.031751022891350456

[2020-01-27 14:51:10.373088][__main__.TRPOAgent.log][linesearch]: improvement: -0.01946551098875826

[2020-01-27 14:51:10.400063][__main__.TRPOAgent.log][linesearch]: improvement: -0.011783638218099135

[2020-01-27 14:51:10.437573][__main__.TRPOAgent.log][linesearch]: improvement: -0.0070926944285585325

[2020-01-27 14:51:10.471672][__main__.TRPOAgent.log][linesearch]: improvement: -0.004263335128646517

[2020-01-27 14:51:10.507432][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025627468202173986

[2020-01-27 14:51:10.532139][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015402616827722149

[2020-01-27 14:51:10.532612][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.730457603026724e-07, Discarded policy loss value: -1.4894647446718783

[2020-01-27 14:51:11.326489][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.59176455160946

[2020-01-27 14:51:11.342050][__main__.TRPOAgent.log][learning]: Episode #16

[2020-01-27 14:51:11.342449][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:51:11.391844][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:51:11.392730][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:51:11.392492][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:51:11.393919][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:51:14.895353][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:51:14.949376][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:51:14.950207][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:51:14.950937][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:51:14.950841][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:51:14.952169][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:51:18.005522][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1306

[2020-01-27 14:51:18.198392][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:51:18.199144][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:51:18.200032][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:51:18.200119][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:51:18.202056][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:51:21.671058][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:51:21.742192][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:51:21.742779][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:51:21.743317][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:51:21.743451][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:51:21.745529][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:51:25.077166][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:51:25.157029][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:51:25.158212][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:51:25.158930][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:51:25.159026][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:51:25.162267][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:51:28.748402][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:51:28.813114][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:51:28.814097][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:51:28.814828][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:51:28.814910][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:51:28.818547][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:51:32.373275][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:51:32.410039][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:51:32.411509][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:51:32.413150][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:51:32.413233][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:51:32.415806][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:51:35.730314][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:51:35.831692][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:51:35.832546][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:51:35.833443][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:51:35.833559][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:51:35.840064][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:51:39.322426][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:51:39.327414][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:51:39.328014][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:51:39.328659][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:51:39.328727][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:51:39.332551][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:51:42.839779][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:51:42.872472][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:51:42.873313][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:51:42.874243][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:51:42.874383][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:51:42.878111][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:51:46.335641][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:51:46.378049][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:51:46.378577][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:51:46.379112][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:51:46.379179][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:51:46.381092][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:51:49.807421][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:51:49.838319][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:51:49.839003][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:51:49.839819][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:51:49.839967][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:51:49.846581][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:51:52.004565][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 912

[2020-01-27 14:51:52.590670][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:51:52.591249][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:51:52.592037][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:51:52.592107][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:51:52.594010][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:51:55.963428][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:51:56.080962][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:51:56.081510][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:51:56.082343][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:51:56.082412][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:51:56.084030][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:51:59.439706][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:51:59.506834][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:51:59.507384][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:51:59.508013][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:51:59.508081][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:51:59.509666][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:52:02.985971][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:52:03.028810][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:52:03.029377][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:52:03.040955][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:52:03.621840][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:52:03.652046][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:52:03.654037][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 44218, Batch size: 4500, Number of batches: 10

[2020-01-27 14:52:03.654530][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:52:03.691711][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:52:07.355813][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.39872648 -0.         ... -0.21557067  0.39044321
 -0.17487254]

[2020-01-27 14:52:07.356222][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:07.791996][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.6476144   0.         ... -0.31959391  0.18920804
  0.13038587], shape=(4547,), dtype=float64)

[2020-01-27 14:52:07.880325][__main__.TRPOAgent.log][linesearch]: improvement: -0.14043165081606884

[2020-01-27 14:52:07.880955][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.42162053595869214

[2020-01-27 14:52:08.709028][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.65032153868975

[2020-01-27 14:52:08.709417][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:52:08.716651][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:52:12.365428][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.14398646 -0.         ... -0.04975958  0.19012115
 -0.14036157]

[2020-01-27 14:52:12.365816][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:12.769470][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.07134501  0.         ... -0.22977289  0.11073913
  0.11903376], shape=(4547,), dtype=float64)

[2020-01-27 14:52:12.863476][__main__.TRPOAgent.log][linesearch]: improvement: -0.13619049672523248

[2020-01-27 14:52:12.902698][__main__.TRPOAgent.log][linesearch]: improvement: -0.08308213063134307

[2020-01-27 14:52:12.948955][__main__.TRPOAgent.log][linesearch]: improvement: -0.05005848626291953

[2020-01-27 14:52:12.984205][__main__.TRPOAgent.log][linesearch]: improvement: -0.030037046543534895

[2020-01-27 14:52:13.028574][__main__.TRPOAgent.log][linesearch]: improvement: -0.018007817018244388

[2020-01-27 14:52:13.066397][__main__.TRPOAgent.log][linesearch]: improvement: -0.010796540167439916

[2020-01-27 14:52:13.103379][__main__.TRPOAgent.log][linesearch]: improvement: -0.006474238776161045

[2020-01-27 14:52:13.142092][__main__.TRPOAgent.log][linesearch]: improvement: -0.0038830120838336812

[2020-01-27 14:52:13.174196][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023292195584441178

[2020-01-27 14:52:13.218654][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013973122905840407

[2020-01-27 14:52:13.219198][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.586601209414854e-07, Discarded policy loss value: -0.6012847658601262

[2020-01-27 14:52:14.092934][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.87635907029664

[2020-01-27 14:52:14.098752][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:52:17.733042][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.35528091 -0.         ... -0.00229188 -0.08449172
  0.0867836 ]

[2020-01-27 14:52:17.733426][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:18.136074][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.49229836  0.         ...  0.10967492  0.01694803
 -0.12662295], shape=(4547,), dtype=float64)

[2020-01-27 14:52:18.226406][__main__.TRPOAgent.log][linesearch]: improvement: -0.10569229102922506

[2020-01-27 14:52:18.261213][__main__.TRPOAgent.log][linesearch]: improvement: -0.06998426454429474

[2020-01-27 14:52:18.296166][__main__.TRPOAgent.log][linesearch]: improvement: -0.04401650066352891

[2020-01-27 14:52:18.330716][__main__.TRPOAgent.log][linesearch]: improvement: -0.026924006253906452

[2020-01-27 14:52:18.364041][__main__.TRPOAgent.log][linesearch]: improvement: -0.016299675376925116

[2020-01-27 14:52:18.398629][__main__.TRPOAgent.log][linesearch]: improvement: -0.00989247799835713

[2020-01-27 14:52:18.430666][__main__.TRPOAgent.log][linesearch]: improvement: -0.006000500593156488

[2020-01-27 14:52:18.466458][__main__.TRPOAgent.log][linesearch]: improvement: -0.0036372719435151524

[2020-01-27 14:52:18.503492][__main__.TRPOAgent.log][linesearch]: improvement: -0.0022090160475367604

[2020-01-27 14:52:18.537450][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013548222365207252

[2020-01-27 14:52:18.537958][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.42836284629571e-07, Discarded policy loss value: -2.4372404335516418

[2020-01-27 14:52:19.412408][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.2151817850947

[2020-01-27 14:52:19.419832][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:52:23.049820][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.23743739 -0.         ...  0.03025215  0.18326789
 -0.21352005]

[2020-01-27 14:52:23.050229][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:23.452827][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.62330382  0.         ... -0.47214924  0.41581464
  0.0563346 ], shape=(4547,), dtype=float64)

[2020-01-27 14:52:23.545964][__main__.TRPOAgent.log][linesearch]: improvement: -0.09478357777290189

[2020-01-27 14:52:23.546478][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.05664181666661059

[2020-01-27 14:52:24.310111][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.36090414492554

[2020-01-27 14:52:24.310487][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:52:24.317475][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:52:27.966919][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.32396678 -0.         ... -0.14540407  0.42340963
 -0.27800556]

[2020-01-27 14:52:27.967320][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:28.364060][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.7136018   0.         ... -0.219845    0.26399025
 -0.04414525], shape=(4547,), dtype=float64)

[2020-01-27 14:52:28.457731][__main__.TRPOAgent.log][linesearch]: improvement: -0.22281136830736914

[2020-01-27 14:52:28.492041][__main__.TRPOAgent.log][linesearch]: improvement: -0.1366796203023808

[2020-01-27 14:52:28.526171][__main__.TRPOAgent.log][linesearch]: improvement: -0.08320705814245732

[2020-01-27 14:52:28.552961][__main__.TRPOAgent.log][linesearch]: improvement: -0.05018789602816143

[2020-01-27 14:52:28.588052][__main__.TRPOAgent.log][linesearch]: improvement: -0.030226937412356714

[2020-01-27 14:52:28.624280][__main__.TRPOAgent.log][linesearch]: improvement: -0.018210811132291393

[2020-01-27 14:52:28.669193][__main__.TRPOAgent.log][linesearch]: improvement: -0.010944133531664457

[2020-01-27 14:52:28.703501][__main__.TRPOAgent.log][linesearch]: improvement: -0.006564452242514784

[2020-01-27 14:52:28.737488][__main__.TRPOAgent.log][linesearch]: improvement: -0.003939941108208611

[2020-01-27 14:52:28.767876][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023631506781267575

[2020-01-27 14:52:28.768642][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.412270593486491e-07, Discarded policy loss value: -1.8691812781305497

[2020-01-27 14:52:29.673856][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.0390679758974

[2020-01-27 14:52:29.680091][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:52:33.295865][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.47139078 -0.         ... -0.0799277   0.2785705
 -0.19864279]

[2020-01-27 14:52:33.296251][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:33.699671][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.44288914  0.         ... -0.34785345  0.55721853
 -0.20936508], shape=(4547,), dtype=float64)

[2020-01-27 14:52:33.792105][__main__.TRPOAgent.log][linesearch]: improvement: -0.14809207844981898

[2020-01-27 14:52:33.830387][__main__.TRPOAgent.log][linesearch]: improvement: -0.09516107895620185

[2020-01-27 14:52:33.858565][__main__.TRPOAgent.log][linesearch]: improvement: -0.05925818534569549

[2020-01-27 14:52:33.894197][__main__.TRPOAgent.log][linesearch]: improvement: -0.03634998318429794

[2020-01-27 14:52:33.926116][__main__.TRPOAgent.log][linesearch]: improvement: -0.022104491722980235

[2020-01-27 14:52:33.963514][__main__.TRPOAgent.log][linesearch]: improvement: -0.01336597605457901

[2020-01-27 14:52:33.994607][__main__.TRPOAgent.log][linesearch]: improvement: -0.008053121604790103

[2020-01-27 14:52:34.032046][__main__.TRPOAgent.log][linesearch]: improvement: -0.00484577500708383

[2020-01-27 14:52:34.069645][__main__.TRPOAgent.log][linesearch]: improvement: -0.0029138065288190074

[2020-01-27 14:52:34.101874][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017507026369638634

[2020-01-27 14:52:34.102588][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.634144105723261e-07, Discarded policy loss value: -1.6047522083481014

[2020-01-27 14:52:34.924050][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.1230814355678

[2020-01-27 14:52:34.929010][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:52:38.565807][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00095564 -0.         ... -0.27467662  0.39061078
 -0.11593416]

[2020-01-27 14:52:38.566189][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:38.964797][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.80282997  0.         ...  0.14518644 -0.01584816
 -0.12933827], shape=(4547,), dtype=float64)

[2020-01-27 14:52:39.054524][__main__.TRPOAgent.log][linesearch]: improvement: -0.17719054185514882

[2020-01-27 14:52:39.092020][__main__.TRPOAgent.log][linesearch]: improvement: -0.1092678205462978

[2020-01-27 14:52:39.126600][__main__.TRPOAgent.log][linesearch]: improvement: -0.06636212083095627

[2020-01-27 14:52:39.157946][__main__.TRPOAgent.log][linesearch]: improvement: -0.04003096854652799

[2020-01-27 14:52:39.190663][__main__.TRPOAgent.log][linesearch]: improvement: -0.024078997688543602

[2020-01-27 14:52:39.225758][__main__.TRPOAgent.log][linesearch]: improvement: -0.014465408909006494

[2020-01-27 14:52:39.252715][__main__.TRPOAgent.log][linesearch]: improvement: -0.008684460823384366

[2020-01-27 14:52:39.286255][__main__.TRPOAgent.log][linesearch]: improvement: -0.005212038851277301

[2020-01-27 14:52:39.321286][__main__.TRPOAgent.log][linesearch]: improvement: -0.003127933718727438

[2020-01-27 14:52:39.354175][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018770204070354124

[2020-01-27 14:52:39.354781][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.564027585518986e-07, Discarded policy loss value: -0.783811490824817

[2020-01-27 14:52:40.190661][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.464328381721

[2020-01-27 14:52:40.196370][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:52:43.844973][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.21838408 -0.         ...  0.15901438 -0.0201052
 -0.13890918]

[2020-01-27 14:52:43.845379][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:44.246638][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.00192502  0.         ...  0.18647053 -0.05549984
 -0.13097069], shape=(4547,), dtype=float64)

[2020-01-27 14:52:44.338056][__main__.TRPOAgent.log][linesearch]: improvement: -0.11302751857597135

[2020-01-27 14:52:44.374811][__main__.TRPOAgent.log][linesearch]: improvement: -0.07292069390923693

[2020-01-27 14:52:44.406720][__main__.TRPOAgent.log][linesearch]: improvement: -0.044650051779579175

[2020-01-27 14:52:44.439299][__main__.TRPOAgent.log][linesearch]: improvement: -0.026496841704433893

[2020-01-27 14:52:44.472091][__main__.TRPOAgent.log][linesearch]: improvement: -0.01616272791419679

[2020-01-27 14:52:44.504861][__main__.TRPOAgent.log][linesearch]: improvement: -0.010023897454355085

[2020-01-27 14:52:44.537038][__main__.TRPOAgent.log][linesearch]: improvement: -0.0059407153364808285

[2020-01-27 14:52:44.570635][__main__.TRPOAgent.log][linesearch]: improvement: -0.00347450464787874

[2020-01-27 14:52:44.602964][__main__.TRPOAgent.log][linesearch]: improvement: -0.002037647329980241

[2020-01-27 14:52:44.636254][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012032093632804164

[2020-01-27 14:52:44.636826][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.747856662688583e-07, Discarded policy loss value: -0.17492480540570512

[2020-01-27 14:52:45.456191][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.6758659623489

[2020-01-27 14:52:45.461260][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:52:48.986013][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.139922   -0.         ...  0.29826087 -0.23541265
 -0.06284822]

[2020-01-27 14:52:48.986416][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:49.396724][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.03155365  0.         ...  0.23798104 -0.42753035
  0.18954931], shape=(4547,), dtype=float64)

[2020-01-27 14:52:49.498258][__main__.TRPOAgent.log][linesearch]: improvement: -0.15269646542036966

[2020-01-27 14:52:49.530296][__main__.TRPOAgent.log][linesearch]: improvement: -0.09299229087075117

[2020-01-27 14:52:49.530931][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.19310283580393792

[2020-01-27 14:52:50.313893][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.3835878361828

[2020-01-27 14:52:50.314296][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:52:50.321031][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 3718

[2020-01-27 14:52:53.315935][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.21504092 -0.         ...  0.14267885 -0.17517194
  0.03249309]

[2020-01-27 14:52:53.316319][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:52:53.659400][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -5.75444337  0.         ... -0.49570803  0.56886818
 -0.07316016], shape=(4547,), dtype=float64)

[2020-01-27 14:52:53.741306][__main__.TRPOAgent.log][linesearch]: improvement: -2.407251370767667

[2020-01-27 14:52:53.774383][__main__.TRPOAgent.log][linesearch]: improvement: -0.552788684458851

[2020-01-27 14:52:53.806407][__main__.TRPOAgent.log][linesearch]: improvement: -0.18469188152602634

[2020-01-27 14:52:53.833070][__main__.TRPOAgent.log][linesearch]: improvement: -0.08286422186419973

[2020-01-27 14:52:53.833648][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 2.4746319827685577

[2020-01-27 14:52:54.482977][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 334.7030018633852

[2020-01-27 14:52:54.483389][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:52:54.513699][__main__.TRPOAgent.log][learning]: Episode #17

[2020-01-27 14:52:54.514191][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:52:54.566429][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:52:54.566974][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:52:54.567770][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:52:54.571257][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:52:58.297394][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:52:58.306799][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:52:58.307414][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:52:58.307964][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:52:58.308034][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:52:58.309539][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:53:01.828384][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:53:01.840213][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:53:01.840775][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:53:01.841375][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:53:01.841472][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:53:01.843460][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:53:05.207592][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:53:05.286315][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:53:05.286912][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:53:05.287468][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:53:05.287640][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:53:05.289940][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:53:08.752176][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:53:08.816881][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:53:08.818054][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:53:08.818621][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:53:08.818704][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:53:08.821950][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:53:12.193571][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:53:12.252278][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:53:12.252841][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:53:12.253452][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:53:12.253386][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:53:12.254622][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:53:15.608221][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:53:15.746057][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:53:15.746581][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:53:15.747092][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:53:15.747157][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:53:15.748871][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:53:19.185888][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:53:19.267656][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:53:19.268348][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:53:19.269007][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:53:19.269084][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:53:19.271207][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:53:22.749182][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:53:22.798723][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:53:22.799262][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:53:22.799875][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:53:22.799949][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:53:22.803786][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:53:26.165613][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:53:26.240027][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:53:26.240567][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:53:26.241147][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:53:26.241223][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:53:26.243367][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:53:29.670191][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:53:29.720096][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:53:29.720994][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:53:29.721582][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:53:29.721637][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:53:29.723626][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:53:33.221274][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:53:33.266437][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:53:33.267015][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:53:33.267743][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:53:33.267815][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:53:33.269870][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:53:36.709257][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:53:36.755192][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:53:36.755764][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:53:36.756773][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:53:36.756840][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:53:36.758841][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:53:40.267866][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:53:40.292801][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:53:40.293437][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:53:40.294023][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:53:40.294102][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:53:40.295998][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:53:43.778686][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:53:43.859018][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:53:43.859574][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:53:43.860235][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:53:43.860162][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:53:43.861103][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:53:47.228815][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:53:47.233682][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:53:47.235069][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:53:47.254308][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:53:47.889018][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:53:47.922086][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:53:47.923788][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:53:47.924350][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:53:47.959985][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:53:51.518496][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01516157 -0.         ... -0.15604545  0.08173208
  0.07431337]

[2020-01-27 14:53:51.518878][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:53:51.916485][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.39873443  0.         ... -0.39652153  0.28785838
  0.10866315], shape=(4547,), dtype=float64)

[2020-01-27 14:53:52.006215][__main__.TRPOAgent.log][linesearch]: improvement: -0.1460512157168533

[2020-01-27 14:53:52.045004][__main__.TRPOAgent.log][linesearch]: improvement: -0.08206411593219898

[2020-01-27 14:53:52.085723][__main__.TRPOAgent.log][linesearch]: improvement: -0.04635185221742699

[2020-01-27 14:53:52.114489][__main__.TRPOAgent.log][linesearch]: improvement: -0.02620561319818171

[2020-01-27 14:53:52.148588][__main__.TRPOAgent.log][linesearch]: improvement: -0.01562933305809966

[2020-01-27 14:53:52.180002][__main__.TRPOAgent.log][linesearch]: improvement: -0.00939708441977849

[2020-01-27 14:53:52.216413][__main__.TRPOAgent.log][linesearch]: improvement: -0.005638609011653917

[2020-01-27 14:53:52.257239][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033719324731767975

[2020-01-27 14:53:52.294180][__main__.TRPOAgent.log][linesearch]: improvement: -0.002025252379177944

[2020-01-27 14:53:52.326772][__main__.TRPOAgent.log][linesearch]: improvement: -0.001217547546694786

[2020-01-27 14:53:52.327414][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.184249573827062e-07, Discarded policy loss value: -0.695233175836526

[2020-01-27 14:53:53.267016][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.76360791370143

[2020-01-27 14:53:53.275825][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:53:56.821058][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.09634061 -0.         ... -0.09017452 -0.0814766
  0.17165113]

[2020-01-27 14:53:56.821477][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:53:57.215364][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.85665876  0.         ...  0.05205569 -0.27873082
  0.22667513], shape=(4547,), dtype=float64)

[2020-01-27 14:53:57.302631][__main__.TRPOAgent.log][linesearch]: improvement: -0.12455686785215181

[2020-01-27 14:53:57.303190][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.1112551264754022

[2020-01-27 14:53:58.077077][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.0481710297221

[2020-01-27 14:53:58.077480][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:53:58.085110][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:54:01.714651][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.16189873 -0.         ... -0.06043894 -0.12328931
  0.18372825]

[2020-01-27 14:54:01.715058][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:02.111523][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.0624508   0.         ... -0.15938436  0.17463556
 -0.0152512 ], shape=(4547,), dtype=float64)

[2020-01-27 14:54:02.195407][__main__.TRPOAgent.log][linesearch]: improvement: -0.08405197519761554

[2020-01-27 14:54:02.196022][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.5792873305446097

[2020-01-27 14:54:02.955636][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.5723966975303

[2020-01-27 14:54:02.956050][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:54:02.963558][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:54:06.509890][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.36939995 -0.         ... -0.31750154  0.11271158
  0.20478996]

[2020-01-27 14:54:06.510286][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:06.900806][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.33019786  0.         ...  0.59140433 -0.53753789
 -0.05386643], shape=(4547,), dtype=float64)

[2020-01-27 14:54:07.004103][__main__.TRPOAgent.log][linesearch]: improvement: -0.2565726210809981

[2020-01-27 14:54:07.048553][__main__.TRPOAgent.log][linesearch]: improvement: -0.15769093833609615

[2020-01-27 14:54:07.049124][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.5123213513406099

[2020-01-27 14:54:07.865907][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.39831663201704

[2020-01-27 14:54:07.866311][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:54:07.874006][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:54:11.427495][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.51207052 -0.         ... -0.21986009 -0.11935203
  0.33921212]

[2020-01-27 14:54:11.427913][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:11.816968][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.9786964   0.         ...  0.25067307  0.10565747
 -0.35633054], shape=(4547,), dtype=float64)

[2020-01-27 14:54:11.914452][__main__.TRPOAgent.log][linesearch]: improvement: -0.48091956005543135

[2020-01-27 14:54:11.949305][__main__.TRPOAgent.log][linesearch]: improvement: -0.19089424494410556

[2020-01-27 14:54:11.979994][__main__.TRPOAgent.log][linesearch]: improvement: -0.085567555985886

[2020-01-27 14:54:12.011134][__main__.TRPOAgent.log][linesearch]: improvement: -0.04377077786004335

[2020-01-27 14:54:12.044587][__main__.TRPOAgent.log][linesearch]: improvement: -0.02432249765843697

[2020-01-27 14:54:12.077410][__main__.TRPOAgent.log][linesearch]: improvement: -0.014504804506289548

[2020-01-27 14:54:12.108581][__main__.TRPOAgent.log][linesearch]: improvement: -0.009129416751899821

[2020-01-27 14:54:12.138503][__main__.TRPOAgent.log][linesearch]: improvement: -0.006049067233017291

[2020-01-27 14:54:12.169597][__main__.TRPOAgent.log][linesearch]: improvement: -0.0038344770856965638

[2020-01-27 14:54:12.197287][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023852896501466714

[2020-01-27 14:54:12.198000][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.172737075934412e-07, Discarded policy loss value: -0.3061365870614857

[2020-01-27 14:54:12.998671][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.12804109205587

[2020-01-27 14:54:13.004069][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:54:16.534493][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.40415561 -0.         ...  0.00874296 -0.4019635
  0.39322054]

[2020-01-27 14:54:16.534873][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:16.934184][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.54346333  0.         ...  0.17130394 -0.22285814
  0.05155421], shape=(4547,), dtype=float64)

[2020-01-27 14:54:17.026921][__main__.TRPOAgent.log][linesearch]: improvement: -0.1865051220259577

[2020-01-27 14:54:17.065426][__main__.TRPOAgent.log][linesearch]: improvement: -0.12204888233993527

[2020-01-27 14:54:17.098496][__main__.TRPOAgent.log][linesearch]: improvement: -0.06411691083937465

[2020-01-27 14:54:17.129854][__main__.TRPOAgent.log][linesearch]: improvement: -0.02828835701204757

[2020-01-27 14:54:17.161096][__main__.TRPOAgent.log][linesearch]: improvement: -0.012354198753747214

[2020-01-27 14:54:17.193833][__main__.TRPOAgent.log][linesearch]: improvement: -0.00679588797069175

[2020-01-27 14:54:17.226707][__main__.TRPOAgent.log][linesearch]: improvement: -0.00492059982924542

[2020-01-27 14:54:17.263553][__main__.TRPOAgent.log][linesearch]: improvement: -0.0039275594922578705

[2020-01-27 14:54:17.300407][__main__.TRPOAgent.log][linesearch]: improvement: -0.003226103795958668

[2020-01-27 14:54:17.333147][__main__.TRPOAgent.log][linesearch]: improvement: -0.002097172271695591

[2020-01-27 14:54:17.333783][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.06171608384856e-07, Discarded policy loss value: -1.6127330001313793

[2020-01-27 14:54:18.097967][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.41951834793514

[2020-01-27 14:54:18.103203][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:54:21.720046][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.06362966 -0.         ... -0.02248736 -0.15069359
  0.17318096]

[2020-01-27 14:54:21.720457][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:22.121835][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          3.03041433  0.         ... -1.4959863   1.35289638
  0.14308992], shape=(4547,), dtype=float64)

[2020-01-27 14:54:22.210825][__main__.TRPOAgent.log][linesearch]: improvement: -0.30081096454277256

[2020-01-27 14:54:22.245098][__main__.TRPOAgent.log][linesearch]: improvement: -0.16302304081900187

[2020-01-27 14:54:22.245613][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.5602893096356454

[2020-01-27 14:54:23.017486][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.2430243950753

[2020-01-27 14:54:23.017880][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:54:23.025413][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:54:26.617228][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.14619011 -0.         ... -0.16790826  0.02703564
  0.14087262]

[2020-01-27 14:54:26.617603][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:27.011935][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.05996712  0.         ...  0.10571561 -0.19455308
  0.08883747], shape=(4547,), dtype=float64)

[2020-01-27 14:54:27.094845][__main__.TRPOAgent.log][linesearch]: improvement: -0.12323982383808105

[2020-01-27 14:54:27.095307][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.2916703639680927

[2020-01-27 14:54:27.847857][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.41603548708844

[2020-01-27 14:54:27.848255][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:54:27.855743][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:54:31.437792][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.3128523  -0.         ... -0.21896538  0.04986955
  0.16909583]

[2020-01-27 14:54:31.438198][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:31.861991][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          2.02721248  0.         ...  0.02321038 -0.26547331
  0.24226293], shape=(4547,), dtype=float64)

[2020-01-27 14:54:31.945857][__main__.TRPOAgent.log][linesearch]: improvement: -0.3346975022048285

[2020-01-27 14:54:31.978663][__main__.TRPOAgent.log][linesearch]: improvement: -0.17476473723198355

[2020-01-27 14:54:32.009097][__main__.TRPOAgent.log][linesearch]: improvement: -0.0688263010635502

[2020-01-27 14:54:32.039941][__main__.TRPOAgent.log][linesearch]: improvement: -0.028355672419794198

[2020-01-27 14:54:32.071167][__main__.TRPOAgent.log][linesearch]: improvement: -0.016980633743468088

[2020-01-27 14:54:32.105724][__main__.TRPOAgent.log][linesearch]: improvement: -0.011121912322555882

[2020-01-27 14:54:32.138504][__main__.TRPOAgent.log][linesearch]: improvement: -0.00757087365457948

[2020-01-27 14:54:32.172117][__main__.TRPOAgent.log][linesearch]: improvement: -0.004943171603012775

[2020-01-27 14:54:32.201814][__main__.TRPOAgent.log][linesearch]: improvement: -0.0031961229286137094

[2020-01-27 14:54:32.237630][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019985321603963047

[2020-01-27 14:54:32.238209][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.733218278252227e-07, Discarded policy loss value: -0.3733322159673769

[2020-01-27 14:54:33.052498][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.56341617310966

[2020-01-27 14:54:33.057592][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:54:36.618527][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.56879105 -0.         ... -0.1815795  -0.18000408
  0.36158358]

[2020-01-27 14:54:36.618922][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:54:37.025852][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.55945399  0.         ...  0.19326747 -0.20248075
  0.00921328], shape=(4547,), dtype=float64)

[2020-01-27 14:54:37.119707][__main__.TRPOAgent.log][linesearch]: improvement: -0.21729321854923733

[2020-01-27 14:54:37.158947][__main__.TRPOAgent.log][linesearch]: improvement: -0.1256518527091155

[2020-01-27 14:54:37.159456][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.987985849845578

[2020-01-27 14:54:37.990627][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.58629662405804

[2020-01-27 14:54:37.991030][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:54:38.010627][__main__.TRPOAgent.log][learning]: Episode #18

[2020-01-27 14:54:38.011089][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:54:38.065445][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:54:38.066868][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:54:38.066010][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:54:38.067918][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:54:39.966523][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 783

[2020-01-27 14:54:40.688427][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:54:40.688997][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:54:40.689814][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:54:40.689880][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:54:40.691682][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:54:42.272160][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 711

[2020-01-27 14:54:43.157686][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:54:43.158382][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:54:43.159029][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:54:43.159095][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:54:43.162677][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:54:44.451102][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 514

[2020-01-27 14:54:45.421539][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:54:45.422684][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:54:45.423467][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:54:45.423736][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:54:45.427841][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:54:48.880795][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:54:48.913217][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:54:48.914136][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:54:48.915024][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:54:48.915118][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:54:48.917558][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:54:52.082282][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1336

[2020-01-27 14:54:52.227085][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:54:52.227600][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:54:52.228057][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:54:52.228111][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:54:52.229868][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:54:55.695187][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:54:55.772435][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:54:55.773724][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:54:55.774470][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:54:55.774328][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:54:55.775474][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:54:59.195457][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:54:59.204714][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:54:59.206089][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:54:59.206944][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:54:59.206880][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:54:59.208124][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:55:02.574798][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:55:02.639385][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:55:02.639957][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:55:02.640579][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:55:02.640651][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:55:02.642589][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:55:05.970031][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:55:06.078906][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:55:06.079745][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:55:06.080263][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:55:06.080319][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:55:06.083111][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:55:09.541237][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:55:09.618207][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:55:09.618989][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:55:09.619558][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:55:09.619620][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:55:09.621424][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:55:12.981462][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:55:13.035724][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:55:13.037232][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:55:13.038665][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:55:13.038463][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:55:13.040152][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:55:16.494960][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:55:16.510281][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:55:16.511161][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:55:16.511876][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:55:16.511953][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:55:16.514350][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:55:19.442822][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1258

[2020-01-27 14:55:19.730537][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:55:19.731159][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:55:19.731834][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:55:19.731771][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:55:19.732849][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:55:23.017650][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1442

[2020-01-27 14:55:23.047231][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:55:23.048029][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:55:23.048650][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:55:23.048717][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:55:23.051314][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:55:26.496079][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:55:26.502724][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:55:26.503377][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:55:26.518343][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:55:27.048828][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:55:27.082976][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:55:27.084670][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 42044, Batch size: 4500, Number of batches: 10

[2020-01-27 14:55:27.085238][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:55:27.118012][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:55:30.601886][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.12755528 -0.         ... -0.05258979  0.08903219
 -0.0364424 ]

[2020-01-27 14:55:30.602277][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:55:30.999307][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.366843    0.         ...  0.02721542  0.07314305
 -0.10035847], shape=(4547,), dtype=float64)

[2020-01-27 14:55:31.106208][__main__.TRPOAgent.log][linesearch]: improvement: -0.16517556641092435

[2020-01-27 14:55:31.139213][__main__.TRPOAgent.log][linesearch]: improvement: -0.09003596290157576

[2020-01-27 14:55:31.139962][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.2254737246788621

[2020-01-27 14:55:31.922545][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.26161668535053

[2020-01-27 14:55:31.922954][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:55:31.930871][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:55:35.472822][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.25532832 -0.         ... -0.05848148  0.3336826
 -0.27520112]

[2020-01-27 14:55:35.473215][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:55:35.869127][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.04706566  0.         ...  0.04805361  0.12451303
 -0.17256664], shape=(4547,), dtype=float64)

[2020-01-27 14:55:35.955626][__main__.TRPOAgent.log][linesearch]: improvement: -0.09059973003849442

[2020-01-27 14:55:35.991533][__main__.TRPOAgent.log][linesearch]: improvement: -0.06562406761248951

[2020-01-27 14:55:36.018811][__main__.TRPOAgent.log][linesearch]: improvement: -0.04378881869025486

[2020-01-27 14:55:36.055272][__main__.TRPOAgent.log][linesearch]: improvement: -0.027691220496617763

[2020-01-27 14:55:36.088015][__main__.TRPOAgent.log][linesearch]: improvement: -0.017179707153698986

[2020-01-27 14:55:36.124701][__main__.TRPOAgent.log][linesearch]: improvement: -0.010555172480016584

[2020-01-27 14:55:36.149959][__main__.TRPOAgent.log][linesearch]: improvement: -0.006590692457655978

[2020-01-27 14:55:36.187668][__main__.TRPOAgent.log][linesearch]: improvement: -0.004143719654284395

[2020-01-27 14:55:36.222581][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025364935686742474

[2020-01-27 14:55:36.256294][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015208198415541663

[2020-01-27 14:55:36.256947][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.190425962717923e-07, Discarded policy loss value: -1.1487203461035578

[2020-01-27 14:55:37.128359][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.44457431316397

[2020-01-27 14:55:37.134467][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:55:40.650788][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.1999143  -0.         ... -0.24990343  0.24574378
  0.00415965]

[2020-01-27 14:55:40.651178][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:55:41.042933][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.36550081  0.         ... -0.29449766  0.28787962
  0.00661804], shape=(4547,), dtype=float64)

[2020-01-27 14:55:41.133192][__main__.TRPOAgent.log][linesearch]: improvement: -0.13505578365256954

[2020-01-27 14:55:41.168294][__main__.TRPOAgent.log][linesearch]: improvement: -0.08330838911875194

[2020-01-27 14:55:41.204193][__main__.TRPOAgent.log][linesearch]: improvement: -0.05345700547674803

[2020-01-27 14:55:41.235954][__main__.TRPOAgent.log][linesearch]: improvement: -0.0328123875042991

[2020-01-27 14:55:41.273002][__main__.TRPOAgent.log][linesearch]: improvement: -0.019583219436437316

[2020-01-27 14:55:41.309964][__main__.TRPOAgent.log][linesearch]: improvement: -0.011125373550685269

[2020-01-27 14:55:41.336099][__main__.TRPOAgent.log][linesearch]: improvement: -0.006570692285044699

[2020-01-27 14:55:41.371936][__main__.TRPOAgent.log][linesearch]: improvement: -0.004012730777091855

[2020-01-27 14:55:41.396005][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024442012887538334

[2020-01-27 14:55:41.420165][__main__.TRPOAgent.log][linesearch]: improvement: -0.001477513396263952

[2020-01-27 14:55:41.420658][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.602616039128886e-07, Discarded policy loss value: -0.3891671681122818

[2020-01-27 14:55:42.139386][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.4389553969172

[2020-01-27 14:55:42.144476][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:55:45.704806][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.18588104 -0.         ...  0.31552216 -0.3294683
  0.01394614]

[2020-01-27 14:55:45.705202][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:55:46.106489][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.15704872  0.         ...  0.25041575 -0.25855783
  0.00814207], shape=(4547,), dtype=float64)

[2020-01-27 14:55:46.195743][__main__.TRPOAgent.log][linesearch]: improvement: -0.2539893266477338

[2020-01-27 14:55:46.231368][__main__.TRPOAgent.log][linesearch]: improvement: -0.14129629255555604

[2020-01-27 14:55:46.260682][__main__.TRPOAgent.log][linesearch]: improvement: -0.08040508919276596

[2020-01-27 14:55:46.295789][__main__.TRPOAgent.log][linesearch]: improvement: -0.045337302171269744

[2020-01-27 14:55:46.327496][__main__.TRPOAgent.log][linesearch]: improvement: -0.02788695066054131

[2020-01-27 14:55:46.359423][__main__.TRPOAgent.log][linesearch]: improvement: -0.017040023532500226

[2020-01-27 14:55:46.390042][__main__.TRPOAgent.log][linesearch]: improvement: -0.010278353721758116

[2020-01-27 14:55:46.418285][__main__.TRPOAgent.log][linesearch]: improvement: -0.006301181344699425

[2020-01-27 14:55:46.452581][__main__.TRPOAgent.log][linesearch]: improvement: -0.0038190449273157734

[2020-01-27 14:55:46.488133][__main__.TRPOAgent.log][linesearch]: improvement: -0.002296143439013465

[2020-01-27 14:55:46.488604][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.622498514075566e-07, Discarded policy loss value: -0.4844364892351706

[2020-01-27 14:55:47.255314][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.78378621616844

[2020-01-27 14:55:47.260593][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:55:50.812831][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.32078209 -0.         ...  0.11772154  0.08348385
 -0.20120538]

[2020-01-27 14:55:50.813512][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:55:51.206284][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.62422155  0.         ... -0.11355746 -0.09273161
  0.20628907], shape=(4547,), dtype=float64)

[2020-01-27 14:55:51.298363][__main__.TRPOAgent.log][linesearch]: improvement: -0.06604465286079297

[2020-01-27 14:55:51.335507][__main__.TRPOAgent.log][linesearch]: improvement: -0.05825062953391738

[2020-01-27 14:55:51.369688][__main__.TRPOAgent.log][linesearch]: improvement: -0.045604261159920306

[2020-01-27 14:55:51.402325][__main__.TRPOAgent.log][linesearch]: improvement: -0.03243167931548732

[2020-01-27 14:55:51.435979][__main__.TRPOAgent.log][linesearch]: improvement: -0.021100173981860093

[2020-01-27 14:55:51.473676][__main__.TRPOAgent.log][linesearch]: improvement: -0.013232352003683334

[2020-01-27 14:55:51.501061][__main__.TRPOAgent.log][linesearch]: improvement: -0.008081112517147826

[2020-01-27 14:55:51.536468][__main__.TRPOAgent.log][linesearch]: improvement: -0.004787780689283372

[2020-01-27 14:55:51.572791][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028477973875781215

[2020-01-27 14:55:51.596694][__main__.TRPOAgent.log][linesearch]: improvement: -0.001709774294705646

[2020-01-27 14:55:51.597212][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.266911241831722e-07, Discarded policy loss value: -0.27922576895448137

[2020-01-27 14:55:52.458893][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.63005434667633

[2020-01-27 14:55:52.465034][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:55:56.046261][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.06351547 -0.         ... -0.05189097  0.02924052
  0.02265045]

[2020-01-27 14:55:56.046642][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:55:56.438372][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.75638104  0.         ...  0.02372669 -0.12611779
  0.10239109], shape=(4547,), dtype=float64)

[2020-01-27 14:55:56.530142][__main__.TRPOAgent.log][linesearch]: improvement: -0.11620998115112868

[2020-01-27 14:55:56.563772][__main__.TRPOAgent.log][linesearch]: improvement: -0.09009486445161424

[2020-01-27 14:55:56.596919][__main__.TRPOAgent.log][linesearch]: improvement: -0.0630260679013514

[2020-01-27 14:55:56.597482][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.29787808843067415

[2020-01-27 14:55:57.356327][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.1145203095644

[2020-01-27 14:55:57.356736][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:55:57.364439][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:56:00.875208][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.30407672 -0.         ...  0.12658051 -0.01250876
 -0.11407175]

[2020-01-27 14:56:00.875589][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:56:01.253791][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.88289121  0.         ... -0.10658526  0.00114137
  0.1054439 ], shape=(4547,), dtype=float64)

[2020-01-27 14:56:01.323895][__main__.TRPOAgent.log][linesearch]: improvement: -0.10282300878370287

[2020-01-27 14:56:01.345181][__main__.TRPOAgent.log][linesearch]: improvement: -0.06742886953021532

[2020-01-27 14:56:01.345614][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.6407069752338498

[2020-01-27 14:56:02.100155][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9618533131118

[2020-01-27 14:56:02.100564][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:56:02.108033][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:56:05.577527][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.29625149 -0.         ... -0.05596459  0.22088814
 -0.16492355]

[2020-01-27 14:56:05.577906][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:56:05.973307][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.24278395  0.         ... -0.44556758  0.45184992
 -0.00628233], shape=(4547,), dtype=float64)

[2020-01-27 14:56:06.055353][__main__.TRPOAgent.log][linesearch]: improvement: -0.19067545189043608

[2020-01-27 14:56:06.087735][__main__.TRPOAgent.log][linesearch]: improvement: -0.1073201035956956

[2020-01-27 14:56:06.088308][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.18372271192591952

[2020-01-27 14:56:06.823845][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.31427262958397

[2020-01-27 14:56:06.824254][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:56:06.831466][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:56:10.423892][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.43683382 -0.         ... -0.27599622  0.36648474
 -0.09048852]

[2020-01-27 14:56:10.424278][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:56:10.815763][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.49030747  0.         ...  0.12514265  0.04422907
 -0.16937172], shape=(4547,), dtype=float64)

[2020-01-27 14:56:10.906441][__main__.TRPOAgent.log][linesearch]: improvement: -0.18857217183374964

[2020-01-27 14:56:10.906939][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 3.7561981267906086

[2020-01-27 14:56:11.676792][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 379.18291645894453

[2020-01-27 14:56:11.677214][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:56:11.681878][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 1544

[2020-01-27 14:56:12.903568][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00000000e+00 -1.41345476e-05 -0.00000000e+00 ... -2.33561254e-01
  2.37849420e-01 -4.28816594e-03]

[2020-01-27 14:56:12.903926][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:56:13.155206][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          2.26464607  0.         ... -0.31513401  0.15091891
  0.1642151 ], shape=(4547,), dtype=float64)

[2020-01-27 14:56:13.219148][__main__.TRPOAgent.log][linesearch]: improvement: -0.26692546769489445

[2020-01-27 14:56:13.240731][__main__.TRPOAgent.log][linesearch]: improvement: -0.172792248914728

[2020-01-27 14:56:13.273483][__main__.TRPOAgent.log][linesearch]: improvement: -0.10834095439169422

[2020-01-27 14:56:13.300440][__main__.TRPOAgent.log][linesearch]: improvement: -0.06459642762634532

[2020-01-27 14:56:13.324585][__main__.TRPOAgent.log][linesearch]: improvement: -0.03804292557549949

[2020-01-27 14:56:13.350060][__main__.TRPOAgent.log][linesearch]: improvement: -0.023188452678976068

[2020-01-27 14:56:13.374011][__main__.TRPOAgent.log][linesearch]: improvement: -0.01429601963150895

[2020-01-27 14:56:13.401460][__main__.TRPOAgent.log][linesearch]: improvement: -0.008665091741679065

[2020-01-27 14:56:13.426022][__main__.TRPOAgent.log][linesearch]: improvement: -0.005201395754313554

[2020-01-27 14:56:13.455612][__main__.TRPOAgent.log][linesearch]: improvement: -0.0031626615962494498

[2020-01-27 14:56:13.456093][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.9865995846266e-07, Discarded policy loss value: -1.3561059064045689

[2020-01-27 14:56:13.758847][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 430.7869404569866

[2020-01-27 14:56:13.776229][__main__.TRPOAgent.log][learning]: Episode #19

[2020-01-27 14:56:13.776765][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:56:13.829291][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:56:13.830190][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:56:13.829840][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:56:13.831388][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:56:17.372991][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:56:17.380529][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:56:17.381454][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:56:17.383156][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:56:17.383095][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:56:17.384404][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:56:20.839489][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:56:20.840363][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:56:20.841095][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:56:20.841834][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:56:20.841923][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:56:20.843512][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:56:24.219837][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:56:24.234703][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:56:24.235358][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:56:24.236137][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:56:24.236209][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:56:24.240179][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:56:26.977913][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1182

[2020-01-27 14:56:27.284867][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:56:27.285538][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:56:27.286391][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:56:27.286269][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:56:27.287637][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:56:30.647698][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:56:30.692357][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:56:30.692937][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:56:30.693452][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:56:30.693550][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:56:30.695544][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:56:34.067343][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:56:34.071523][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:56:34.072129][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:56:34.072656][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:56:34.072718][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:56:34.074765][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:56:37.485504][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:56:37.523466][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:56:37.524370][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:56:37.525325][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:56:37.525639][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:56:37.528818][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:56:40.916958][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:56:40.929499][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:56:40.930966][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:56:40.931830][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:56:40.932755][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:56:40.939497][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:56:44.253115][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:56:44.308802][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:56:44.309622][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:56:44.310286][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:56:44.310356][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:56:44.312408][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:56:47.633445][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:56:47.673893][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:56:47.674485][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:56:47.675147][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:56:47.675278][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:56:47.677322][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:56:49.815597][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 918

[2020-01-27 14:56:50.419003][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:56:50.419582][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:56:50.420550][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:56:50.420437][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:56:50.424500][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:56:53.422048][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1371

[2020-01-27 14:56:53.602679][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:56:53.603342][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:56:53.604171][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:56:53.604232][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:56:53.606163][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:56:56.983131][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:56:57.078089][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:56:57.078678][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:56:57.079244][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:56:57.079179][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:56:57.080232][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:57:00.434560][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:57:00.483602][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:57:00.484186][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:57:00.484851][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:57:00.484784][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:57:00.485846][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:57:03.769479][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:57:03.849517][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:57:03.850009][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:57:03.861335][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:57:04.460859][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:57:04.488815][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:57:04.490157][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 43971, Batch size: 4500, Number of batches: 10

[2020-01-27 14:57:04.490516][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:57:04.521473][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:57:08.051114][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.04442084 -0.         ...  0.09055595 -0.03809179
 -0.05246415]

[2020-01-27 14:57:08.051505][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:08.451715][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.00947419  0.         ... -0.13553587  0.16089603
 -0.02536016], shape=(4547,), dtype=float64)

[2020-01-27 14:57:08.538998][__main__.TRPOAgent.log][linesearch]: improvement: -0.03135534195029255

[2020-01-27 14:57:08.579292][__main__.TRPOAgent.log][linesearch]: improvement: -0.015767904810988664

[2020-01-27 14:57:08.606582][__main__.TRPOAgent.log][linesearch]: improvement: -0.012284789970260945

[2020-01-27 14:57:08.643916][__main__.TRPOAgent.log][linesearch]: improvement: -0.010386322043924423

[2020-01-27 14:57:08.678637][__main__.TRPOAgent.log][linesearch]: improvement: -0.004365123117525016

[2020-01-27 14:57:08.704959][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030012361277926836

[2020-01-27 14:57:08.739554][__main__.TRPOAgent.log][linesearch]: improvement: -0.002757183521823414

[2020-01-27 14:57:08.774365][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021493671033363526

[2020-01-27 14:57:08.809714][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014635303423413681

[2020-01-27 14:57:08.847756][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009742502256400165

[2020-01-27 14:57:08.848292][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.86612744022534e-07, Discarded policy loss value: -2.1314384686947014

[2020-01-27 14:57:09.670747][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9805593786087

[2020-01-27 14:57:09.676442][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:57:13.206860][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.17993276 -0.         ...  0.04080921  0.06608851
 -0.10689772]

[2020-01-27 14:57:13.207309][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:13.657247][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.08583915  0.         ... -0.12136788  0.08125508
  0.0401128 ], shape=(4547,), dtype=float64)

[2020-01-27 14:57:13.759420][__main__.TRPOAgent.log][linesearch]: improvement: -0.05061585436045041

[2020-01-27 14:57:13.797616][__main__.TRPOAgent.log][linesearch]: improvement: -0.0481125137544839

[2020-01-27 14:57:13.824499][__main__.TRPOAgent.log][linesearch]: improvement: -0.03686734301932504

[2020-01-27 14:57:13.861725][__main__.TRPOAgent.log][linesearch]: improvement: -0.02492352556513934

[2020-01-27 14:57:13.895573][__main__.TRPOAgent.log][linesearch]: improvement: -0.01527940952682548

[2020-01-27 14:57:13.923822][__main__.TRPOAgent.log][linesearch]: improvement: -0.009049743871905075

[2020-01-27 14:57:13.961134][__main__.TRPOAgent.log][linesearch]: improvement: -0.005574434547104629

[2020-01-27 14:57:13.998559][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033744640695189954

[2020-01-27 14:57:14.024136][__main__.TRPOAgent.log][linesearch]: improvement: -0.002042203317315694

[2020-01-27 14:57:14.061057][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012301006321469854

[2020-01-27 14:57:14.061550][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.366324615526599e-07, Discarded policy loss value: -0.13627635403365013

[2020-01-27 14:57:14.884373][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.99347378936795

[2020-01-27 14:57:14.889835][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:57:18.394421][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.06872388 -0.         ... -0.07003347  0.1026862
 -0.03265273]

[2020-01-27 14:57:18.394794][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:18.800128][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.12570688  0.         ...  0.14289813 -0.16432047
  0.02142234], shape=(4547,), dtype=float64)

[2020-01-27 14:57:18.889182][__main__.TRPOAgent.log][linesearch]: improvement: -0.13489135893072812

[2020-01-27 14:57:18.931429][__main__.TRPOAgent.log][linesearch]: improvement: -0.08497798631139913

[2020-01-27 14:57:18.931961][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.12801351239418404

[2020-01-27 14:57:19.738931][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9640451746751

[2020-01-27 14:57:19.739338][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:19.746972][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:57:23.222307][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.29170275 -0.         ... -0.12720361 -0.07528932
  0.20249293]

[2020-01-27 14:57:23.222759][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:23.621534][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.0506204   0.         ...  0.26982131 -0.13868761
 -0.1311337 ], shape=(4547,), dtype=float64)

[2020-01-27 14:57:23.707712][__main__.TRPOAgent.log][linesearch]: improvement: -0.12483622574411635

[2020-01-27 14:57:23.737879][__main__.TRPOAgent.log][linesearch]: improvement: -0.07576000317316407

[2020-01-27 14:57:23.738363][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.07079904316498828

[2020-01-27 14:57:24.508863][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.00669534843956

[2020-01-27 14:57:24.509311][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:24.516914][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:57:28.117451][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.11973524 -0.         ...  0.11481815 -0.01182673
 -0.10299142]

[2020-01-27 14:57:28.117839][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:28.509615][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.23888268  0.         ... -0.29065887  0.23195361
  0.05870526], shape=(4547,), dtype=float64)

[2020-01-27 14:57:28.591855][__main__.TRPOAgent.log][linesearch]: improvement: -0.03157602455215859

[2020-01-27 14:57:28.627915][__main__.TRPOAgent.log][linesearch]: improvement: -0.008557461270893685

[2020-01-27 14:57:28.629043][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.09440883664350547

[2020-01-27 14:57:29.433518][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.89968842323117

[2020-01-27 14:57:29.433964][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:29.446144][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:57:33.067100][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02076271 -0.         ...  0.02505994 -0.02744772
  0.00238778]

[2020-01-27 14:57:33.067477][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:33.463780][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.26858925  0.         ... -0.00870776 -0.03507548
  0.04378323], shape=(4547,), dtype=float64)

[2020-01-27 14:57:33.555324][__main__.TRPOAgent.log][linesearch]: improvement: -0.03659872611220283

[2020-01-27 14:57:33.591818][__main__.TRPOAgent.log][linesearch]: improvement: -0.037014978832177026

[2020-01-27 14:57:33.592347][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.4466627771086465

[2020-01-27 14:57:34.356092][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9334261211524

[2020-01-27 14:57:34.356502][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:34.364102][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:57:37.973523][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.14569639 -0.         ...  0.02264836  0.14428302
 -0.16693138]

[2020-01-27 14:57:37.973910][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:38.364960][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.63036885  0.         ... -0.25087357  0.25006728
  0.00080629], shape=(4547,), dtype=float64)

[2020-01-27 14:57:38.448826][__main__.TRPOAgent.log][linesearch]: improvement: -0.09767991881932458

[2020-01-27 14:57:38.449457][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.41188839410162287

[2020-01-27 14:57:39.209175][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0300618330224

[2020-01-27 14:57:39.209585][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:39.217115][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:57:42.789258][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.10953181 -0.         ... -0.01375404  0.05872806
 -0.04497402]

[2020-01-27 14:57:42.789640][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:43.200309][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.61564485  0.         ...  0.31272365 -0.57959615
  0.2668725 ], shape=(4547,), dtype=float64)

[2020-01-27 14:57:43.300228][__main__.TRPOAgent.log][linesearch]: improvement: -0.17896100248346153

[2020-01-27 14:57:43.339343][__main__.TRPOAgent.log][linesearch]: improvement: -0.12008543243290215

[2020-01-27 14:57:43.339882][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.27222254470109875

[2020-01-27 14:57:44.107057][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.6407355924891

[2020-01-27 14:57:44.107477][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:44.119041][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:57:47.644749][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.08900705 -0.         ...  0.00284873  0.09769998
 -0.10054871]

[2020-01-27 14:57:47.645155][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:48.040688][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.01193156  0.         ... -0.33334533  0.38110428
 -0.04775895], shape=(4547,), dtype=float64)

[2020-01-27 14:57:48.129253][__main__.TRPOAgent.log][linesearch]: improvement: -0.09333298732688793

[2020-01-27 14:57:48.164035][__main__.TRPOAgent.log][linesearch]: improvement: -0.0692290956111788

[2020-01-27 14:57:48.164640][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.1411926978740238

[2020-01-27 14:57:48.904176][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.99609937345883

[2020-01-27 14:57:48.904589][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:48.911320][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 3471

[2020-01-27 14:57:51.716934][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.26520394 -0.         ...  0.06320366  0.07734831
 -0.14055197]

[2020-01-27 14:57:51.717369][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:57:52.060953][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.77131276  0.         ... -0.01340173  0.12775804
 -0.11435631], shape=(4547,), dtype=float64)

[2020-01-27 14:57:52.140587][__main__.TRPOAgent.log][linesearch]: improvement: -0.11243628876603906

[2020-01-27 14:57:52.141145][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.994431011537719

[2020-01-27 14:57:52.740648][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 353.0585496960691

[2020-01-27 14:57:52.741070][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:57:52.758260][__main__.TRPOAgent.log][learning]: Episode #20

[2020-01-27 14:57:52.758628][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:57:52.810891][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:57:52.811535][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:57:52.812144][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:57:52.813861][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:57:56.426197][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:57:56.467666][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:57:56.468236][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:57:56.469045][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:57:56.468955][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:57:56.470141][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:57:59.758141][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:57:59.844371][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:57:59.844986][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:57:59.845660][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:57:59.845587][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:57:59.846841][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:58:03.233442][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:58:03.280364][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:58:03.280991][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:58:03.281622][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:58:03.281687][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:58:03.283974][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:58:05.946862][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1137

[2020-01-27 14:58:06.287861][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:58:06.288520][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:58:06.289454][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:58:06.289635][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:58:06.293270][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:58:09.637571][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:58:09.700884][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:58:09.701465][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:58:09.702179][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:58:09.702367][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:58:09.706042][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:58:13.078470][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:58:13.109521][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:58:13.110043][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:58:13.110658][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:58:13.110718][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:58:13.112481][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:58:16.412595][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:58:16.478669][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:58:16.479329][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:58:16.480321][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:58:16.480390][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:58:16.482163][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:58:19.691914][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:58:19.735894][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:58:19.736487][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:58:19.737159][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:58:19.737222][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:58:19.738941][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:58:22.986194][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:58:23.026546][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:58:23.027196][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:58:23.027944][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:58:23.028032][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:58:23.029819][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:58:26.258526][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:58:26.279662][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:58:26.280442][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:58:26.281024][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:58:26.281149][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:58:26.283260][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:58:29.467360][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:58:29.503944][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:58:29.504536][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:58:29.505125][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:58:29.505229][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:58:29.507455][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:58:32.775841][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:58:32.807512][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:58:32.808059][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:58:32.808916][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:58:32.808989][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:58:32.810991][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:58:36.072680][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:58:36.093016][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:58:36.093607][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:58:36.094519][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:58:36.094576][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:58:36.096364][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:58:38.099585][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 897

[2020-01-27 14:58:38.646413][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:58:38.646983][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:58:38.647731][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:58:38.647834][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:58:38.653223][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:58:41.874635][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:58:41.920103][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:58:41.920749][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:58:41.931591][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:58:42.317746][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:58:42.343285][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:58:42.344670][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 44034, Batch size: 4500, Number of batches: 10

[2020-01-27 14:58:42.345044][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:58:42.373292][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:58:45.798441][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.21604795 -0.         ...  0.02634663 -0.19343594
  0.16708932]

[2020-01-27 14:58:45.798845][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:58:46.181012][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.03144064  0.         ...  0.08302478 -0.11356071
  0.03053593], shape=(4547,), dtype=float64)

[2020-01-27 14:58:46.252605][__main__.TRPOAgent.log][linesearch]: improvement: -0.11990015683203836

[2020-01-27 14:58:46.275698][__main__.TRPOAgent.log][linesearch]: improvement: -0.06822897451465026

[2020-01-27 14:58:46.298346][__main__.TRPOAgent.log][linesearch]: improvement: -0.039370116247619436

[2020-01-27 14:58:46.321504][__main__.TRPOAgent.log][linesearch]: improvement: -0.02342140115917246

[2020-01-27 14:58:46.344047][__main__.TRPOAgent.log][linesearch]: improvement: -0.01410535380035638

[2020-01-27 14:58:46.366507][__main__.TRPOAgent.log][linesearch]: improvement: -0.008510063702464654

[2020-01-27 14:58:46.390150][__main__.TRPOAgent.log][linesearch]: improvement: -0.005067682244801253

[2020-01-27 14:58:46.412796][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030338248043024763

[2020-01-27 14:58:46.436432][__main__.TRPOAgent.log][linesearch]: improvement: -0.00181042930838049

[2020-01-27 14:58:46.460467][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010721196160630164

[2020-01-27 14:58:46.460903][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.603941723946936e-07, Discarded policy loss value: -1.8522118655724036

[2020-01-27 14:58:47.210817][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.980933579521

[2020-01-27 14:58:47.216147][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:58:50.667026][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.19186691 -0.         ... -0.10683503 -0.01525204
  0.12208707]

[2020-01-27 14:58:50.667422][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:58:51.046753][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.18847139  0.         ... -0.16093989  0.1567977
  0.00414218], shape=(4547,), dtype=float64)

[2020-01-27 14:58:51.116501][__main__.TRPOAgent.log][linesearch]: improvement: -0.12150614973409887

[2020-01-27 14:58:51.116926][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.0671057138765309

[2020-01-27 14:58:51.844758][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.7515628707931

[2020-01-27 14:58:51.845166][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:58:51.852350][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:58:55.268172][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.11646958 -0.         ... -0.07546443 -0.00399979
  0.07946422]

[2020-01-27 14:58:55.268561][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:58:55.664562][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.10430413  0.         ... -0.03606039  0.11053084
 -0.07447046], shape=(4547,), dtype=float64)

[2020-01-27 14:58:55.742240][__main__.TRPOAgent.log][linesearch]: improvement: -0.10979623440603359

[2020-01-27 14:58:55.764100][__main__.TRPOAgent.log][linesearch]: improvement: -0.07256621519412154

[2020-01-27 14:58:55.764545][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.6424619068994352

[2020-01-27 14:58:56.481792][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9978220791998

[2020-01-27 14:58:56.482203][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:58:56.493049][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:58:59.989745][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.16774551 -0.         ... -0.07745301 -0.01171121
  0.08916422]

[2020-01-27 14:58:59.990124][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:59:00.373029][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.03456497  0.         ...  0.00519956  0.0673617
 -0.07256126], shape=(4547,), dtype=float64)

[2020-01-27 14:59:00.446412][__main__.TRPOAgent.log][linesearch]: improvement: -0.12633083785201385

[2020-01-27 14:59:00.468386][__main__.TRPOAgent.log][linesearch]: improvement: -0.0731341469906925

[2020-01-27 14:59:00.468828][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.07421810622014265

[2020-01-27 14:59:01.208907][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.90327166737217

[2020-01-27 14:59:01.209325][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:59:01.216737][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:59:04.596218][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.34067358 -0.         ... -0.32464338  0.24389567
  0.08074771]

[2020-01-27 14:59:04.596609][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:59:04.986085][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.80746187  0.         ...  0.05371507  0.00183134
 -0.05554641], shape=(4547,), dtype=float64)

[2020-01-27 14:59:05.058534][__main__.TRPOAgent.log][linesearch]: improvement: -0.2278215645605097

[2020-01-27 14:59:05.079271][__main__.TRPOAgent.log][linesearch]: improvement: -0.14627352664422338

[2020-01-27 14:59:05.101087][__main__.TRPOAgent.log][linesearch]: improvement: -0.09031842415317598

[2020-01-27 14:59:05.123209][__main__.TRPOAgent.log][linesearch]: improvement: -0.05538842441567288

[2020-01-27 14:59:05.143902][__main__.TRPOAgent.log][linesearch]: improvement: -0.03355069211063133

[2020-01-27 14:59:05.165309][__main__.TRPOAgent.log][linesearch]: improvement: -0.02025743416184783

[2020-01-27 14:59:05.186418][__main__.TRPOAgent.log][linesearch]: improvement: -0.01222740796246502

[2020-01-27 14:59:05.208780][__main__.TRPOAgent.log][linesearch]: improvement: -0.007340505961478672

[2020-01-27 14:59:05.230629][__main__.TRPOAgent.log][linesearch]: improvement: -0.004409432463339302

[2020-01-27 14:59:05.251720][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026453432818307476

[2020-01-27 14:59:05.252153][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.71031106774234e-07, Discarded policy loss value: -0.4312387914090779

[2020-01-27 14:59:05.981837][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.27193844507417

[2020-01-27 14:59:05.986784][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:59:09.432708][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.21023239 -0.         ... -0.1074401  -0.15446179
  0.2619019 ]

[2020-01-27 14:59:09.433096][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:59:09.819483][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.10752016  0.         ... -0.04686442 -0.01148205
  0.05834647], shape=(4547,), dtype=float64)

[2020-01-27 14:59:09.889761][__main__.TRPOAgent.log][linesearch]: improvement: -0.15772141929276232

[2020-01-27 14:59:09.912201][__main__.TRPOAgent.log][linesearch]: improvement: -0.10051914533290336

[2020-01-27 14:59:09.933913][__main__.TRPOAgent.log][linesearch]: improvement: -0.06360107849837954

[2020-01-27 14:59:09.957062][__main__.TRPOAgent.log][linesearch]: improvement: -0.03959896997642773

[2020-01-27 14:59:09.979552][__main__.TRPOAgent.log][linesearch]: improvement: -0.024161568525883914

[2020-01-27 14:59:10.002903][__main__.TRPOAgent.log][linesearch]: improvement: -0.014557942982542954

[2020-01-27 14:59:10.025236][__main__.TRPOAgent.log][linesearch]: improvement: -0.00873825299416603

[2020-01-27 14:59:10.050097][__main__.TRPOAgent.log][linesearch]: improvement: -0.005243164744283713

[2020-01-27 14:59:10.072853][__main__.TRPOAgent.log][linesearch]: improvement: -0.0031487092377566195

[2020-01-27 14:59:10.095382][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018943132449801559

[2020-01-27 14:59:10.095814][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.369780587423832e-07, Discarded policy loss value: -0.16823655492653639

[2020-01-27 14:59:10.822258][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.2050001719188

[2020-01-27 14:59:10.827722][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:59:14.251362][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.2386401  -0.         ... -0.1862229  -0.07177116
  0.25799406]

[2020-01-27 14:59:14.251760][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:59:14.634102][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.15227836  0.         ... -0.03231073  0.010325
  0.02198573], shape=(4547,), dtype=float64)

[2020-01-27 14:59:14.711319][__main__.TRPOAgent.log][linesearch]: improvement: -0.12158793532160425

[2020-01-27 14:59:14.734623][__main__.TRPOAgent.log][linesearch]: improvement: -0.07407275241129374

[2020-01-27 14:59:14.735088][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.04244095725865807

[2020-01-27 14:59:15.454933][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.01117384028186

[2020-01-27 14:59:15.455334][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:59:15.462490][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:59:18.884226][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02721576 -0.         ... -0.13089232 -0.02891619
  0.1598085 ]

[2020-01-27 14:59:18.884602][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:59:19.271604][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.2423057   0.         ...  0.03639415  0.00871454
 -0.0451087 ], shape=(4547,), dtype=float64)

[2020-01-27 14:59:19.345011][__main__.TRPOAgent.log][linesearch]: improvement: -0.13282227249672826

[2020-01-27 14:59:19.366889][__main__.TRPOAgent.log][linesearch]: improvement: -0.07966270353610408

[2020-01-27 14:59:19.367313][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.3064247613326167

[2020-01-27 14:59:20.070695][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.82382772581417

[2020-01-27 14:59:20.071092][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:59:20.077745][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:59:23.509248][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.05277181 -0.         ... -0.33230692 -0.00726825
  0.33957517]

[2020-01-27 14:59:23.509637][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:59:23.899421][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.66750977  0.         ... -0.1257467   0.03261446
  0.09313225], shape=(4547,), dtype=float64)

[2020-01-27 14:59:23.969240][__main__.TRPOAgent.log][linesearch]: improvement: -1.3393489147792146

[2020-01-27 14:59:23.992238][__main__.TRPOAgent.log][linesearch]: improvement: -0.5095047280859123

[2020-01-27 14:59:24.014180][__main__.TRPOAgent.log][linesearch]: improvement: -0.21408631177075527

[2020-01-27 14:59:24.037082][__main__.TRPOAgent.log][linesearch]: improvement: -0.10734637531666746

[2020-01-27 14:59:24.058745][__main__.TRPOAgent.log][linesearch]: improvement: -0.05939413483103434

[2020-01-27 14:59:24.080797][__main__.TRPOAgent.log][linesearch]: improvement: -0.03454243025988077

[2020-01-27 14:59:24.102690][__main__.TRPOAgent.log][linesearch]: improvement: -0.02015503193740875

[2020-01-27 14:59:24.125063][__main__.TRPOAgent.log][linesearch]: improvement: -0.01198070643873772

[2020-01-27 14:59:24.147670][__main__.TRPOAgent.log][linesearch]: improvement: -0.007165416487459886

[2020-01-27 14:59:24.169242][__main__.TRPOAgent.log][linesearch]: improvement: -0.004309653731446428

[2020-01-27 14:59:24.169668][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.58897331311398e-07, Discarded policy loss value: -0.2740149152113267

[2020-01-27 14:59:24.893429][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9148313892209

[2020-01-27 14:59:24.897512][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 3534

[2020-01-27 14:59:27.593274][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.29917473 -0.         ...  0.08170505 -0.06338035
 -0.0183247 ]

[2020-01-27 14:59:27.593671][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:59:27.929780][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.05729679  0.         ...  0.02593079 -0.00803209
 -0.0178987 ], shape=(4547,), dtype=float64)

[2020-01-27 14:59:27.993979][__main__.TRPOAgent.log][linesearch]: improvement: -0.11994822557011342

[2020-01-27 14:59:28.014230][__main__.TRPOAgent.log][linesearch]: improvement: -0.07713075515684498

[2020-01-27 14:59:28.014656][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.6412757093483026

[2020-01-27 14:59:28.583824][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 347.748506326392

[2020-01-27 14:59:28.584228][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:59:28.602029][__main__.TRPOAgent.log][learning]: Episode #21

[2020-01-27 14:59:28.602383][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:59:28.647688][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:59:28.649191][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:59:28.648293][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:59:28.650459][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:59:31.951515][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:59:32.017407][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:59:32.018216][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:59:32.019462][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:59:32.019536][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:59:32.021681][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:59:35.270051][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:59:35.286360][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:59:35.286879][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:59:35.287525][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:59:35.287464][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:59:35.288379][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:59:38.536571][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:59:38.571491][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:59:38.572235][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:59:38.572957][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:59:38.573043][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:59:38.577008][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:59:41.814358][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:59:41.861655][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:59:41.862451][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:59:41.863205][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:59:41.863279][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:59:41.865122][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:59:45.052987][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:59:45.116629][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:59:45.117471][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:59:45.118349][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:59:45.118416][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:59:45.121931][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:59:48.371903][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:59:48.386242][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:59:48.386979][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:59:48.387461][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:59:48.387555][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:59:48.391650][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:59:51.671456][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:59:51.710808][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:59:51.711570][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:59:51.712324][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:59:51.712393][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:59:51.714285][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:59:54.935265][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:59:54.976621][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:59:54.977450][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:59:54.978070][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:59:54.978127][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:59:54.979759][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:59:58.275135][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:59:58.288584][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:59:58.289210][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:59:58.290120][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:59:58.290185][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:59:58.292335][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:00:01.588304][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:00:01.606524][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:00:01.607153][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:00:01.607871][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:00:01.607933][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:00:01.610225][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:00:04.784910][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:00:04.908823][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:00:04.909484][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:00:04.910523][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:00:04.910611][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:00:04.912022][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:00:08.111525][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:00:08.174764][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:00:08.175413][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:00:08.176152][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:00:08.176204][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:00:08.178037][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:00:11.476451][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:00:11.490481][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:00:11.491157][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:00:11.491933][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:00:11.492001][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:00:11.497521][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:00:14.732286][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:00:14.780605][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:00:14.781219][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:00:14.782058][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:00:14.782126][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:00:14.784559][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:00:17.983031][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:00:18.070789][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:00:18.071469][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:00:18.082854][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:00:18.480554][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:00:18.506223][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:00:18.507585][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:00:18.507931][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:00:18.536559][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:00:22.013663][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.14827883 -0.         ...  0.19253911 -0.2063069
  0.01376779]

[2020-01-27 15:00:22.014090][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:22.391345][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.87667562  0.         ...  0.25108813 -0.15149564
 -0.09959249], shape=(4547,), dtype=float64)

[2020-01-27 15:00:22.463831][__main__.TRPOAgent.log][linesearch]: improvement: -0.4505541765432175

[2020-01-27 15:00:22.485633][__main__.TRPOAgent.log][linesearch]: improvement: -0.2140836757686415

[2020-01-27 15:00:22.508076][__main__.TRPOAgent.log][linesearch]: improvement: -0.11159590902153904

[2020-01-27 15:00:22.531284][__main__.TRPOAgent.log][linesearch]: improvement: -0.061508344892284184

[2020-01-27 15:00:22.552769][__main__.TRPOAgent.log][linesearch]: improvement: -0.035320569426299686

[2020-01-27 15:00:22.573928][__main__.TRPOAgent.log][linesearch]: improvement: -0.020605941056608845

[2020-01-27 15:00:22.596134][__main__.TRPOAgent.log][linesearch]: improvement: -0.012067501131500347

[2020-01-27 15:00:22.617715][__main__.TRPOAgent.log][linesearch]: improvement: -0.007143755475645275

[2020-01-27 15:00:22.650760][__main__.TRPOAgent.log][linesearch]: improvement: -0.004264577274511128

[2020-01-27 15:00:22.673514][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025548793517979895

[2020-01-27 15:00:22.673977][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.775695298202649e-07, Discarded policy loss value: -1.4584397041936563

[2020-01-27 15:00:23.400772][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0698917317186

[2020-01-27 15:00:23.406065][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:00:26.869318][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07148491 -0.         ... -0.04244087 -0.08068798
  0.12312885]

[2020-01-27 15:00:26.869749][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:27.252295][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.55276022  0.         ...  0.00788314  0.00412091
 -0.01200405], shape=(4547,), dtype=float64)

[2020-01-27 15:00:27.324703][__main__.TRPOAgent.log][linesearch]: improvement: -0.07748597820498435

[2020-01-27 15:00:27.325141][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.2816968306773742

[2020-01-27 15:00:28.051787][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9430062406588

[2020-01-27 15:00:28.052182][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:00:28.059476][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:00:31.448973][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00000000e+00  3.47869192e-03 -0.00000000e+00 ...  1.32793831e-01
 -1.32873754e-01  7.99231781e-05]

[2020-01-27 15:00:31.449363][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:31.847771][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.5137887   0.         ... -0.07367897 -0.04226643
  0.1159454 ], shape=(4547,), dtype=float64)

[2020-01-27 15:00:31.917656][__main__.TRPOAgent.log][linesearch]: improvement: -0.5450843972130182

[2020-01-27 15:00:31.939225][__main__.TRPOAgent.log][linesearch]: improvement: -0.2549607209122714

[2020-01-27 15:00:31.939658][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.0017073484375733743

[2020-01-27 15:00:32.652167][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9672205201745

[2020-01-27 15:00:32.652569][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:00:32.659817][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:00:36.021223][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.10382799 -0.         ... -0.07826655 -0.16970511
  0.24797167]

[2020-01-27 15:00:36.021608][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:36.407776][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.67536348  0.         ...  0.09445831 -0.14031853
  0.04586022], shape=(4547,), dtype=float64)

[2020-01-27 15:00:36.480040][__main__.TRPOAgent.log][linesearch]: improvement: -0.04381313678110604

[2020-01-27 15:00:36.502912][__main__.TRPOAgent.log][linesearch]: improvement: -0.04153713317324784

[2020-01-27 15:00:36.525274][__main__.TRPOAgent.log][linesearch]: improvement: -0.030001273400767477

[2020-01-27 15:00:36.547655][__main__.TRPOAgent.log][linesearch]: improvement: -0.021247120723874906

[2020-01-27 15:00:36.569641][__main__.TRPOAgent.log][linesearch]: improvement: -0.014392908024851242

[2020-01-27 15:00:36.593060][__main__.TRPOAgent.log][linesearch]: improvement: -0.009387369866308282

[2020-01-27 15:00:36.615831][__main__.TRPOAgent.log][linesearch]: improvement: -0.0059220618746108344

[2020-01-27 15:00:36.648451][__main__.TRPOAgent.log][linesearch]: improvement: -0.0036169688871638406

[2020-01-27 15:00:36.671791][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021822728669299574

[2020-01-27 15:00:36.695763][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013182629343106433

[2020-01-27 15:00:36.696216][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.418594726990827e-07, Discarded policy loss value: -0.07531760677956623

[2020-01-27 15:00:37.422547][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9532334200903

[2020-01-27 15:00:37.427654][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:00:40.849424][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07981296 -0.         ... -0.06156321  0.02362754
  0.03793566]

[2020-01-27 15:00:40.849816][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:41.231861][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.56973867  0.         ...  0.08564434 -0.05271519
 -0.03292915], shape=(4547,), dtype=float64)

[2020-01-27 15:00:41.304108][__main__.TRPOAgent.log][linesearch]: improvement: -0.06447379752594033

[2020-01-27 15:00:41.326698][__main__.TRPOAgent.log][linesearch]: improvement: -0.046143593303598504

[2020-01-27 15:00:41.327141][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.13662072391868302

[2020-01-27 15:00:42.063579][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8641921210047

[2020-01-27 15:00:42.063977][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:00:42.070949][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:00:45.492686][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02259167 -0.         ...  0.11088722 -0.257596
  0.14670878]

[2020-01-27 15:00:45.493087][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:45.885641][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.00057896  0.         ...  0.01207772  0.11340062
 -0.12547834], shape=(4547,), dtype=float64)

[2020-01-27 15:00:45.957467][__main__.TRPOAgent.log][linesearch]: improvement: -0.21546233462026734

[2020-01-27 15:00:45.979866][__main__.TRPOAgent.log][linesearch]: improvement: -0.11156263135597755

[2020-01-27 15:00:46.002529][__main__.TRPOAgent.log][linesearch]: improvement: -0.060727111105537135

[2020-01-27 15:00:46.025585][__main__.TRPOAgent.log][linesearch]: improvement: -0.03401688974767786

[2020-01-27 15:00:46.047605][__main__.TRPOAgent.log][linesearch]: improvement: -0.01942509416758087

[2020-01-27 15:00:46.070459][__main__.TRPOAgent.log][linesearch]: improvement: -0.01161135402056912

[2020-01-27 15:00:46.094063][__main__.TRPOAgent.log][linesearch]: improvement: -0.007129610727747926

[2020-01-27 15:00:46.117185][__main__.TRPOAgent.log][linesearch]: improvement: -0.004268110261343994

[2020-01-27 15:00:46.140132][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025567003444116887

[2020-01-27 15:00:46.162622][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015086870859193358

[2020-01-27 15:00:46.163056][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.487736912471178e-07, Discarded policy loss value: -0.34183334493977235

[2020-01-27 15:00:46.875703][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.94530749870376

[2020-01-27 15:00:46.881123][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:00:50.317714][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.10948701 -0.         ... -0.03408269 -0.04184372
  0.07592641]

[2020-01-27 15:00:50.318090][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:50.703674][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.13677757  0.         ...  0.02817975 -0.14645262
  0.11827287], shape=(4547,), dtype=float64)

[2020-01-27 15:00:50.774689][__main__.TRPOAgent.log][linesearch]: improvement: -0.10910672227808499

[2020-01-27 15:00:50.775138][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.476902427583805

[2020-01-27 15:00:51.486847][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9094899463825

[2020-01-27 15:00:51.494070][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:00:51.513730][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:00:54.965706][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.10431551 -0.         ... -0.11023413  0.141577
 -0.03134288]

[2020-01-27 15:00:54.966090][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:00:55.342742][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -2.32468938  0.         ...  0.17222935 -0.03020192
 -0.14202743], shape=(4547,), dtype=float64)

[2020-01-27 15:00:55.411727][__main__.TRPOAgent.log][linesearch]: improvement: -0.028232764633160268

[2020-01-27 15:00:55.433010][__main__.TRPOAgent.log][linesearch]: improvement: -0.039940393157479936

[2020-01-27 15:00:55.454892][__main__.TRPOAgent.log][linesearch]: improvement: -0.04191001421203788

[2020-01-27 15:00:55.477177][__main__.TRPOAgent.log][linesearch]: improvement: -0.034039683951057664

[2020-01-27 15:00:55.498829][__main__.TRPOAgent.log][linesearch]: improvement: -0.02146606077912777

[2020-01-27 15:00:55.519354][__main__.TRPOAgent.log][linesearch]: improvement: -0.014029523207328864

[2020-01-27 15:00:55.542426][__main__.TRPOAgent.log][linesearch]: improvement: -0.008733128423012715

[2020-01-27 15:00:55.563815][__main__.TRPOAgent.log][linesearch]: improvement: -0.0053704276113769045

[2020-01-27 15:00:55.585860][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032926706643219283

[2020-01-27 15:00:55.607338][__main__.TRPOAgent.log][linesearch]: improvement: -0.0020013151830806097

[2020-01-27 15:00:55.607772][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.79219583279603e-07, Discarded policy loss value: -0.38148367659498134

[2020-01-27 15:00:56.288840][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.1064490795626

[2020-01-27 15:00:56.293393][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:00:59.739429][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02598784 -0.         ...  0.0477181  -0.05974215
  0.01202405]

[2020-01-27 15:00:59.739808][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:01:00.120260][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.36797089  0.         ...  0.1759546  -0.21649424
  0.04053964], shape=(4547,), dtype=float64)

[2020-01-27 15:01:00.190697][__main__.TRPOAgent.log][linesearch]: improvement: -0.07049917913745696

[2020-01-27 15:01:00.212367][__main__.TRPOAgent.log][linesearch]: improvement: -0.05024220641522109

[2020-01-27 15:01:00.234418][__main__.TRPOAgent.log][linesearch]: improvement: -0.031861634468431876

[2020-01-27 15:01:00.234850][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.4095357275923825

[2020-01-27 15:01:00.959360][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0073626304165

[2020-01-27 15:01:00.959786][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:01:00.966969][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:01:04.343823][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.15591568 -0.         ... -0.09349758 -0.23599905
  0.32949663]

[2020-01-27 15:01:04.344221][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:01:04.728004][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.26850551  0.         ...  0.0056029  -0.05332375
  0.04772085], shape=(4547,), dtype=float64)

[2020-01-27 15:01:04.797939][__main__.TRPOAgent.log][linesearch]: improvement: -0.16135700190035035

[2020-01-27 15:01:04.819008][__main__.TRPOAgent.log][linesearch]: improvement: -0.09681774104368499

[2020-01-27 15:01:04.819434][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.020671980243204314

[2020-01-27 15:01:05.536690][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.03712565812845

[2020-01-27 15:01:05.537094][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:01:05.553673][__main__.TRPOAgent.log][learning]: Episode #22

[2020-01-27 15:01:05.554024][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:01:05.593519][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:01:05.595107][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:01:05.594038][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:01:05.596453][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:01:08.965755][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:01:08.969225][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:01:08.969742][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:01:08.970227][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:01:08.970348][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:01:08.971939][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:01:12.256246][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:01:12.266195][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:01:12.267182][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:01:12.267815][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:01:12.267894][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:01:12.269759][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:01:15.500422][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:01:15.510721][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:01:15.511322][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:01:15.511885][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:01:15.512011][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:01:15.515449][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:01:18.787828][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:01:18.811651][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:01:18.812251][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:01:18.812860][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:01:18.812921][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:01:18.815196][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:01:22.068005][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:01:22.068434][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:01:22.069789][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:01:22.070757][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:01:22.070822][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:01:22.072891][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:01:25.320599][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:01:25.334523][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:01:25.335128][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:01:25.336012][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:01:25.336080][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:01:25.337924][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:01:28.618053][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:01:28.634170][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:01:28.634982][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:01:28.635926][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:01:28.636034][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:01:28.640988][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:01:31.859632][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:01:31.895018][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:01:31.895675][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:01:31.896390][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:01:31.896472][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:01:31.899649][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:01:35.170446][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:01:35.196797][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:01:35.197489][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:01:35.198304][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:01:35.198244][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:01:35.199288][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:01:38.425488][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:01:38.479199][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:01:38.479842][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:01:38.480558][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:01:38.480498][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:01:38.481446][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:01:41.672734][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:01:41.724197][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:01:41.724801][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:01:41.725616][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:01:41.725778][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:01:41.727581][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:01:44.932070][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:01:44.989354][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:01:44.990118][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:01:44.990892][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:01:44.991056][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:01:44.992617][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:01:48.245154][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:01:48.276422][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:01:48.277029][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:01:48.277854][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:01:48.277731][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:01:48.278830][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:01:51.497632][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:01:51.530085][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:01:51.530656][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:01:51.531436][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:01:51.531523][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:01:51.533870][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:01:54.840923][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:01:54.858194][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:01:54.858720][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:01:54.870063][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:01:55.266730][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:01:55.292777][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:01:55.294408][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:01:55.294935][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:01:55.328076][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:01:58.744326][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.04850582 -0.         ... -0.00056945  0.20566292
 -0.20509347]

[2020-01-27 15:01:58.744704][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:01:59.140025][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.51940499  0.         ... -0.0670298  -0.0746222
  0.141652  ], shape=(4547,), dtype=float64)

[2020-01-27 15:01:59.212684][__main__.TRPOAgent.log][linesearch]: improvement: -0.03279338052513352

[2020-01-27 15:01:59.235309][__main__.TRPOAgent.log][linesearch]: improvement: -0.039031774668537966

[2020-01-27 15:01:59.257645][__main__.TRPOAgent.log][linesearch]: improvement: -0.028669500069959927

[2020-01-27 15:01:59.279773][__main__.TRPOAgent.log][linesearch]: improvement: -0.018908723558581197

[2020-01-27 15:01:59.301616][__main__.TRPOAgent.log][linesearch]: improvement: -0.011823485933267278

[2020-01-27 15:01:59.323808][__main__.TRPOAgent.log][linesearch]: improvement: -0.006883733664279537

[2020-01-27 15:01:59.346193][__main__.TRPOAgent.log][linesearch]: improvement: -0.003932524373239488

[2020-01-27 15:01:59.369662][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023667055729555547

[2020-01-27 15:01:59.393038][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014750474821317444

[2020-01-27 15:01:59.415553][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009094470891207879

[2020-01-27 15:01:59.415985][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.1399027806412e-07, Discarded policy loss value: -0.11493844455724138

[2020-01-27 15:02:00.141725][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.1186133278186

[2020-01-27 15:02:00.146817][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:02:03.579143][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05655233 -0.         ...  0.02939599 -0.16743849
  0.1380425 ]

[2020-01-27 15:02:03.579538][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:03.981910][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.40124211  0.         ...  0.15378798 -0.10517955
 -0.04860843], shape=(4547,), dtype=float64)

[2020-01-27 15:02:04.050931][__main__.TRPOAgent.log][linesearch]: improvement: -0.338970840415616

[2020-01-27 15:02:04.072070][__main__.TRPOAgent.log][linesearch]: improvement: -0.1427365684197652

[2020-01-27 15:02:04.094747][__main__.TRPOAgent.log][linesearch]: improvement: -0.06688995648116602

[2020-01-27 15:02:04.116852][__main__.TRPOAgent.log][linesearch]: improvement: -0.03209958476093763

[2020-01-27 15:02:04.117292][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 0.14464061757109165

[2020-01-27 15:02:04.853449][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.92077344506646

[2020-01-27 15:02:04.853850][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:02:04.861139][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:02:08.301222][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.06253064 -0.         ... -0.08671194 -0.02009481
  0.10680675]

[2020-01-27 15:02:08.301598][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:08.695365][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.63223751  0.         ...  0.14301501 -0.55643995
  0.41342494], shape=(4547,), dtype=float64)

[2020-01-27 15:02:08.769166][__main__.TRPOAgent.log][linesearch]: improvement: -0.14631901547941764

[2020-01-27 15:02:08.790757][__main__.TRPOAgent.log][linesearch]: improvement: 0.017430894782589024

[2020-01-27 15:02:08.812665][__main__.TRPOAgent.log][linesearch]: improvement: 0.02473609841087293

[2020-01-27 15:02:08.834598][__main__.TRPOAgent.log][linesearch]: improvement: 0.013652882886889206

[2020-01-27 15:02:08.856026][__main__.TRPOAgent.log][linesearch]: improvement: 0.004816523510407955

[2020-01-27 15:02:08.878187][__main__.TRPOAgent.log][linesearch]: improvement: -0.0007738213870502886

[2020-01-27 15:02:08.901269][__main__.TRPOAgent.log][linesearch]: improvement: -0.004135259822544979

[2020-01-27 15:02:08.923990][__main__.TRPOAgent.log][linesearch]: improvement: -0.004999837983313937

[2020-01-27 15:02:08.945350][__main__.TRPOAgent.log][linesearch]: improvement: -0.003454525079952253

[2020-01-27 15:02:08.967197][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025646007257897896

[2020-01-27 15:02:08.967633][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 4.467886357349573e-06, Discarded policy loss value: -0.3030724245951266

[2020-01-27 15:02:09.697227][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0367572821304

[2020-01-27 15:02:09.702259][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:02:13.137863][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.0523146  -0.         ... -0.00724461  0.19827108
 -0.19102647]

[2020-01-27 15:02:13.138244][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:13.516505][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.6364636   0.         ... -0.02796441  0.03226323
 -0.00429882], shape=(4547,), dtype=float64)

[2020-01-27 15:02:13.588968][__main__.TRPOAgent.log][linesearch]: improvement: 0.009136156473168336

[2020-01-27 15:02:13.589426][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.004807982741064027

[2020-01-27 15:02:14.314596][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9707615795385

[2020-01-27 15:02:14.314997][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:02:14.322174][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:02:17.745298][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.04150897 -0.         ...  0.04092151  0.19857975
 -0.23950126]

[2020-01-27 15:02:17.745690][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:18.134733][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.30909823  0.         ... -0.01611156  0.10071203
 -0.08460047], shape=(4547,), dtype=float64)

[2020-01-27 15:02:18.204549][__main__.TRPOAgent.log][linesearch]: improvement: -0.10461179630733791

[2020-01-27 15:02:18.226158][__main__.TRPOAgent.log][linesearch]: improvement: -0.06711216705942591

[2020-01-27 15:02:18.247334][__main__.TRPOAgent.log][linesearch]: improvement: -0.04327809382259855

[2020-01-27 15:02:18.269220][__main__.TRPOAgent.log][linesearch]: improvement: -0.026514846763367433

[2020-01-27 15:02:18.290225][__main__.TRPOAgent.log][linesearch]: improvement: -0.016884428943018268

[2020-01-27 15:02:18.311616][__main__.TRPOAgent.log][linesearch]: improvement: -0.010310564027961946

[2020-01-27 15:02:18.333364][__main__.TRPOAgent.log][linesearch]: improvement: -0.0062440355584452295

[2020-01-27 15:02:18.355335][__main__.TRPOAgent.log][linesearch]: improvement: -0.003820124155636234

[2020-01-27 15:02:18.375960][__main__.TRPOAgent.log][linesearch]: improvement: -0.00231976429083644

[2020-01-27 15:02:18.398213][__main__.TRPOAgent.log][linesearch]: improvement: -0.001394015514348812

[2020-01-27 15:02:18.398658][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.147898583680525e-07, Discarded policy loss value: -0.12817119406360225

[2020-01-27 15:02:19.126422][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.951400747983

[2020-01-27 15:02:19.131619][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:02:22.491922][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00392445 -0.         ... -0.01276424 -0.01358465
  0.02634889]

[2020-01-27 15:02:22.492307][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:22.877672][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.87094253  0.         ... -0.05773655 -0.03464837
  0.09238492], shape=(4547,), dtype=float64)

[2020-01-27 15:02:22.954433][__main__.TRPOAgent.log][linesearch]: improvement: -0.47027988208819693

[2020-01-27 15:02:22.978052][__main__.TRPOAgent.log][linesearch]: improvement: 0.013402030743352111

[2020-01-27 15:02:22.999887][__main__.TRPOAgent.log][linesearch]: improvement: -0.01941399948545508

[2020-01-27 15:02:23.022276][__main__.TRPOAgent.log][linesearch]: improvement: -0.03152003047206928

[2020-01-27 15:02:23.043502][__main__.TRPOAgent.log][linesearch]: improvement: -0.02484208389562914

[2020-01-27 15:02:23.067256][__main__.TRPOAgent.log][linesearch]: improvement: -0.015017404509980542

[2020-01-27 15:02:23.090022][__main__.TRPOAgent.log][linesearch]: improvement: -0.00861583173813113

[2020-01-27 15:02:23.112123][__main__.TRPOAgent.log][linesearch]: improvement: -0.005152394913357283

[2020-01-27 15:02:23.134135][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030627186573116694

[2020-01-27 15:02:23.155774][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018466369094585766

[2020-01-27 15:02:23.156230][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.3230997160342e-07, Discarded policy loss value: -0.26168362264564327

[2020-01-27 15:02:23.877731][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9386243783336

[2020-01-27 15:02:23.882706][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:02:27.246798][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01403494 -0.         ...  0.00541645 -0.0319891
  0.02657264]

[2020-01-27 15:02:27.247189][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:27.618704][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.02204355  0.         ... -0.02434286  0.05616957
 -0.03182671], shape=(4547,), dtype=float64)

[2020-01-27 15:02:27.699667][__main__.TRPOAgent.log][linesearch]: improvement: -0.06746217429050327

[2020-01-27 15:02:27.700116][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.7080093119018124

[2020-01-27 15:02:28.424760][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.00658773167953

[2020-01-27 15:02:28.425170][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:02:28.432196][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:02:31.841620][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00000000e+00 -9.81387461e-02 -0.00000000e+00 ...  3.13654773e-04
 -3.44839466e-01  3.44525811e-01]

[2020-01-27 15:02:31.842108][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:32.220535][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.14366955  0.         ... -0.11785322  0.04669123
  0.07116198], shape=(4547,), dtype=float64)

[2020-01-27 15:02:32.290669][__main__.TRPOAgent.log][linesearch]: improvement: -0.1046215217449808

[2020-01-27 15:02:32.312488][__main__.TRPOAgent.log][linesearch]: improvement: -0.07145569546119361

[2020-01-27 15:02:32.334945][__main__.TRPOAgent.log][linesearch]: improvement: -0.0468873945548403

[2020-01-27 15:02:32.356600][__main__.TRPOAgent.log][linesearch]: improvement: -0.02970827355783978

[2020-01-27 15:02:32.377851][__main__.TRPOAgent.log][linesearch]: improvement: -0.018495372759694678

[2020-01-27 15:02:32.399783][__main__.TRPOAgent.log][linesearch]: improvement: -0.0113760247478151

[2020-01-27 15:02:32.421043][__main__.TRPOAgent.log][linesearch]: improvement: -0.006943016651628164

[2020-01-27 15:02:32.443279][__main__.TRPOAgent.log][linesearch]: improvement: -0.004219513281697007

[2020-01-27 15:02:32.465043][__main__.TRPOAgent.log][linesearch]: improvement: -0.002562885589225433

[2020-01-27 15:02:32.488204][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015425525455498856

[2020-01-27 15:02:32.488643][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.084607993296455e-07, Discarded policy loss value: -0.15204902121955496

[2020-01-27 15:02:33.212670][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.83624708672056

[2020-01-27 15:02:33.219701][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:02:36.590616][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.10853616 -0.         ... -0.18601558  0.08173811
  0.10427746]

[2020-01-27 15:02:36.591007][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:36.991789][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.02347633  0.         ... -0.05215042  0.05148596
  0.00066446], shape=(4547,), dtype=float64)

[2020-01-27 15:02:37.063385][__main__.TRPOAgent.log][linesearch]: improvement: -0.15494847230588282

[2020-01-27 15:02:37.063814][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.6088129383848924

[2020-01-27 15:02:37.793893][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.995166699301

[2020-01-27 15:02:37.794295][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:02:37.801211][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:02:41.224149][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03272145 -0.         ...  0.007632   -0.14120036
  0.13356836]

[2020-01-27 15:02:41.224531][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:02:41.600641][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.07017035  0.         ...  0.08956658 -0.13288352
  0.04331694], shape=(4547,), dtype=float64)

[2020-01-27 15:02:41.680384][__main__.TRPOAgent.log][linesearch]: improvement: -0.08918167106743892

[2020-01-27 15:02:41.701178][__main__.TRPOAgent.log][linesearch]: improvement: -0.059275447786568

[2020-01-27 15:02:41.723673][__main__.TRPOAgent.log][linesearch]: improvement: -0.03691384256820049

[2020-01-27 15:02:41.745191][__main__.TRPOAgent.log][linesearch]: improvement: -0.022374356049083485

[2020-01-27 15:02:41.766941][__main__.TRPOAgent.log][linesearch]: improvement: -0.01345777354019427

[2020-01-27 15:02:41.787735][__main__.TRPOAgent.log][linesearch]: improvement: -0.008100418478883722

[2020-01-27 15:02:41.810497][__main__.TRPOAgent.log][linesearch]: improvement: -0.004844799423654386

[2020-01-27 15:02:41.832017][__main__.TRPOAgent.log][linesearch]: improvement: -0.002897963380562335

[2020-01-27 15:02:41.854377][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017385124758321768

[2020-01-27 15:02:41.877246][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010429308579661956

[2020-01-27 15:02:41.877673][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.354973547193371e-07, Discarded policy loss value: -0.12834954956021688

[2020-01-27 15:02:42.594288][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.2878067253012

[2020-01-27 15:02:42.609319][__main__.TRPOAgent.log][learning]: Episode #23

[2020-01-27 15:02:42.609717][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:02:42.657090][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:02:42.657673][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:02:42.658497][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:02:42.660454][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:02:46.002762][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:02:46.018216][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:02:46.018738][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:02:46.019373][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:02:46.019301][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:02:46.020353][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:02:49.388541][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:02:49.426043][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:02:49.426704][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:02:49.428288][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:02:49.428372][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:02:49.430352][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:02:52.624911][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:02:52.692024][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:02:52.692579][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:02:52.693186][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:02:52.693122][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:02:52.694602][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:02:55.917527][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:02:55.941483][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:02:55.941998][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:02:55.942570][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:02:55.942503][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:02:55.943532][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:02:59.179985][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:02:59.234824][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:02:59.235510][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:02:59.236242][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:02:59.236403][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:02:59.238990][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:03:02.470798][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:03:02.491537][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:03:02.492749][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:03:02.493549][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:03:02.493607][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:03:02.496650][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:03:05.758801][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:03:05.785199][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:03:05.785797][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:03:05.786679][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:03:05.786608][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:03:05.787650][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:03:09.038890][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:03:09.056978][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:03:09.057670][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:03:09.058562][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:03:09.058480][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:03:09.059649][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:03:12.236025][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:03:12.290394][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:03:12.291195][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:03:12.291856][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:03:12.291913][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:03:12.293706][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:03:15.499417][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:03:15.539921][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:03:15.540514][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:03:15.541182][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:03:15.541252][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:03:15.543194][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:03:18.831891][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:03:18.848416][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:03:18.849054][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:03:18.849614][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:03:18.849686][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:03:18.851724][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:03:22.038118][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:03:22.066413][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:03:22.067016][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:03:22.067645][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:03:22.067701][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:03:22.070592][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:03:25.245515][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:03:25.314846][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:03:25.315431][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:03:25.316403][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:03:25.316307][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:03:25.317322][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:03:28.566716][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:03:28.578540][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:03:28.579095][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:03:28.579818][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:03:28.579722][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:03:28.580955][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:03:31.778128][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:03:31.807113][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:03:31.807600][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:03:31.819228][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:03:32.219574][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:03:32.245799][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:03:32.247166][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:03:32.247512][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:03:32.276051][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:03:35.677135][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01498588 -0.         ...  0.01239341  0.05099112
 -0.06338453]

[2020-01-27 15:03:35.677524][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:03:36.060762][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.31372195  0.         ...  0.12703439 -0.13573185
  0.00869746], shape=(4547,), dtype=float64)

[2020-01-27 15:03:36.132336][__main__.TRPOAgent.log][linesearch]: improvement: -0.0967805056255232

[2020-01-27 15:03:36.154994][__main__.TRPOAgent.log][linesearch]: improvement: -0.06940722340971621

[2020-01-27 15:03:36.155429][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.01474962123834161

[2020-01-27 15:03:36.918266][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.7902957697455

[2020-01-27 15:03:36.918669][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:03:36.926229][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:03:40.350589][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.0533597  -0.         ...  0.13969741 -0.19651695
  0.05681953]

[2020-01-27 15:03:40.350986][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:03:40.742810][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.21393237  0.         ... -0.10958918 -0.0686578
  0.17824698], shape=(4547,), dtype=float64)

[2020-01-27 15:03:40.814387][__main__.TRPOAgent.log][linesearch]: improvement: -0.2513815537101043

[2020-01-27 15:03:40.836438][__main__.TRPOAgent.log][linesearch]: improvement: -0.14847500810278091

[2020-01-27 15:03:40.859220][__main__.TRPOAgent.log][linesearch]: improvement: -0.09327838636281771

[2020-01-27 15:03:40.880774][__main__.TRPOAgent.log][linesearch]: improvement: -0.05481519340783092

[2020-01-27 15:03:40.902515][__main__.TRPOAgent.log][linesearch]: improvement: -0.03177331609159495

[2020-01-27 15:03:40.924845][__main__.TRPOAgent.log][linesearch]: improvement: -0.018650259456052698

[2020-01-27 15:03:40.947787][__main__.TRPOAgent.log][linesearch]: improvement: -0.01149716590135974

[2020-01-27 15:03:40.970428][__main__.TRPOAgent.log][linesearch]: improvement: -0.007114529125588964

[2020-01-27 15:03:40.992675][__main__.TRPOAgent.log][linesearch]: improvement: -0.004282430364441758

[2020-01-27 15:03:41.015415][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025829785432458507

[2020-01-27 15:03:41.015857][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.307975778772639e-07, Discarded policy loss value: -0.6668219903245566

[2020-01-27 15:03:41.737489][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.719054462644

[2020-01-27 15:03:41.742617][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:03:45.181845][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.07412735 -0.         ... -0.02963502 -0.12112754
  0.15076256]

[2020-01-27 15:03:45.182224][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:03:45.554563][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.66000901  0.         ...  0.02160912 -0.04492167
  0.02331255], shape=(4547,), dtype=float64)

[2020-01-27 15:03:45.632369][__main__.TRPOAgent.log][linesearch]: improvement: 0.03469408134262297

[2020-01-27 15:03:45.661785][__main__.TRPOAgent.log][linesearch]: improvement: -0.021483607248165115

[2020-01-27 15:03:45.662226][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.8049372831704747

[2020-01-27 15:03:46.350858][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.947620226052

[2020-01-27 15:03:46.351243][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:03:46.361879][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:03:49.792447][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01490826 -0.         ... -0.06312806  0.0697908
 -0.00666275]

[2020-01-27 15:03:49.792850][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:03:50.168219][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.64843359  0.         ...  0.11889625 -0.14755982
  0.02866357], shape=(4547,), dtype=float64)

[2020-01-27 15:03:50.239492][__main__.TRPOAgent.log][linesearch]: improvement: -0.13520152585781905

[2020-01-27 15:03:50.261617][__main__.TRPOAgent.log][linesearch]: improvement: -0.08462676176781375

[2020-01-27 15:03:50.262044][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.2404758513523492

[2020-01-27 15:03:50.992754][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8445382816877

[2020-01-27 15:03:50.993163][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:03:51.000758][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:03:54.389559][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.06359375 -0.         ...  0.18010901 -0.19356567
  0.01345665]

[2020-01-27 15:03:54.389957][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:03:54.780025][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.89968462  0.         ... -0.22977297  0.34426749
 -0.11449452], shape=(4547,), dtype=float64)

[2020-01-27 15:03:54.852326][__main__.TRPOAgent.log][linesearch]: improvement: -0.7188388163149317

[2020-01-27 15:03:54.852768][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 1.089660283709649

[2020-01-27 15:03:55.591928][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.18996337228106

[2020-01-27 15:03:55.592323][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:03:55.599355][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:03:58.991203][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.1327409  -0.         ... -0.00873513 -0.08595134
  0.09468647]

[2020-01-27 15:03:58.991820][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:03:59.369501][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.83068818  0.         ... -0.11874927  0.25601994
 -0.13727066], shape=(4547,), dtype=float64)

[2020-01-27 15:03:59.441102][__main__.TRPOAgent.log][linesearch]: improvement: -0.030439469799059232

[2020-01-27 15:03:59.462379][__main__.TRPOAgent.log][linesearch]: improvement: -0.03712802035025076

[2020-01-27 15:03:59.462806][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.09090230091964682

[2020-01-27 15:04:00.157580][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.81713774727865

[2020-01-27 15:04:00.157957][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:04:00.172281][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:04:03.603282][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.18434472 -0.         ...  0.07859075 -0.23010286
  0.15151211]

[2020-01-27 15:04:03.603673][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:04:03.994127][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.5034831   0.         ...  0.23385112 -0.48410521
  0.2502541 ], shape=(4547,), dtype=float64)

[2020-01-27 15:04:04.072899][__main__.TRPOAgent.log][linesearch]: improvement: -0.1937581779980915

[2020-01-27 15:04:04.096531][__main__.TRPOAgent.log][linesearch]: improvement: -0.11816109588165474

[2020-01-27 15:04:04.120187][__main__.TRPOAgent.log][linesearch]: improvement: -0.07097320878131236

[2020-01-27 15:04:04.144093][__main__.TRPOAgent.log][linesearch]: improvement: -0.0424146813721909

[2020-01-27 15:04:04.167133][__main__.TRPOAgent.log][linesearch]: improvement: -0.02527064619960906

[2020-01-27 15:04:04.191357][__main__.TRPOAgent.log][linesearch]: improvement: -0.015161135851047614

[2020-01-27 15:04:04.215767][__main__.TRPOAgent.log][linesearch]: improvement: -0.0090955335402082

[2020-01-27 15:04:04.238095][__main__.TRPOAgent.log][linesearch]: improvement: -0.005456722357866373

[2020-01-27 15:04:04.260434][__main__.TRPOAgent.log][linesearch]: improvement: -0.0032737859821602155

[2020-01-27 15:04:04.287390][__main__.TRPOAgent.log][linesearch]: improvement: -0.00196417556168077

[2020-01-27 15:04:04.287855][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.223005422811479e-07, Discarded policy loss value: -0.39946882391444855

[2020-01-27 15:04:05.019800][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9809743635756

[2020-01-27 15:04:05.024847][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:04:08.427667][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.12467583 -0.         ...  0.01083598  0.06405598
 -0.07489196]

[2020-01-27 15:04:08.428057][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:04:08.814109][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.19904235  0.         ... -0.07272389  0.13556527
 -0.06284138], shape=(4547,), dtype=float64)

[2020-01-27 15:04:08.885543][__main__.TRPOAgent.log][linesearch]: improvement: -0.1589230435773853

[2020-01-27 15:04:08.908376][__main__.TRPOAgent.log][linesearch]: improvement: -0.07311865219817668

[2020-01-27 15:04:08.908804][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.18930943939947872

[2020-01-27 15:04:09.637720][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8401159936898

[2020-01-27 15:04:09.638168][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:04:09.647660][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:04:13.013748][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.24598449 -0.         ...  0.11925074 -0.10032751
 -0.01892323]

[2020-01-27 15:04:13.014122][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:04:13.392435][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.64181719  0.         ...  0.03162391 -0.09439639
  0.06277249], shape=(4547,), dtype=float64)

[2020-01-27 15:04:13.464729][__main__.TRPOAgent.log][linesearch]: improvement: -0.29495810642989884

[2020-01-27 15:04:13.486912][__main__.TRPOAgent.log][linesearch]: improvement: -0.12465109891154622

[2020-01-27 15:04:13.487337][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.3320963780664774

[2020-01-27 15:04:14.220425][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9367299969275

[2020-01-27 15:04:14.220822][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:04:14.227792][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:04:17.635371][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.04818616 -0.         ... -0.01472446 -0.11185221
  0.12657668]

[2020-01-27 15:04:17.635853][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:04:18.024911][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.05792576  0.         ... -0.27498412  0.71549895
 -0.44051483], shape=(4547,), dtype=float64)

[2020-01-27 15:04:18.095629][__main__.TRPOAgent.log][linesearch]: improvement: -0.02870157943230156

[2020-01-27 15:04:18.096060][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.29195085657805897

[2020-01-27 15:04:18.820493][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.99871063173043

[2020-01-27 15:04:18.820900][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:04:18.842135][__main__.TRPOAgent.log][learning]: Episode #24

[2020-01-27 15:04:18.842664][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:04:18.888939][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:04:18.890373][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:04:18.889452][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:04:18.891520][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:04:22.246186][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:04:22.265594][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:04:22.266455][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:04:22.267189][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:04:22.267137][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:04:22.268676][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:04:25.529381][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:04:25.539620][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:04:25.540200][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:04:25.540914][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:04:25.540845][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:04:25.541868][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:04:28.794570][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:04:28.845497][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:04:28.846086][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:04:28.846982][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:04:28.846926][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:04:28.847909][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:04:32.099109][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:04:32.139221][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:04:32.140153][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:04:32.140879][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:04:32.140989][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:04:32.143168][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:04:35.387944][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:04:35.436243][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:04:35.437407][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:04:35.438133][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:04:35.438189][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:04:35.439693][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:04:38.679775][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:04:38.725627][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:04:38.726476][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:04:38.727229][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:04:38.727161][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:04:38.728996][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:04:41.898788][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:04:41.975502][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:04:41.976048][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:04:41.976755][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:04:41.976810][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:04:41.978916][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:04:45.356501][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:04:45.423889][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:04:45.424660][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:04:45.425311][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:04:45.425258][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:04:45.426570][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:04:48.672724][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:04:48.698708][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:04:48.699528][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:04:48.700123][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:04:48.700341][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:04:48.701922][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:04:51.899994][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:04:51.963225][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:04:51.963882][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:04:51.964587][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:04:51.964685][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:04:51.967501][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:04:55.130387][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:04:55.208152][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:04:55.208861][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:04:55.209529][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:04:55.209585][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:04:55.213052][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:04:58.446378][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:04:58.447035][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:04:58.448221][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:04:58.448754][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:04:58.448815][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:04:58.450561][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:05:01.695810][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:05:01.740548][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:05:01.741148][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:05:01.741951][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:05:01.742016][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:05:01.743786][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:05:04.967389][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:05:05.013749][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:05:05.014294][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:05:05.015320][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:05:05.015227][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:05:05.016342][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:05:08.192284][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:05:08.258198][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:05:08.258761][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:05:08.270101][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:05:08.685185][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:05:08.711560][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:05:08.712966][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:05:08.713338][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:05:08.742076][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:05:12.201938][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.35913083 -0.         ...  0.11962533  0.06073297
 -0.1803583 ]

[2020-01-27 15:05:12.202317][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:12.579868][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.23006532  0.         ...  0.01880793  0.0058467
 -0.02465463], shape=(4547,), dtype=float64)

[2020-01-27 15:05:12.659430][__main__.TRPOAgent.log][linesearch]: improvement: -0.1235755446203276

[2020-01-27 15:05:12.681914][__main__.TRPOAgent.log][linesearch]: improvement: -0.06815837654361812

[2020-01-27 15:05:12.682480][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.39116541512479913

[2020-01-27 15:05:13.362629][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9200131635644

[2020-01-27 15:05:13.362996][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:13.369783][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:05:16.798724][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.18399366 -0.         ... -0.10121656  0.08411489
  0.01710167]

[2020-01-27 15:05:16.799133][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:17.185327][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.54762473  0.         ... -0.02889336  0.05036586
 -0.02147249], shape=(4547,), dtype=float64)

[2020-01-27 15:05:17.259145][__main__.TRPOAgent.log][linesearch]: improvement: -0.08862020612963432

[2020-01-27 15:05:17.259586][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.17021209782232796

[2020-01-27 15:05:17.992617][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.04720949152653

[2020-01-27 15:05:17.993030][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:18.004289][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:05:21.442247][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.61772508 -0.         ... -0.23116696  0.08353251
  0.14763445]

[2020-01-27 15:05:21.442636][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:21.831049][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.33709122  0.         ...  0.00191804  0.02776631
 -0.02968435], shape=(4547,), dtype=float64)

[2020-01-27 15:05:21.903274][__main__.TRPOAgent.log][linesearch]: improvement: -0.11674519301256361

[2020-01-27 15:05:21.903701][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.1424933894061562

[2020-01-27 15:05:22.623390][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.025198643818

[2020-01-27 15:05:22.623794][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:22.636733][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:05:26.113407][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.18829318 -0.         ... -0.05458512 -0.06540538
  0.1199905 ]

[2020-01-27 15:05:26.113787][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:26.494869][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.73997918  0.         ...  0.04635119 -0.0133992
 -0.03295199], shape=(4547,), dtype=float64)

[2020-01-27 15:05:26.566965][__main__.TRPOAgent.log][linesearch]: improvement: -0.09710740733703255

[2020-01-27 15:05:26.589153][__main__.TRPOAgent.log][linesearch]: improvement: -0.0535884346160132

[2020-01-27 15:05:26.611244][__main__.TRPOAgent.log][linesearch]: improvement: -0.031527566073812796

[2020-01-27 15:05:26.639393][__main__.TRPOAgent.log][linesearch]: improvement: -0.017881836663941497

[2020-01-27 15:05:26.665450][__main__.TRPOAgent.log][linesearch]: improvement: -0.010286208463138491

[2020-01-27 15:05:26.687701][__main__.TRPOAgent.log][linesearch]: improvement: -0.005803484226904798

[2020-01-27 15:05:26.710683][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033708320090788946

[2020-01-27 15:05:26.734489][__main__.TRPOAgent.log][linesearch]: improvement: -0.0020044102789096296

[2020-01-27 15:05:26.757718][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011938899661587676

[2020-01-27 15:05:26.780182][__main__.TRPOAgent.log][linesearch]: improvement: -0.0007154323699392617

[2020-01-27 15:05:26.780622][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.049708039082612e-07, Discarded policy loss value: -0.2685404785460806

[2020-01-27 15:05:27.500558][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.06615025507216

[2020-01-27 15:05:27.505701][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:05:30.955088][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.13050944 -0.         ... -0.00690527  0.26252876
 -0.25562349]

[2020-01-27 15:05:30.955472][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:31.334159][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.13100531  0.         ...  0.10148609  0.0528555
 -0.15434159], shape=(4547,), dtype=float64)

[2020-01-27 15:05:31.405159][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023005099908847693

[2020-01-27 15:05:31.405592][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.15574346359771496

[2020-01-27 15:05:32.124393][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8721854837956

[2020-01-27 15:05:32.124789][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:32.131963][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:05:35.550343][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.07852086 -0.         ... -0.01650512  0.2435289
 -0.22702378]

[2020-01-27 15:05:35.550726][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:35.936081][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.09646936  0.         ... -0.007517    0.09687799
 -0.08936099], shape=(4547,), dtype=float64)

[2020-01-27 15:05:36.009019][__main__.TRPOAgent.log][linesearch]: improvement: -0.4872346405224102

[2020-01-27 15:05:36.031742][__main__.TRPOAgent.log][linesearch]: improvement: -0.22743678658055916

[2020-01-27 15:05:36.032180][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.5840082248805932

[2020-01-27 15:05:36.755977][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0263914131494

[2020-01-27 15:05:36.756372][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:36.763667][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:05:40.177593][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03681936 -0.         ...  0.01776293 -0.05608799
  0.03832506]

[2020-01-27 15:05:40.177972][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:40.555309][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.80374267  0.         ... -0.02067092  0.04466267
 -0.02399175], shape=(4547,), dtype=float64)

[2020-01-27 15:05:40.630932][__main__.TRPOAgent.log][linesearch]: improvement: 0.0191120014452289

[2020-01-27 15:05:40.659154][__main__.TRPOAgent.log][linesearch]: improvement: -0.005881344127804666

[2020-01-27 15:05:40.680707][__main__.TRPOAgent.log][linesearch]: improvement: -0.011972336879116682

[2020-01-27 15:05:40.681149][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.10817073575318197

[2020-01-27 15:05:41.398761][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.97395659429344

[2020-01-27 15:05:41.399164][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:41.406490][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:05:44.837267][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.04050099 -0.         ...  0.05146143 -0.14538843
  0.093927  ]

[2020-01-27 15:05:44.837650][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:45.231104][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.16244319  0.         ... -0.03435466  0.11579722
 -0.08144257], shape=(4547,), dtype=float64)

[2020-01-27 15:05:45.301218][__main__.TRPOAgent.log][linesearch]: improvement: -0.08189327221290255

[2020-01-27 15:05:45.301645][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.2171739546078255

[2020-01-27 15:05:46.031295][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0867057157427

[2020-01-27 15:05:46.031695][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:46.038811][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:05:49.447997][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.24272992 -0.         ... -0.05130549  0.32972173
 -0.27841624]

[2020-01-27 15:05:49.448388][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:49.841524][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.15921663  0.         ... -0.0649087   0.14198524
 -0.07707654], shape=(4547,), dtype=float64)

[2020-01-27 15:05:49.912171][__main__.TRPOAgent.log][linesearch]: improvement: -0.1375270325113402

[2020-01-27 15:05:49.912615][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.3859921667052734

[2020-01-27 15:05:50.618800][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9807018971588

[2020-01-27 15:05:50.619190][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:50.628792][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:05:54.048135][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.17197305 -0.         ... -0.0211938  -0.08966742
  0.11086123]

[2020-01-27 15:05:54.048524][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:05:54.424239][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.14467607  0.         ...  0.62699958  0.16722528
 -0.79422487], shape=(4547,), dtype=float64)

[2020-01-27 15:05:54.495626][__main__.TRPOAgent.log][linesearch]: improvement: 0.18285658891402415

[2020-01-27 15:05:54.516266][__main__.TRPOAgent.log][linesearch]: improvement: 0.03405363082090743

[2020-01-27 15:05:54.516729][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.08804484878551254

[2020-01-27 15:05:55.245305][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.11377367601705

[2020-01-27 15:05:55.245722][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:05:55.266254][__main__.TRPOAgent.log][learning]: Episode #25

[2020-01-27 15:05:55.266754][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:05:55.311675][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:05:55.312197][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:05:55.312820][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:05:55.314721][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:05:58.588543][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:05:58.685071][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:05:58.686324][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:05:58.687017][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:05:58.686943][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:05:58.688188][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:06:01.968317][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:06:01.987630][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:06:01.988570][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:06:01.989248][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:06:01.989364][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:06:01.992797][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:06:05.214292][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:06:05.241802][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:06:05.242372][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:06:05.243454][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:06:05.243305][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:06:05.244286][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:06:08.469907][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:06:08.508947][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:06:08.509645][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:06:08.510440][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:06:08.510517][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:06:08.512847][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:06:11.777805][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:06:11.789377][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:06:11.790014][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:06:11.790508][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:06:11.790577][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:06:11.792853][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:06:15.006537][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:06:15.027398][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:06:15.028092][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:06:15.028752][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:06:15.028813][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:06:15.030928][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:06:18.243601][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:06:18.271960][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:06:18.272527][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:06:18.273060][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:06:18.273126][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:06:18.275019][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:06:21.514392][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:06:21.555171][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:06:21.555886][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:06:21.556516][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:06:21.556448][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:06:21.557558][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:06:24.823814][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:06:24.847213][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:06:24.848278][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:06:24.848964][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:06:24.849036][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:06:24.851187][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:06:28.069582][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:06:28.088400][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:06:28.088948][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:06:28.089657][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:06:28.089726][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:06:28.093157][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:06:31.279241][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:06:31.328598][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:06:31.329458][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:06:31.330152][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:06:31.330083][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:06:31.331246][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:06:34.616699][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:06:34.622555][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:06:34.623181][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:06:34.623960][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:06:34.624020][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:06:34.630187][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:06:37.948707][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:06:37.975503][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:06:37.976011][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:06:37.977060][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:06:37.977234][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:06:37.978934][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:06:41.264903][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:06:41.268441][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:06:41.268860][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:06:41.269422][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:06:41.269363][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:06:41.270654][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:06:44.451622][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:06:44.520961][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:06:44.521595][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:06:44.532722][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:06:44.942168][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:06:44.968132][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:06:44.969587][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:06:44.970020][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:06:44.998682][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:06:48.385123][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10520254 -0.         ...  0.10870428 -0.06500229
 -0.04370199]

[2020-01-27 15:06:48.385501][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:06:48.774454][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.02740809  0.         ...  0.04703739 -0.02297301
 -0.02406438], shape=(4547,), dtype=float64)

[2020-01-27 15:06:48.855285][__main__.TRPOAgent.log][linesearch]: improvement: -0.11245986001437366

[2020-01-27 15:06:48.878210][__main__.TRPOAgent.log][linesearch]: improvement: -0.06500737974341647

[2020-01-27 15:06:48.901167][__main__.TRPOAgent.log][linesearch]: improvement: -0.03784853557797577

[2020-01-27 15:06:48.924326][__main__.TRPOAgent.log][linesearch]: improvement: -0.022203998989314827

[2020-01-27 15:06:48.946186][__main__.TRPOAgent.log][linesearch]: improvement: -0.013100895997792877

[2020-01-27 15:06:48.968350][__main__.TRPOAgent.log][linesearch]: improvement: -0.007767844094027976

[2020-01-27 15:06:48.990545][__main__.TRPOAgent.log][linesearch]: improvement: -0.004627015595726924

[2020-01-27 15:06:49.012710][__main__.TRPOAgent.log][linesearch]: improvement: -0.002765385857798619

[2020-01-27 15:06:49.035051][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016564954578413005

[2020-01-27 15:06:49.057181][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009927290963737123

[2020-01-27 15:06:49.057623][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.537039756365773e-07, Discarded policy loss value: -0.26024625675942153

[2020-01-27 15:06:49.813035][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.13775598649687

[2020-01-27 15:06:49.818424][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:06:53.297494][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.27290851 -0.         ...  0.1340391  -0.30548118
  0.17144208]

[2020-01-27 15:06:53.297894][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:06:53.695704][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.08330118  0.         ...  0.25208522 -0.37023531
  0.1181501 ], shape=(4547,), dtype=float64)

[2020-01-27 15:06:53.767351][__main__.TRPOAgent.log][linesearch]: improvement: -0.3312149079146382

[2020-01-27 15:06:53.788896][__main__.TRPOAgent.log][linesearch]: improvement: -0.1030764593240096

[2020-01-27 15:06:53.812225][__main__.TRPOAgent.log][linesearch]: improvement: -0.05847248670565265

[2020-01-27 15:06:53.836094][__main__.TRPOAgent.log][linesearch]: improvement: -0.03951522569964598

[2020-01-27 15:06:53.858731][__main__.TRPOAgent.log][linesearch]: improvement: -0.02602895077822319

[2020-01-27 15:06:53.883316][__main__.TRPOAgent.log][linesearch]: improvement: -0.016566151756102043

[2020-01-27 15:06:53.906657][__main__.TRPOAgent.log][linesearch]: improvement: -0.010298483053019988

[2020-01-27 15:06:53.929799][__main__.TRPOAgent.log][linesearch]: improvement: -0.006304624733372399

[2020-01-27 15:06:53.952536][__main__.TRPOAgent.log][linesearch]: improvement: -0.0038282193976467427

[2020-01-27 15:06:53.975309][__main__.TRPOAgent.log][linesearch]: improvement: -0.002314115351442936

[2020-01-27 15:06:53.975747][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.760072963554221e-07, Discarded policy loss value: -0.0857237896401562

[2020-01-27 15:06:54.695459][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.1539313824498

[2020-01-27 15:06:54.700727][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:06:58.180901][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03563436 -0.         ...  0.00408935  0.02465468
 -0.02874403]

[2020-01-27 15:06:58.181313][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:06:58.555749][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.07740289  0.         ...  0.00441541 -0.00774656
  0.00333115], shape=(4547,), dtype=float64)

[2020-01-27 15:06:58.631820][__main__.TRPOAgent.log][linesearch]: improvement: -0.026079448708867248

[2020-01-27 15:06:58.661922][__main__.TRPOAgent.log][linesearch]: improvement: -0.016941608709342637

[2020-01-27 15:06:58.662358][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.36755423483217603

[2020-01-27 15:06:59.381643][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0006080320939

[2020-01-27 15:06:59.381994][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:06:59.388598][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:07:02.836566][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10040788 -0.         ...  0.06502497 -0.01220232
 -0.05282265]

[2020-01-27 15:07:02.836962][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:07:03.217298][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.18334259  0.         ...  0.09213995 -0.11481355
  0.0226736 ], shape=(4547,), dtype=float64)

[2020-01-27 15:07:03.288657][__main__.TRPOAgent.log][linesearch]: improvement: -0.011677846283838842

[2020-01-27 15:07:03.311070][__main__.TRPOAgent.log][linesearch]: improvement: -0.026821101526117976

[2020-01-27 15:07:03.334440][__main__.TRPOAgent.log][linesearch]: improvement: -0.023233986869964914

[2020-01-27 15:07:03.356468][__main__.TRPOAgent.log][linesearch]: improvement: -0.015781858133183453

[2020-01-27 15:07:03.378018][__main__.TRPOAgent.log][linesearch]: improvement: -0.009890017026572728

[2020-01-27 15:07:03.400529][__main__.TRPOAgent.log][linesearch]: improvement: -0.0060666262200130515

[2020-01-27 15:07:03.423053][__main__.TRPOAgent.log][linesearch]: improvement: -0.003678959602277248

[2020-01-27 15:07:03.446708][__main__.TRPOAgent.log][linesearch]: improvement: -0.002221670237020388

[2020-01-27 15:07:03.470794][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013373892961867084

[2020-01-27 15:07:03.492899][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008037737553913751

[2020-01-27 15:07:03.493337][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.925270260731601e-07, Discarded policy loss value: -0.026278515242249747

[2020-01-27 15:07:04.218854][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.04657043512566

[2020-01-27 15:07:04.224044][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:07:07.682515][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.17244491 -0.         ...  0.06614019 -0.18195157
  0.11581138]

[2020-01-27 15:07:07.682914][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:07:08.068030][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.41536128  0.         ... -0.04848311  0.07258132
 -0.02409821], shape=(4547,), dtype=float64)

[2020-01-27 15:07:08.139463][__main__.TRPOAgent.log][linesearch]: improvement: -0.04197779164583457

[2020-01-27 15:07:08.161207][__main__.TRPOAgent.log][linesearch]: improvement: -0.040702009911169325

[2020-01-27 15:07:08.183362][__main__.TRPOAgent.log][linesearch]: improvement: -0.029122731775199605

[2020-01-27 15:07:08.206539][__main__.TRPOAgent.log][linesearch]: improvement: -0.019145198070495903

[2020-01-27 15:07:08.228088][__main__.TRPOAgent.log][linesearch]: improvement: -0.011998509723357226

[2020-01-27 15:07:08.251097][__main__.TRPOAgent.log][linesearch]: improvement: -0.007373035208607895

[2020-01-27 15:07:08.274757][__main__.TRPOAgent.log][linesearch]: improvement: -0.004483349935903491

[2020-01-27 15:07:08.297570][__main__.TRPOAgent.log][linesearch]: improvement: -0.002710122863933878

[2020-01-27 15:07:08.320168][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016318616382668544

[2020-01-27 15:07:08.342698][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009819174658168839

[2020-01-27 15:07:08.343138][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.996680051384229e-07, Discarded policy loss value: -0.08565691636250732

[2020-01-27 15:07:09.071609][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.98686934650226

[2020-01-27 15:07:09.076678][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:07:12.747990][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.17695196 -0.         ...  0.04783874 -0.16122924
  0.1133905 ]

[2020-01-27 15:07:12.748419][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:07:13.483607][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.23838346  0.         ...  0.08993168  0.006647
 -0.09657868], shape=(4547,), dtype=float64)

[2020-01-27 15:07:13.575030][__main__.TRPOAgent.log][linesearch]: improvement: -0.24502665717346928

[2020-01-27 15:07:13.602323][__main__.TRPOAgent.log][linesearch]: improvement: -0.11413540570960223

[2020-01-27 15:07:13.634558][__main__.TRPOAgent.log][linesearch]: improvement: -0.05630986894146173

[2020-01-27 15:07:13.635218][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.30708610733788355

[2020-01-27 15:07:14.447247][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.98939055997704

[2020-01-27 15:07:14.447641][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:07:14.455674][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:07:18.158452][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.17834215 -0.         ... -0.06841395  0.17866341
 -0.11024946]

[2020-01-27 15:07:18.158840][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:07:18.547964][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.23082971  0.         ...  0.03073998 -0.0581476
  0.02740762], shape=(4547,), dtype=float64)

[2020-01-27 15:07:18.631639][__main__.TRPOAgent.log][linesearch]: improvement: -0.10016223755298295

[2020-01-27 15:07:18.663213][__main__.TRPOAgent.log][linesearch]: improvement: -0.06973865766791137

[2020-01-27 15:07:18.690644][__main__.TRPOAgent.log][linesearch]: improvement: -0.04696015531945524

[2020-01-27 15:07:18.716846][__main__.TRPOAgent.log][linesearch]: improvement: -0.03060796178238606

[2020-01-27 15:07:18.747557][__main__.TRPOAgent.log][linesearch]: improvement: -0.01914191964192813

[2020-01-27 15:07:18.775122][__main__.TRPOAgent.log][linesearch]: improvement: -0.011772889883056747

[2020-01-27 15:07:18.800423][__main__.TRPOAgent.log][linesearch]: improvement: -0.007162358045002815

[2020-01-27 15:07:18.830725][__main__.TRPOAgent.log][linesearch]: improvement: -0.004336131199829563

[2020-01-27 15:07:18.857639][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026182211440682335

[2020-01-27 15:07:18.885207][__main__.TRPOAgent.log][linesearch]: improvement: -0.00157650757941602

[2020-01-27 15:07:18.885713][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.676022449984177e-07, Discarded policy loss value: -0.3390244656838719

[2020-01-27 15:07:19.580786][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.88932397051144

[2020-01-27 15:07:19.585687][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:07:23.570950][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.26183762 -0.         ...  0.06954603 -0.25236401
  0.18281798]

[2020-01-27 15:07:23.571337][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:07:24.002038][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.0226167   0.         ... -0.08320613  0.10312211
 -0.01991599], shape=(4547,), dtype=float64)

[2020-01-27 15:07:24.092685][__main__.TRPOAgent.log][linesearch]: improvement: -0.06641281388238313

[2020-01-27 15:07:24.120890][__main__.TRPOAgent.log][linesearch]: improvement: -0.04504487510385197

[2020-01-27 15:07:24.149002][__main__.TRPOAgent.log][linesearch]: improvement: -0.02893077564429203

[2020-01-27 15:07:24.179630][__main__.TRPOAgent.log][linesearch]: improvement: -0.018033961358247363

[2020-01-27 15:07:24.206353][__main__.TRPOAgent.log][linesearch]: improvement: -0.011106944492259407

[2020-01-27 15:07:24.235564][__main__.TRPOAgent.log][linesearch]: improvement: -0.006809978654445659

[2020-01-27 15:07:24.266077][__main__.TRPOAgent.log][linesearch]: improvement: -0.004163256403231619

[2020-01-27 15:07:24.293112][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025091970391007967

[2020-01-27 15:07:24.320235][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015095640847828662

[2020-01-27 15:07:24.351146][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009071945704011708

[2020-01-27 15:07:24.351621][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.553031436266274e-07, Discarded policy loss value: -0.3417410577812805

[2020-01-27 15:07:25.173514][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.99429942008555

[2020-01-27 15:07:25.182050][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:07:29.785649][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.21891582 -0.         ... -0.0002846   0.1138465
 -0.1135619 ]

[2020-01-27 15:07:29.786038][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:07:30.208931][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.47053459  0.         ... -0.16911217  0.16678453
  0.00232764], shape=(4547,), dtype=float64)

[2020-01-27 15:07:30.290340][__main__.TRPOAgent.log][linesearch]: improvement: -0.06494425460410097

[2020-01-27 15:07:30.319626][__main__.TRPOAgent.log][linesearch]: improvement: -0.04884636394391584

[2020-01-27 15:07:30.348995][__main__.TRPOAgent.log][linesearch]: improvement: -0.03623442528582485

[2020-01-27 15:07:30.377921][__main__.TRPOAgent.log][linesearch]: improvement: -0.023532863300150197

[2020-01-27 15:07:30.403738][__main__.TRPOAgent.log][linesearch]: improvement: -0.014694092613467047

[2020-01-27 15:07:30.434110][__main__.TRPOAgent.log][linesearch]: improvement: -0.008654329620817136

[2020-01-27 15:07:30.461433][__main__.TRPOAgent.log][linesearch]: improvement: -0.005053038153672695

[2020-01-27 15:07:30.488171][__main__.TRPOAgent.log][linesearch]: improvement: -0.0029224050870978457

[2020-01-27 15:07:30.517092][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017408205583673841

[2020-01-27 15:07:30.546110][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010383077046705602

[2020-01-27 15:07:30.546608][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.453041544526179e-07, Discarded policy loss value: -0.2843432053282971

[2020-01-27 15:07:31.302464][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8869810348623

[2020-01-27 15:07:31.310215][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:07:34.735058][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.06287359 -0.         ... -0.04574183  0.04732869
 -0.00158685]

[2020-01-27 15:07:34.735456][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:07:35.130515][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.0244466   0.         ... -0.06103233  0.00739927
  0.05363307], shape=(4547,), dtype=float64)

[2020-01-27 15:07:35.216754][__main__.TRPOAgent.log][linesearch]: improvement: 0.1739394375818562

[2020-01-27 15:07:35.245868][__main__.TRPOAgent.log][linesearch]: improvement: 0.01484355283047746

[2020-01-27 15:07:35.273300][__main__.TRPOAgent.log][linesearch]: improvement: -0.015695892764179806

[2020-01-27 15:07:35.302636][__main__.TRPOAgent.log][linesearch]: improvement: -0.014690908363642133

[2020-01-27 15:07:35.331820][__main__.TRPOAgent.log][linesearch]: improvement: -0.009955977967055635

[2020-01-27 15:07:35.360314][__main__.TRPOAgent.log][linesearch]: improvement: -0.006279511384625333

[2020-01-27 15:07:35.385559][__main__.TRPOAgent.log][linesearch]: improvement: -0.0038521498716163477

[2020-01-27 15:07:35.416044][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023306326607653305

[2020-01-27 15:07:35.445353][__main__.TRPOAgent.log][linesearch]: improvement: -0.0014089463880233438

[2020-01-27 15:07:35.472927][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008492033625186884

[2020-01-27 15:07:35.473455][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.60242634918517e-07, Discarded policy loss value: -0.12380053546331005

[2020-01-27 15:07:36.179754][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.95773549116825

[2020-01-27 15:07:36.196372][__main__.TRPOAgent.log][learning]: Episode #26

[2020-01-27 15:07:36.196727][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:07:36.244815][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:07:36.246929][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:07:36.247910][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:07:36.245395][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:07:39.842071][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:07:39.940582][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:07:39.941193][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:07:39.941886][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:07:39.941938][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:07:39.943974][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:07:43.188594][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:07:43.231931][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:07:43.232535][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:07:43.233406][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:07:43.234304][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:07:43.233339][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:07:46.645384][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:07:46.682013][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:07:46.682852][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:07:46.683623][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:07:46.683722][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:07:46.686179][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:07:50.963495][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:07:50.997187][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:07:50.998451][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:07:50.999547][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:07:50.999452][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:07:51.001516][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:07:55.344158][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:07:55.349875][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:07:55.350931][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:07:55.351888][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:07:55.351971][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:07:55.357180][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:07:59.379047][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:07:59.410335][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:07:59.410948][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:07:59.411531][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:07:59.411675][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:07:59.413827][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:08:02.863716][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:08:02.884046][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:08:02.885168][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:08:02.885899][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:08:02.885993][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:08:02.888829][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:08:06.322932][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:08:06.459844][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:08:06.460571][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:08:06.461152][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:08:06.461207][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:08:06.463421][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:08:09.867565][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:08:09.880405][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:08:09.881289][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:08:09.882472][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:08:09.882696][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:08:09.885021][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:08:13.217733][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:08:13.229280][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:08:13.229987][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:08:13.230769][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:08:13.230687][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:08:13.231799][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:08:16.666094][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:08:16.715383][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:08:16.716076][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:08:16.716617][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:08:16.716682][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:08:16.718609][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:08:20.048071][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:08:20.126784][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:08:20.127407][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:08:20.128000][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:08:20.128067][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:08:20.130809][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:08:23.791142][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:08:23.794382][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:08:23.794875][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:08:23.795394][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:08:23.795494][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:08:23.797880][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:08:27.070932][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:08:27.083114][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:08:27.083719][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:08:27.084266][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:08:27.084212][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:08:27.085077][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:08:30.344186][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:08:30.417869][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:08:30.418387][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:08:30.431289][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:08:30.954024][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:08:30.985467][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:08:30.986955][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:08:30.987383][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:08:31.019008][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:08:34.531209][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.15068687 -0.         ... -0.0900681   0.01400755
  0.07606055]

[2020-01-27 15:08:34.531590][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:08:34.929626][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.07321998  0.         ...  0.01188145  0.13661264
 -0.14849409], shape=(4547,), dtype=float64)

[2020-01-27 15:08:35.014667][__main__.TRPOAgent.log][linesearch]: improvement: -0.1989281559864008

[2020-01-27 15:08:35.041985][__main__.TRPOAgent.log][linesearch]: improvement: -0.13331271063853153

[2020-01-27 15:08:35.067838][__main__.TRPOAgent.log][linesearch]: improvement: -0.07568247612767742

[2020-01-27 15:08:35.097587][__main__.TRPOAgent.log][linesearch]: improvement: -0.04210170578807004

[2020-01-27 15:08:35.127994][__main__.TRPOAgent.log][linesearch]: improvement: -0.023922885614374723

[2020-01-27 15:08:35.155509][__main__.TRPOAgent.log][linesearch]: improvement: -0.013821898171001545

[2020-01-27 15:08:35.183961][__main__.TRPOAgent.log][linesearch]: improvement: -0.008097329483603755

[2020-01-27 15:08:35.213856][__main__.TRPOAgent.log][linesearch]: improvement: -0.004768175598499513

[2020-01-27 15:08:35.243001][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028254057546484024

[2020-01-27 15:08:35.269942][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016846454041405945

[2020-01-27 15:08:35.270461][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.893170655499923e-07, Discarded policy loss value: -0.38003043556598926

[2020-01-27 15:08:36.056955][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0781411813071

[2020-01-27 15:08:36.062795][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:08:39.572319][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.13350448 -0.         ...  0.03688474 -0.12403055
  0.08714581]

[2020-01-27 15:08:39.572703][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:08:39.961512][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.03746741  0.         ... -0.10815479  0.16585554
 -0.05770075], shape=(4547,), dtype=float64)

[2020-01-27 15:08:40.045783][__main__.TRPOAgent.log][linesearch]: improvement: -0.07141676873727032

[2020-01-27 15:08:40.046283][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.3730612689111786

[2020-01-27 15:08:40.779709][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.97783058408464

[2020-01-27 15:08:40.780111][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:08:40.796263][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:08:44.280598][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.29016295 -0.         ... -0.06876614  0.28816534
 -0.2193992 ]

[2020-01-27 15:08:44.280989][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:08:44.670309][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.19718722  0.         ... -0.08355475  0.10210327
 -0.01854852], shape=(4547,), dtype=float64)

[2020-01-27 15:08:44.751531][__main__.TRPOAgent.log][linesearch]: improvement: -0.149155991797009

[2020-01-27 15:08:44.781014][__main__.TRPOAgent.log][linesearch]: improvement: -0.08150909215296975

[2020-01-27 15:08:44.781511][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.36846821750510894

[2020-01-27 15:08:45.534084][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.81530426990514

[2020-01-27 15:08:45.534498][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:08:45.542148][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:08:49.020655][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.15478041 -0.         ...  0.07893773 -0.20873878
  0.12980105]

[2020-01-27 15:08:49.021048][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:08:49.408062][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.11364958  0.         ...  0.04512029 -0.0176175
 -0.0275028 ], shape=(4547,), dtype=float64)

[2020-01-27 15:08:49.492407][__main__.TRPOAgent.log][linesearch]: improvement: -0.19570950331556564

[2020-01-27 15:08:49.522827][__main__.TRPOAgent.log][linesearch]: improvement: -0.10303957089308075

[2020-01-27 15:08:49.523281][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.5682392962368912

[2020-01-27 15:08:50.273194][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.96227216517156

[2020-01-27 15:08:50.273612][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:08:50.286029][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:08:53.735787][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.09218105 -0.         ... -0.11131735  0.21483333
 -0.10351598]

[2020-01-27 15:08:53.736178][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:08:54.127180][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.12889703  0.         ... -0.04574207  0.02928508
  0.01645699], shape=(4547,), dtype=float64)

[2020-01-27 15:08:54.208176][__main__.TRPOAgent.log][linesearch]: improvement: -0.12857411006033523

[2020-01-27 15:08:54.240570][__main__.TRPOAgent.log][linesearch]: improvement: -0.07888909430197297

[2020-01-27 15:08:54.270616][__main__.TRPOAgent.log][linesearch]: improvement: -0.04714795492823154

[2020-01-27 15:08:54.297947][__main__.TRPOAgent.log][linesearch]: improvement: -0.028105462784714597

[2020-01-27 15:08:54.326210][__main__.TRPOAgent.log][linesearch]: improvement: -0.016819112080315668

[2020-01-27 15:08:54.356397][__main__.TRPOAgent.log][linesearch]: improvement: -0.010071455444659372

[2020-01-27 15:08:54.383966][__main__.TRPOAgent.log][linesearch]: improvement: -0.006034798629350524

[2020-01-27 15:08:54.409821][__main__.TRPOAgent.log][linesearch]: improvement: -0.00361779104959975

[2020-01-27 15:08:54.439040][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021695246475947894

[2020-01-27 15:08:54.470960][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013012926492275145

[2020-01-27 15:08:54.471417][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.563145086643576e-07, Discarded policy loss value: -0.2813437143688304

[2020-01-27 15:08:55.168732][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.96746317778315

[2020-01-27 15:08:55.176397][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:08:58.616829][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.1515323  -0.         ...  0.03599657  0.04662261
 -0.08261917]

[2020-01-27 15:08:58.617273][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:08:59.011852][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.27311873  0.         ... -0.01006647 -0.03264622
  0.04271269], shape=(4547,), dtype=float64)

[2020-01-27 15:08:59.094936][__main__.TRPOAgent.log][linesearch]: improvement: -0.16646634881261604

[2020-01-27 15:08:59.125760][__main__.TRPOAgent.log][linesearch]: improvement: -0.09698131386492118

[2020-01-27 15:08:59.154500][__main__.TRPOAgent.log][linesearch]: improvement: -0.047005571071372765

[2020-01-27 15:08:59.184183][__main__.TRPOAgent.log][linesearch]: improvement: -0.022688153651777176

[2020-01-27 15:08:59.211486][__main__.TRPOAgent.log][linesearch]: improvement: -0.011486277090732744

[2020-01-27 15:08:59.241724][__main__.TRPOAgent.log][linesearch]: improvement: -0.006100411362492286

[2020-01-27 15:08:59.271356][__main__.TRPOAgent.log][linesearch]: improvement: -0.003371161951092272

[2020-01-27 15:08:59.297038][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019182161916549978

[2020-01-27 15:08:59.325625][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011131555685037764

[2020-01-27 15:08:59.356729][__main__.TRPOAgent.log][linesearch]: improvement: -0.0006547656536074475

[2020-01-27 15:08:59.357188][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.19185932994499e-07, Discarded policy loss value: -0.3293462739646671

[2020-01-27 15:09:00.113247][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.1526113221359

[2020-01-27 15:09:00.120848][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:09:03.578563][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.20297231 -0.         ... -0.01471679  0.15866534
 -0.14394856]

[2020-01-27 15:09:03.578968][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:09:03.975428][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.26210768  0.         ...  0.10254185 -0.04283912
 -0.05970272], shape=(4547,), dtype=float64)

[2020-01-27 15:09:04.061383][__main__.TRPOAgent.log][linesearch]: improvement: -2.2054449738880533

[2020-01-27 15:09:04.090959][__main__.TRPOAgent.log][linesearch]: improvement: -0.6524465862351689

[2020-01-27 15:09:04.091402][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.0323774949545679

[2020-01-27 15:09:04.842718][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8923351435084

[2020-01-27 15:09:04.843134][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:09:04.855468][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:09:08.255602][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.201321   -0.         ...  0.11062058  0.0086291
 -0.11924968]

[2020-01-27 15:09:08.256034][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:09:08.651430][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.59356039  0.         ... -0.08441317  0.1270752
 -0.04266203], shape=(4547,), dtype=float64)

[2020-01-27 15:09:08.736447][__main__.TRPOAgent.log][linesearch]: improvement: -0.16793847105923548

[2020-01-27 15:09:08.763802][__main__.TRPOAgent.log][linesearch]: improvement: -0.08464933935302744

[2020-01-27 15:09:08.764253][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.04325859456934409

[2020-01-27 15:09:09.521925][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.99888466735973

[2020-01-27 15:09:09.522349][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:09:09.538535][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:09:12.977395][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.0251255  -0.         ... -0.0126241  -0.00138703
  0.01401114]

[2020-01-27 15:09:12.977790][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:09:13.373719][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.091639    0.         ...  0.0826027  -0.14841625
  0.06581355], shape=(4547,), dtype=float64)

[2020-01-27 15:09:13.454264][__main__.TRPOAgent.log][linesearch]: improvement: 0.00020221212889293128

[2020-01-27 15:09:13.482841][__main__.TRPOAgent.log][linesearch]: improvement: -0.012902031396446134

[2020-01-27 15:09:13.516874][__main__.TRPOAgent.log][linesearch]: improvement: -0.019311364540066683

[2020-01-27 15:09:13.547108][__main__.TRPOAgent.log][linesearch]: improvement: -0.020654374933167963

[2020-01-27 15:09:13.575087][__main__.TRPOAgent.log][linesearch]: improvement: -0.012677252070296158

[2020-01-27 15:09:13.603026][__main__.TRPOAgent.log][linesearch]: improvement: -0.007385723171805775

[2020-01-27 15:09:13.632108][__main__.TRPOAgent.log][linesearch]: improvement: -0.00481565256899838

[2020-01-27 15:09:13.660527][__main__.TRPOAgent.log][linesearch]: improvement: -0.002873326031954171

[2020-01-27 15:09:13.687376][__main__.TRPOAgent.log][linesearch]: improvement: -0.0016894633358618405

[2020-01-27 15:09:13.715329][__main__.TRPOAgent.log][linesearch]: improvement: -0.0010007101939584762

[2020-01-27 15:09:13.715769][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.628156346419152e-07, Discarded policy loss value: -0.16246086046358424

[2020-01-27 15:09:14.466711][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9878738324292

[2020-01-27 15:09:14.471886][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:09:17.933549][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03222323 -0.         ... -0.02216222  0.00301205
  0.01915017]

[2020-01-27 15:09:17.933936][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:09:18.325343][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.07724493  0.         ... -0.06433487  0.10711109
 -0.04277622], shape=(4547,), dtype=float64)

[2020-01-27 15:09:18.407824][__main__.TRPOAgent.log][linesearch]: improvement: -0.0346361045742635

[2020-01-27 15:09:18.433669][__main__.TRPOAgent.log][linesearch]: improvement: -0.02599253920269115

[2020-01-27 15:09:18.460473][__main__.TRPOAgent.log][linesearch]: improvement: -0.015434023829758295

[2020-01-27 15:09:18.491960][__main__.TRPOAgent.log][linesearch]: improvement: -0.009265224787503024

[2020-01-27 15:09:18.518331][__main__.TRPOAgent.log][linesearch]: improvement: -0.005741387416229066

[2020-01-27 15:09:18.545563][__main__.TRPOAgent.log][linesearch]: improvement: -0.003520577152060167

[2020-01-27 15:09:18.582290][__main__.TRPOAgent.log][linesearch]: improvement: -0.002174786380086502

[2020-01-27 15:09:18.614006][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013540864225815297

[2020-01-27 15:09:18.645233][__main__.TRPOAgent.log][linesearch]: improvement: -0.000868219217322691

[2020-01-27 15:09:18.671668][__main__.TRPOAgent.log][linesearch]: improvement: -0.0005214114864013353

[2020-01-27 15:09:18.672142][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.182191157984617e-07, Discarded policy loss value: -0.13261949134954293

[2020-01-27 15:09:20.349691][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8729362374065

[2020-01-27 15:09:20.408802][__main__.TRPOAgent.log][learning]: Episode #27

[2020-01-27 15:09:20.410103][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:09:20.571507][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:09:20.573147][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:09:20.574532][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:09:20.585243][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:09:34.498757][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:09:34.658014][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:09:34.660680][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:09:34.662581][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:09:34.662808][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:09:34.670449][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:09:48.622661][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:09:48.712000][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:09:48.714623][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:09:48.716663][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:09:48.717193][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:09:48.723275][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:10:02.623865][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:10:02.664773][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:10:02.667049][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:10:02.669126][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:10:02.668922][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:10:02.673062][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:10:16.203076][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:10:16.606837][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:10:16.609022][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:10:16.611082][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:10:16.610877][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:10:16.615141][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:10:30.248186][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:10:30.571304][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:10:30.572903][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:10:30.574571][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:10:30.574770][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:10:30.581620][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:10:44.447280][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:10:44.603054][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:10:44.606076][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:10:44.609055][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:10:44.608789][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:10:44.612650][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:10:58.456765][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:10:58.602125][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:10:58.604058][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:10:58.606430][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:10:58.606653][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:10:58.618590][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:11:12.856777][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:11:12.972474][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:11:12.974211][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:11:12.976233][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:11:12.976483][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:11:12.987768][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:11:26.837708][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:11:26.930850][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:11:26.933378][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:11:26.935284][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:11:26.935515][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:11:26.941014][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:11:33.667889][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:11:33.691477][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:11:33.692336][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:11:33.693266][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:11:33.693161][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:11:33.694351][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:11:36.971196][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:11:37.028120][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:11:37.028715][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:11:37.029433][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:11:37.029498][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:11:37.032835][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:11:40.256730][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:11:40.333341][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:11:40.333877][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:11:40.334382][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:11:40.334445][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:11:40.336335][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:11:43.692859][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:11:43.717226][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:11:43.717738][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:11:43.718366][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:11:43.718310][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:11:43.719463][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:11:46.942423][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:11:47.001333][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:11:47.001880][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:11:47.002382][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:11:47.002582][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:11:47.005797][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:11:50.257926][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:11:50.342885][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:11:50.343394][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:11:50.356082][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:11:50.883356][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:11:50.913382][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:11:50.915018][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:11:50.915496][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:11:50.947370][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:11:54.395776][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.15249707 -0.         ...  0.08815794  0.01906611
 -0.10722405]

[2020-01-27 15:11:54.396178][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:11:54.792415][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.13032392  0.         ...  0.01069481 -0.10671813
  0.09602332], shape=(4547,), dtype=float64)

[2020-01-27 15:11:54.874929][__main__.TRPOAgent.log][linesearch]: improvement: 0.1281508558986139

[2020-01-27 15:11:54.905852][__main__.TRPOAgent.log][linesearch]: improvement: 0.036945508598806465

[2020-01-27 15:11:54.934238][__main__.TRPOAgent.log][linesearch]: improvement: 0.0016559259376221491

[2020-01-27 15:11:54.934690][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.4249741534205498

[2020-01-27 15:11:55.684506][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.20341695355177

[2020-01-27 15:11:55.684900][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:11:55.692176][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:11:59.122849][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.17306477 -0.         ...  0.05556015  0.15598237
 -0.21154251]

[2020-01-27 15:11:59.123242][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:11:59.514011][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.25566906  0.         ... -0.02683968  0.01897794
  0.00786174], shape=(4547,), dtype=float64)

[2020-01-27 15:11:59.598184][__main__.TRPOAgent.log][linesearch]: improvement: -0.10579132199637307

[2020-01-27 15:11:59.627975][__main__.TRPOAgent.log][linesearch]: improvement: -0.06282213682215944

[2020-01-27 15:11:59.628448][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.1525534671819967

[2020-01-27 15:12:00.384112][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.93975635944486

[2020-01-27 15:12:00.384523][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:12:00.391943][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:12:03.840519][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.09669577 -0.         ... -0.0150364  -0.11934343
  0.13437983]

[2020-01-27 15:12:03.840915][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:04.231159][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.16637131  0.         ... -0.20470492  0.28908674
 -0.08438182], shape=(4547,), dtype=float64)

[2020-01-27 15:12:04.319668][__main__.TRPOAgent.log][linesearch]: improvement: -0.06948913107404991

[2020-01-27 15:12:04.320111][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.0605641518753566

[2020-01-27 15:12:05.072157][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.99196866778425

[2020-01-27 15:12:05.072571][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:12:05.080569][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:12:08.507692][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10881199 -0.         ...  0.07947519  0.23280641
 -0.3122816 ]

[2020-01-27 15:12:08.508081][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:08.899119][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.51893528  0.         ...  0.16608532 -0.18829845
  0.02221313], shape=(4547,), dtype=float64)

[2020-01-27 15:12:09.021023][__main__.TRPOAgent.log][linesearch]: improvement: -0.3034758204244986

[2020-01-27 15:12:09.049196][__main__.TRPOAgent.log][linesearch]: improvement: -0.1511645865168167

[2020-01-27 15:12:09.076334][__main__.TRPOAgent.log][linesearch]: improvement: -0.07634545275421704

[2020-01-27 15:12:09.076797][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.02038912984776693

[2020-01-27 15:12:09.839812][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.94276855915285

[2020-01-27 15:12:09.840228][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:12:09.847878][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:12:13.289147][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00858965 -0.         ...  0.10825344 -0.14511404
  0.0368606 ]

[2020-01-27 15:12:13.289531][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:13.681797][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.73054085  0.         ... -0.09090748  0.15946817
 -0.06856069], shape=(4547,), dtype=float64)

[2020-01-27 15:12:13.763342][__main__.TRPOAgent.log][linesearch]: improvement: -0.042051612740278466

[2020-01-27 15:12:13.793089][__main__.TRPOAgent.log][linesearch]: improvement: -0.039464533492059986

[2020-01-27 15:12:13.793574][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.11925442270438867

[2020-01-27 15:12:14.541828][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.79662224944553

[2020-01-27 15:12:14.542234][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:12:14.552720][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:12:17.992118][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03000661 -0.         ...  0.03943507 -0.25388353
  0.21444846]

[2020-01-27 15:12:17.992508][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:18.382406][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.01126689  0.         ... -0.12440201  0.17904736
 -0.05464534], shape=(4547,), dtype=float64)

[2020-01-27 15:12:18.462701][__main__.TRPOAgent.log][linesearch]: improvement: -0.07904315524658323

[2020-01-27 15:12:18.491870][__main__.TRPOAgent.log][linesearch]: improvement: -0.046463753510352035

[2020-01-27 15:12:18.521498][__main__.TRPOAgent.log][linesearch]: improvement: -0.028346519900776856

[2020-01-27 15:12:18.549980][__main__.TRPOAgent.log][linesearch]: improvement: -0.017369905690457377

[2020-01-27 15:12:18.576989][__main__.TRPOAgent.log][linesearch]: improvement: -0.010534571673533999

[2020-01-27 15:12:18.607789][__main__.TRPOAgent.log][linesearch]: improvement: -0.006323281495937749

[2020-01-27 15:12:18.637703][__main__.TRPOAgent.log][linesearch]: improvement: -0.003807946938235607

[2020-01-27 15:12:18.663978][__main__.TRPOAgent.log][linesearch]: improvement: -0.0022873464651820274

[2020-01-27 15:12:18.691798][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013712921633211028

[2020-01-27 15:12:18.722208][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008227002414467055

[2020-01-27 15:12:18.722657][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.62508067994688e-07, Discarded policy loss value: -0.6101172145072463

[2020-01-27 15:12:19.478245][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.05356339226694

[2020-01-27 15:12:19.483639][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:12:22.894257][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03236285 -0.         ... -0.00365452  0.19590864
 -0.19225412]

[2020-01-27 15:12:22.894640][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:23.290343][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.05583242  0.         ...  0.04366812 -0.08011212
  0.036444  ], shape=(4547,), dtype=float64)

[2020-01-27 15:12:23.372951][__main__.TRPOAgent.log][linesearch]: improvement: -0.03855137367904535

[2020-01-27 15:12:23.373434][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.2152444329037765

[2020-01-27 15:12:24.104449][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0388565717607

[2020-01-27 15:12:24.104853][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:12:24.112329][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:12:27.575334][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00030178 -0.         ...  0.02740682 -0.01051952
 -0.0168873 ]

[2020-01-27 15:12:27.575716][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:27.971351][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.72905519  0.         ...  0.34896744 -0.50254528
  0.15357784], shape=(4547,), dtype=float64)

[2020-01-27 15:12:28.053635][__main__.TRPOAgent.log][linesearch]: improvement: -0.4596059096800297

[2020-01-27 15:12:28.081156][__main__.TRPOAgent.log][linesearch]: improvement: -0.25380704652722996

[2020-01-27 15:12:28.109071][__main__.TRPOAgent.log][linesearch]: improvement: -0.141731313076707

[2020-01-27 15:12:28.136421][__main__.TRPOAgent.log][linesearch]: improvement: -0.07341122607555675

[2020-01-27 15:12:28.163549][__main__.TRPOAgent.log][linesearch]: improvement: -0.0390354654920195

[2020-01-27 15:12:28.193495][__main__.TRPOAgent.log][linesearch]: improvement: -0.021677979488347765

[2020-01-27 15:12:28.220618][__main__.TRPOAgent.log][linesearch]: improvement: -0.01223340056204869

[2020-01-27 15:12:28.247119][__main__.TRPOAgent.log][linesearch]: improvement: -0.006693325328315652

[2020-01-27 15:12:28.277053][__main__.TRPOAgent.log][linesearch]: improvement: -0.003860217725926013

[2020-01-27 15:12:28.306996][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021755843112954976

[2020-01-27 15:12:28.307488][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.651532113175434e-07, Discarded policy loss value: -0.17022820837886543

[2020-01-27 15:12:29.058493][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9125161114401

[2020-01-27 15:12:29.064063][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:12:32.480246][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00181121 -0.         ...  0.04909348  0.2296753
 -0.27876877]

[2020-01-27 15:12:32.480637][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:32.868782][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.40501832  0.         ...  0.09210855 -0.33198615
  0.2398776 ], shape=(4547,), dtype=float64)

[2020-01-27 15:12:32.953945][__main__.TRPOAgent.log][linesearch]: improvement: -0.07116038599930793

[2020-01-27 15:12:32.981171][__main__.TRPOAgent.log][linesearch]: improvement: -0.06847928413047419

[2020-01-27 15:12:33.009824][__main__.TRPOAgent.log][linesearch]: improvement: -0.04379145863387002

[2020-01-27 15:12:33.037856][__main__.TRPOAgent.log][linesearch]: improvement: -0.027784922872298307

[2020-01-27 15:12:33.065731][__main__.TRPOAgent.log][linesearch]: improvement: -0.017201773599240078

[2020-01-27 15:12:33.094462][__main__.TRPOAgent.log][linesearch]: improvement: -0.010050073902973966

[2020-01-27 15:12:33.121558][__main__.TRPOAgent.log][linesearch]: improvement: -0.0057694892618419535

[2020-01-27 15:12:33.148708][__main__.TRPOAgent.log][linesearch]: improvement: -0.0031970724240361903

[2020-01-27 15:12:33.177496][__main__.TRPOAgent.log][linesearch]: improvement: -0.001836837150514442

[2020-01-27 15:12:33.205525][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011027030347542932

[2020-01-27 15:12:33.206017][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.117029789164269e-07, Discarded policy loss value: -0.19784450672259643

[2020-01-27 15:12:33.952949][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.00584493998963

[2020-01-27 15:12:33.958011][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:12:37.353049][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00475146 -0.         ... -0.04352889  0.14836221
 -0.10483332]

[2020-01-27 15:12:37.353434][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:12:37.740487][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.38581914  0.         ... -0.3266729   0.21945901
  0.10721389], shape=(4547,), dtype=float64)

[2020-01-27 15:12:37.819433][__main__.TRPOAgent.log][linesearch]: improvement: -0.07125620439465032

[2020-01-27 15:12:37.846614][__main__.TRPOAgent.log][linesearch]: improvement: -0.09710199269289436

[2020-01-27 15:12:37.875954][__main__.TRPOAgent.log][linesearch]: improvement: -0.06369632858316922

[2020-01-27 15:12:37.901820][__main__.TRPOAgent.log][linesearch]: improvement: -0.03407243117052712

[2020-01-27 15:12:37.902393][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 0.14091105207481713

[2020-01-27 15:12:38.655673][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9434381467682

[2020-01-27 15:12:38.656085][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:12:38.680623][__main__.TRPOAgent.log][learning]: Episode #28

[2020-01-27 15:12:38.681079][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:12:38.728410][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:12:38.729072][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:12:38.729010][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:12:38.729851][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:12:41.998518][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:12:42.053392][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:12:42.054000][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:12:42.054771][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:12:42.054852][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:12:42.057237][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:12:45.413112][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:12:45.456311][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:12:45.457005][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:12:45.457719][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:12:45.457656][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:12:45.459011][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:12:48.777751][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:12:48.813934][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:12:48.814879][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:12:48.815377][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:12:48.815650][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:12:48.818140][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:12:52.378523][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:12:52.423238][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:12:52.424097][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:12:52.424811][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:12:52.424876][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:12:52.427475][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:12:55.757347][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:12:55.791143][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:12:55.792872][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:12:55.793777][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:12:55.793669][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:12:55.794899][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:12:59.045047][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:12:59.163890][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:12:59.164410][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:12:59.165059][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:12:59.165125][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:12:59.167178][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:13:02.485579][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:13:02.495691][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:13:02.496471][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:13:02.497134][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:13:02.497396][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:13:02.500517][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:13:05.833908][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:13:05.857412][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:13:05.857958][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:13:05.858594][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:13:05.858650][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:13:05.862837][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:13:09.155696][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:13:09.226939][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:13:09.227671][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:13:09.228381][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:13:09.228311][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:13:09.229563][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:13:12.563030][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:13:12.581510][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:13:12.582473][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:13:12.583184][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:13:12.583257][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:13:12.587149][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:13:15.972164][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:13:15.984167][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:13:15.984774][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:13:15.985471][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:13:15.985536][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:13:15.988564][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:13:19.236634][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:13:19.323251][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:13:19.323821][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:13:19.324524][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:13:19.324444][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:13:19.325885][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:13:22.636364][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:13:22.673846][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:13:22.674501][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:13:22.675267][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:13:22.675199][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:13:22.676402][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:13:25.981108][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:13:26.038693][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:13:26.039221][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:13:26.039805][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:13:26.039736][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:13:26.041304][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:13:29.386417][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:13:29.421374][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:13:29.421919][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:13:29.433642][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:13:29.908494][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:13:29.935837][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:13:29.937454][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:13:29.937931][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:13:29.967881][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:13:33.364093][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00575826 -0.         ...  0.12080341 -0.08483609
 -0.03596732]

[2020-01-27 15:13:33.364475][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:13:33.758773][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.82007455  0.         ... -0.68066388  0.79100815
 -0.11034427], shape=(4547,), dtype=float64)

[2020-01-27 15:13:33.834217][__main__.TRPOAgent.log][linesearch]: improvement: -0.009095842466555246

[2020-01-27 15:13:33.861678][__main__.TRPOAgent.log][linesearch]: improvement: -0.02688720269771147

[2020-01-27 15:13:33.862130][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.06701979637288685

[2020-01-27 15:13:34.613698][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9851831887105

[2020-01-27 15:13:34.614121][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:13:34.627043][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:13:38.042842][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02207136 -0.         ... -0.11579648  0.01868021
  0.09711627]

[2020-01-27 15:13:38.043237][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:13:38.432368][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.19658684  0.         ... -0.10266728  0.17435346
 -0.07168617], shape=(4547,), dtype=float64)

[2020-01-27 15:13:38.510950][__main__.TRPOAgent.log][linesearch]: improvement: -0.2110062307019323

[2020-01-27 15:13:38.540080][__main__.TRPOAgent.log][linesearch]: improvement: -0.14540831644093244

[2020-01-27 15:13:38.565060][__main__.TRPOAgent.log][linesearch]: improvement: -0.08150939548431435

[2020-01-27 15:13:38.590953][__main__.TRPOAgent.log][linesearch]: improvement: -0.04653554898172052

[2020-01-27 15:13:38.616881][__main__.TRPOAgent.log][linesearch]: improvement: -0.027416625500874925

[2020-01-27 15:13:38.648625][__main__.TRPOAgent.log][linesearch]: improvement: -0.016275162733284054

[2020-01-27 15:13:38.673308][__main__.TRPOAgent.log][linesearch]: improvement: -0.009689005858351474

[2020-01-27 15:13:38.698418][__main__.TRPOAgent.log][linesearch]: improvement: -0.005790835760009938

[2020-01-27 15:13:38.725997][__main__.TRPOAgent.log][linesearch]: improvement: -0.003463935993738343

[2020-01-27 15:13:38.749454][__main__.TRPOAgent.log][linesearch]: improvement: -0.002079465879122444

[2020-01-27 15:13:38.749955][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.378204663800861e-07, Discarded policy loss value: -0.42250741273680886

[2020-01-27 15:13:39.501092][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0868167432779

[2020-01-27 15:13:39.506595][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:13:42.913481][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01570433 -0.         ...  0.20964418 -0.23567748
  0.0260333 ]

[2020-01-27 15:13:42.913876][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:13:43.317864][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.41761167  0.         ...  0.25668229 -0.28880963
  0.03212733], shape=(4547,), dtype=float64)

[2020-01-27 15:13:43.396306][__main__.TRPOAgent.log][linesearch]: improvement: -0.14598791572262215

[2020-01-27 15:13:43.426414][__main__.TRPOAgent.log][linesearch]: improvement: -0.08600318658698235

[2020-01-27 15:13:43.450841][__main__.TRPOAgent.log][linesearch]: improvement: -0.052441091428698405

[2020-01-27 15:13:43.478093][__main__.TRPOAgent.log][linesearch]: improvement: -0.03199643363257848

[2020-01-27 15:13:43.504040][__main__.TRPOAgent.log][linesearch]: improvement: -0.019396073197064753

[2020-01-27 15:13:43.533700][__main__.TRPOAgent.log][linesearch]: improvement: -0.011694891869863777

[2020-01-27 15:13:43.560931][__main__.TRPOAgent.log][linesearch]: improvement: -0.0070054283521551325

[2020-01-27 15:13:43.585596][__main__.TRPOAgent.log][linesearch]: improvement: -0.004207989265191128

[2020-01-27 15:13:43.612964][__main__.TRPOAgent.log][linesearch]: improvement: -0.002530364139767302

[2020-01-27 15:13:43.639753][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015203410358406305

[2020-01-27 15:13:43.640356][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.452700139684221e-07, Discarded policy loss value: -0.10120407717686754

[2020-01-27 15:13:44.390647][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0177738134062

[2020-01-27 15:13:44.396457][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:13:47.825176][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00487542 -0.         ... -0.06274184  0.05172997
  0.01101187]

[2020-01-27 15:13:47.825564][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:13:48.216842][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.67078694  0.         ... -0.39123914  0.71822606
 -0.32698692], shape=(4547,), dtype=float64)

[2020-01-27 15:13:48.296376][__main__.TRPOAgent.log][linesearch]: improvement: 0.06252923438171881

[2020-01-27 15:13:48.324581][__main__.TRPOAgent.log][linesearch]: improvement: -0.007245510175636405

[2020-01-27 15:13:48.349645][__main__.TRPOAgent.log][linesearch]: improvement: -0.013407628101564095

[2020-01-27 15:13:48.376483][__main__.TRPOAgent.log][linesearch]: improvement: -0.013896901286829022

[2020-01-27 15:13:48.376924][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 0.24836969605355969

[2020-01-27 15:13:49.127899][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.90672813980234

[2020-01-27 15:13:49.128319][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:13:49.136154][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:13:52.545680][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02200341 -0.         ... -0.19077236  0.22015006
 -0.02937769]

[2020-01-27 15:13:52.546075][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:13:52.932866][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.68559903  0.         ...  0.06032545 -0.27350239
  0.21317694], shape=(4547,), dtype=float64)

[2020-01-27 15:13:53.010784][__main__.TRPOAgent.log][linesearch]: improvement: -0.011753879986039162

[2020-01-27 15:13:53.036295][__main__.TRPOAgent.log][linesearch]: improvement: -0.04687796956035445

[2020-01-27 15:13:53.063195][__main__.TRPOAgent.log][linesearch]: improvement: -0.04208466570414787

[2020-01-27 15:13:53.089930][__main__.TRPOAgent.log][linesearch]: improvement: -0.030429321203418236

[2020-01-27 15:13:53.121450][__main__.TRPOAgent.log][linesearch]: improvement: -0.020118094464941838

[2020-01-27 15:13:53.154310][__main__.TRPOAgent.log][linesearch]: improvement: -0.012775920939518404

[2020-01-27 15:13:53.177902][__main__.TRPOAgent.log][linesearch]: improvement: -0.00792756419954177

[2020-01-27 15:13:53.206843][__main__.TRPOAgent.log][linesearch]: improvement: -0.004849060372418723

[2020-01-27 15:13:53.233463][__main__.TRPOAgent.log][linesearch]: improvement: -0.002940573500076815

[2020-01-27 15:13:53.260419][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017749494824307321

[2020-01-27 15:13:53.260917][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.887642934294928e-07, Discarded policy loss value: -0.34736340779923475

[2020-01-27 15:13:54.012735][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.00202242653353

[2020-01-27 15:13:54.018157][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:13:57.442774][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.07522826 -0.         ...  0.03175026  0.05796448
 -0.08971474]

[2020-01-27 15:13:57.443154][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:13:57.829616][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.77675369  0.         ... -0.11230132  0.00281994
  0.10948138], shape=(4547,), dtype=float64)

[2020-01-27 15:13:57.910222][__main__.TRPOAgent.log][linesearch]: improvement: -0.1734270569528219

[2020-01-27 15:13:57.937251][__main__.TRPOAgent.log][linesearch]: improvement: -0.10782709962799997

[2020-01-27 15:13:57.963600][__main__.TRPOAgent.log][linesearch]: improvement: -0.06308024422902353

[2020-01-27 15:13:57.987705][__main__.TRPOAgent.log][linesearch]: improvement: -0.03644077456106065

[2020-01-27 15:13:58.016038][__main__.TRPOAgent.log][linesearch]: improvement: -0.022357832173718173

[2020-01-27 15:13:58.041757][__main__.TRPOAgent.log][linesearch]: improvement: -0.013750112210886234

[2020-01-27 15:13:58.068248][__main__.TRPOAgent.log][linesearch]: improvement: -0.008337450562546977

[2020-01-27 15:13:58.096164][__main__.TRPOAgent.log][linesearch]: improvement: -0.0050137190088631045

[2020-01-27 15:13:58.122014][__main__.TRPOAgent.log][linesearch]: improvement: -0.003048771030491948

[2020-01-27 15:13:58.152020][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018822135737579515

[2020-01-27 15:13:58.152493][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.418303566986629e-07, Discarded policy loss value: -0.5012895649006995

[2020-01-27 15:13:58.900231][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.06576598862426

[2020-01-27 15:13:58.905549][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:14:02.299840][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.08403944 -0.         ... -0.01524752  0.22483574
 -0.20958822]

[2020-01-27 15:14:02.300230][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:14:02.690874][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.36768748  0.         ... -0.15249631  0.06210076
  0.09039556], shape=(4547,), dtype=float64)

[2020-01-27 15:14:02.770058][__main__.TRPOAgent.log][linesearch]: improvement: -0.19310984363567435

[2020-01-27 15:14:02.798142][__main__.TRPOAgent.log][linesearch]: improvement: -0.1289735356281701

[2020-01-27 15:14:02.820642][__main__.TRPOAgent.log][linesearch]: improvement: -0.08220593002921112

[2020-01-27 15:14:02.821108][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.4155202294740681

[2020-01-27 15:14:03.576631][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9965964951077

[2020-01-27 15:14:03.577041][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:14:03.584222][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:14:07.068165][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.0347306  -0.         ...  0.06065161 -0.27071562
  0.210064  ]

[2020-01-27 15:14:07.068673][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:14:07.560095][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.21086918  0.         ... -0.16147236  0.29199474
 -0.13052238], shape=(4547,), dtype=float64)

[2020-01-27 15:14:07.646368][__main__.TRPOAgent.log][linesearch]: improvement: -0.19792544233623244

[2020-01-27 15:14:07.671143][__main__.TRPOAgent.log][linesearch]: improvement: -0.11288619619559659

[2020-01-27 15:14:07.696174][__main__.TRPOAgent.log][linesearch]: improvement: -0.06614742755563502

[2020-01-27 15:14:07.722558][__main__.TRPOAgent.log][linesearch]: improvement: -0.039885483225029994

[2020-01-27 15:14:07.750749][__main__.TRPOAgent.log][linesearch]: improvement: -0.024696052812653413

[2020-01-27 15:14:07.773393][__main__.TRPOAgent.log][linesearch]: improvement: -0.015684130156483622

[2020-01-27 15:14:07.800744][__main__.TRPOAgent.log][linesearch]: improvement: -0.010099573469496637

[2020-01-27 15:14:07.828566][__main__.TRPOAgent.log][linesearch]: improvement: -0.005976474278788335

[2020-01-27 15:14:07.851342][__main__.TRPOAgent.log][linesearch]: improvement: -0.003602153744849901

[2020-01-27 15:14:07.881005][__main__.TRPOAgent.log][linesearch]: improvement: -0.002268038998766597

[2020-01-27 15:14:07.881535][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 1.0422208844251577e-06, Discarded policy loss value: -0.3692088102612015

[2020-01-27 15:14:08.762211][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9742344445724

[2020-01-27 15:14:08.768355][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:14:12.617268][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03965349 -0.         ... -0.04274506 -0.12193866
  0.16468373]

[2020-01-27 15:14:12.617661][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:14:12.993744][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.07451562  0.         ...  0.13956411 -0.20840247
  0.06883836], shape=(4547,), dtype=float64)

[2020-01-27 15:14:13.071070][__main__.TRPOAgent.log][linesearch]: improvement: -0.6074616917603335

[2020-01-27 15:14:13.099770][__main__.TRPOAgent.log][linesearch]: improvement: -0.4161874350963667

[2020-01-27 15:14:13.124351][__main__.TRPOAgent.log][linesearch]: improvement: -0.16765739820611109

[2020-01-27 15:14:13.149774][__main__.TRPOAgent.log][linesearch]: improvement: -0.0641181083511294

[2020-01-27 15:14:13.174524][__main__.TRPOAgent.log][linesearch]: improvement: -0.026437934952152617

[2020-01-27 15:14:13.174983][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 4, New policy loss value: 0.12977166763071

[2020-01-27 15:14:13.909367][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.96469193935275

[2020-01-27 15:14:13.909763][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:14:13.919823][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:14:17.586776][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02086418 -0.         ...  0.09958056 -0.07978547
 -0.01979509]

[2020-01-27 15:14:17.587160][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:14:17.972478][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.27487921  0.         ...  0.10214928 -0.12838759
  0.02623832], shape=(4547,), dtype=float64)

[2020-01-27 15:14:18.051816][__main__.TRPOAgent.log][linesearch]: improvement: -0.16977917475461884

[2020-01-27 15:14:18.081395][__main__.TRPOAgent.log][linesearch]: improvement: -0.08597645661738253

[2020-01-27 15:14:18.081879][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.18460424189928806

[2020-01-27 15:14:18.826118][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9678662717836

[2020-01-27 15:14:18.826525][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:14:18.846997][__main__.TRPOAgent.log][learning]: Episode #29

[2020-01-27 15:14:18.847359][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:14:18.894140][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:14:18.895466][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:14:18.894707][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:14:18.896772][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:14:22.464162][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:14:22.508610][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:14:22.509534][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:14:22.510651][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:14:22.510741][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:14:22.512260][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:14:26.119001][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:14:26.131254][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:14:26.132239][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:14:26.132800][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:14:26.132863][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:14:26.136663][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:14:29.411364][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:14:29.414915][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:14:29.415816][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:14:29.416332][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:14:29.416456][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:14:29.418625][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:14:32.678856][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:14:32.731073][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:14:32.731737][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:14:32.732358][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:14:32.732302][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:14:32.733339][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:14:36.002064][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:14:36.073924][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:14:36.074601][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:14:36.075231][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:14:36.075343][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:14:36.079287][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:14:39.311616][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:14:39.356552][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:14:39.357875][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:14:39.358534][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:14:39.358730][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:14:39.362301][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:14:42.674784][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:14:42.719945][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:14:42.720884][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:14:42.721641][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:14:42.721907][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:14:42.724508][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:14:46.165847][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:14:46.224803][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:14:46.225850][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:14:46.226455][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:14:46.226527][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:14:46.230740][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:14:49.564835][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:14:49.601052][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:14:49.601910][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:14:49.602556][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:14:49.602627][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:14:49.604698][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:14:52.797743][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:14:52.875519][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:14:52.876091][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:14:52.876987][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:14:52.877105][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:14:52.880993][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:14:56.198780][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:14:56.235476][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:14:56.236070][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:14:56.236754][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:14:56.236675][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:14:56.239175][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:14:59.535775][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:14:59.576792][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:14:59.577399][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:14:59.578032][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:14:59.578094][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:14:59.580068][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:15:02.813453][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:15:02.841231][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:15:02.842067][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:15:02.842796][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:15:02.842910][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:15:02.846508][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:15:06.098500][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:15:06.173460][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:15:06.174027][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:15:06.174980][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:15:06.175066][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:15:06.177312][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:15:10.337896][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:15:10.395815][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:15:10.396290][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:15:10.408872][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:15:10.893045][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:15:10.923872][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:15:10.925485][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:15:10.925933][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:15:10.955025][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:15:14.422330][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.10727951 -0.         ...  0.09248865 -0.00460828
 -0.08788037]

[2020-01-27 15:15:14.422748][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:14.822511][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.45462733  0.         ...  0.30081877 -0.66093968
  0.36012091], shape=(4547,), dtype=float64)

[2020-01-27 15:15:14.902645][__main__.TRPOAgent.log][linesearch]: improvement: -0.2715821383766775

[2020-01-27 15:15:14.903117][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.21156181092676019

[2020-01-27 15:15:15.660839][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.85833998682506

[2020-01-27 15:15:15.661275][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:15:15.669095][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:15:19.079714][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.08127609 -0.         ... -0.09850974 -0.21585384
  0.31436357]

[2020-01-27 15:15:19.080126][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:19.475498][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.19149565  0.         ...  0.05737502 -0.30022191
  0.24284689], shape=(4547,), dtype=float64)

[2020-01-27 15:15:19.553452][__main__.TRPOAgent.log][linesearch]: improvement: -0.11806796447279864

[2020-01-27 15:15:19.582638][__main__.TRPOAgent.log][linesearch]: improvement: -0.07541552661137657

[2020-01-27 15:15:19.608054][__main__.TRPOAgent.log][linesearch]: improvement: -0.04671036666795447

[2020-01-27 15:15:19.636997][__main__.TRPOAgent.log][linesearch]: improvement: -0.02855463655644233

[2020-01-27 15:15:19.663803][__main__.TRPOAgent.log][linesearch]: improvement: -0.01730956493339586

[2020-01-27 15:15:19.692180][__main__.TRPOAgent.log][linesearch]: improvement: -0.010433353275830698

[2020-01-27 15:15:19.719270][__main__.TRPOAgent.log][linesearch]: improvement: -0.00627541391757605

[2020-01-27 15:15:19.742396][__main__.TRPOAgent.log][linesearch]: improvement: -0.0037712481848534984

[2020-01-27 15:15:19.771934][__main__.TRPOAgent.log][linesearch]: improvement: -0.0022648424765389363

[2020-01-27 15:15:19.799784][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013594743996260816

[2020-01-27 15:15:19.800257][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.319545798266002e-07, Discarded policy loss value: -0.20991178913478728

[2020-01-27 15:15:20.543348][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0903497396224

[2020-01-27 15:15:20.551105][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:15:23.974271][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02942381 -0.         ... -0.0450561   0.10959
 -0.0645339 ]

[2020-01-27 15:15:23.974653][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:24.365900][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.23252134  0.         ...  0.07513918 -0.00236169
 -0.0727775 ], shape=(4547,), dtype=float64)

[2020-01-27 15:15:24.443886][__main__.TRPOAgent.log][linesearch]: improvement: -0.06302598604594581

[2020-01-27 15:15:24.472524][__main__.TRPOAgent.log][linesearch]: improvement: -0.06393361163319444

[2020-01-27 15:15:24.472989][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.1301929946918877

[2020-01-27 15:15:25.212831][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.90015984978805

[2020-01-27 15:15:25.213260][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:15:25.221029][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:15:28.620762][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00273625 -0.         ... -0.03272022  0.07964196
 -0.04692174]

[2020-01-27 15:15:28.621183][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:29.014448][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.03502262  0.         ... -0.02682139  0.07311787
 -0.04629648], shape=(4547,), dtype=float64)

[2020-01-27 15:15:29.092683][__main__.TRPOAgent.log][linesearch]: improvement: -0.11707823289164662

[2020-01-27 15:15:29.093143][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.008317166352703579

[2020-01-27 15:15:29.837999][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9903605124732

[2020-01-27 15:15:29.838405][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:15:29.846098][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:15:33.222000][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.13516007 -0.         ...  0.18501992 -0.10310216
 -0.08191776]

[2020-01-27 15:15:33.222408][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:33.605475][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.04210062  0.         ...  0.0235363   0.05762004
 -0.08115634], shape=(4547,), dtype=float64)

[2020-01-27 15:15:33.687118][__main__.TRPOAgent.log][linesearch]: improvement: -0.13967360539314339

[2020-01-27 15:15:33.711641][__main__.TRPOAgent.log][linesearch]: improvement: -0.07666632524474559

[2020-01-27 15:15:33.739192][__main__.TRPOAgent.log][linesearch]: improvement: -0.043266524001698614

[2020-01-27 15:15:33.762795][__main__.TRPOAgent.log][linesearch]: improvement: -0.025162870590131226

[2020-01-27 15:15:33.790178][__main__.TRPOAgent.log][linesearch]: improvement: -0.01482877023269874

[2020-01-27 15:15:33.813296][__main__.TRPOAgent.log][linesearch]: improvement: -0.008806580217800009

[2020-01-27 15:15:33.840790][__main__.TRPOAgent.log][linesearch]: improvement: -0.005248641593364467

[2020-01-27 15:15:33.867313][__main__.TRPOAgent.log][linesearch]: improvement: -0.0031417748912101473

[2020-01-27 15:15:33.893600][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018833452752436974

[2020-01-27 15:15:33.919185][__main__.TRPOAgent.log][linesearch]: improvement: -0.0011290840238967204

[2020-01-27 15:15:33.919711][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.986865387896361e-07, Discarded policy loss value: -0.4668875258716007

[2020-01-27 15:15:34.667621][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.1068883436586

[2020-01-27 15:15:34.672920][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:15:38.133625][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00321444 -0.         ...  0.01335309  0.33341925
 -0.34677235]

[2020-01-27 15:15:38.134013][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:38.520667][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.00000000e+00  4.20064184e-01  0.00000000e+00 ...  1.89191895e-04
  4.08756455e-01 -4.08945647e-01], shape=(4547,), dtype=float64)

[2020-01-27 15:15:38.597138][__main__.TRPOAgent.log][linesearch]: improvement: -0.12193612562534123

[2020-01-27 15:15:38.626695][__main__.TRPOAgent.log][linesearch]: improvement: -0.09238075771427046

[2020-01-27 15:15:38.627225][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.31074485896401893

[2020-01-27 15:15:39.364936][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9520473905657

[2020-01-27 15:15:39.365354][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:15:39.372559][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:15:42.833495][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.03525155 -0.         ... -0.03084044  0.23215267
 -0.20131223]

[2020-01-27 15:15:42.833895][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:43.220766][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.00923077  0.         ... -0.02213691 -0.11683819
  0.1389751 ], shape=(4547,), dtype=float64)

[2020-01-27 15:15:43.297894][__main__.TRPOAgent.log][linesearch]: improvement: -0.10113653616726899

[2020-01-27 15:15:43.298340][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.0735117046161328

[2020-01-27 15:15:44.025797][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.84486600573086

[2020-01-27 15:15:44.026201][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:15:44.038177][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:15:47.473397][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.10508418 -0.         ... -0.07575237  0.18749843
 -0.11174606]

[2020-01-27 15:15:47.473776][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:47.857444][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.8634868   0.         ...  0.13967813 -0.08217569
 -0.05750244], shape=(4547,), dtype=float64)

[2020-01-27 15:15:47.934785][__main__.TRPOAgent.log][linesearch]: improvement: -0.052550575919594056

[2020-01-27 15:15:47.962892][__main__.TRPOAgent.log][linesearch]: improvement: -0.04607331503257611

[2020-01-27 15:15:47.990447][__main__.TRPOAgent.log][linesearch]: improvement: -0.04277996326121908

[2020-01-27 15:15:48.013668][__main__.TRPOAgent.log][linesearch]: improvement: -0.030491247480970984

[2020-01-27 15:15:48.042400][__main__.TRPOAgent.log][linesearch]: improvement: -0.01919520285841292

[2020-01-27 15:15:48.067760][__main__.TRPOAgent.log][linesearch]: improvement: -0.01146546893439726

[2020-01-27 15:15:48.094997][__main__.TRPOAgent.log][linesearch]: improvement: -0.007068295521431844

[2020-01-27 15:15:48.120411][__main__.TRPOAgent.log][linesearch]: improvement: -0.004336684774722266

[2020-01-27 15:15:48.148969][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026357688402354196

[2020-01-27 15:15:48.174062][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015995443619070127

[2020-01-27 15:15:48.174562][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.715924768191804e-07, Discarded policy loss value: -0.5273183228239692

[2020-01-27 15:15:48.915841][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.01711132335106

[2020-01-27 15:15:48.921164][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:15:52.343392][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.09381086 -0.         ...  0.15253035 -0.38773723
  0.23520688]

[2020-01-27 15:15:52.343770][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:52.726104][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.66833531  0.         ...  0.12654827  0.45292949
 -0.57947776], shape=(4547,), dtype=float64)

[2020-01-27 15:15:52.810546][__main__.TRPOAgent.log][linesearch]: improvement: -0.21826870506189333

[2020-01-27 15:15:52.810986][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.2699694378086044

[2020-01-27 15:15:53.534681][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.97687544482926

[2020-01-27 15:15:53.535099][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:15:53.548489][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:15:56.972749][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01122234 -0.         ... -0.0001401  -0.0143392
  0.0144793 ]

[2020-01-27 15:15:56.973200][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:15:57.359480][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.04556169  0.         ...  0.15670505  0.03462106
 -0.19132611], shape=(4547,), dtype=float64)

[2020-01-27 15:15:57.436488][__main__.TRPOAgent.log][linesearch]: improvement: -0.10553486889724305

[2020-01-27 15:15:57.463942][__main__.TRPOAgent.log][linesearch]: improvement: -0.05107034486455915

[2020-01-27 15:15:57.488548][__main__.TRPOAgent.log][linesearch]: improvement: -0.026448129902604714

[2020-01-27 15:15:57.489022][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.09378956751580275

[2020-01-27 15:15:58.220596][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9175158171117

[2020-01-27 15:15:58.221010][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:15:58.244657][__main__.TRPOAgent.log][learning]: Episode #30

[2020-01-27 15:15:58.245188][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:15:58.296970][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:15:58.297546][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:15:58.298228][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:15:58.300594][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:16:01.702876][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:16:01.748832][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:16:01.749364][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:16:01.749844][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:16:01.749918][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:16:01.751720][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:16:05.018425][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:16:05.085028][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:16:05.086012][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:16:05.087006][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:16:05.086925][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:16:05.088013][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:16:08.331412][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:16:08.411584][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:16:08.412125][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:16:08.412615][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:16:08.412668][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:16:08.414897][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:16:11.805963][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:16:11.810732][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:16:11.811292][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:16:11.811875][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:16:11.811812][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:16:11.812882][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:16:15.187868][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:16:15.196231][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:16:15.196846][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:16:15.197459][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:16:15.197522][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:16:15.198978][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:16:18.454743][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:16:18.510697][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:16:18.511545][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:16:18.512150][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:16:18.512209][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:16:18.514117][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:16:21.830617][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:16:21.856794][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:16:21.857531][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:16:21.858115][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:16:21.858171][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:16:21.859831][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:16:25.158240][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:16:25.159347][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:16:25.160523][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:16:25.161198][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:16:25.161133][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:16:25.161903][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:16:28.404024][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:16:28.446505][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:16:28.447154][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:16:28.447640][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:16:28.447701][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:16:28.450174][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:16:31.740755][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:16:31.773329][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:16:31.774185][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:16:31.774794][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:16:31.774862][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:16:31.776917][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:16:35.037879][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:16:35.063166][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:16:35.064201][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:16:35.065036][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:16:35.065168][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:16:35.067419][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:16:38.294459][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:16:38.339956][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:16:38.340471][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:16:38.340948][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:16:38.341013][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:16:38.343333][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:16:41.611908][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:16:41.665169][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:16:41.665666][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:16:41.666207][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:16:41.666377][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:16:41.668825][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:16:44.885218][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:16:44.940558][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:16:44.941084][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:16:44.941690][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:16:44.941752][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:16:44.945785][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:16:48.225727][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:16:48.261006][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:16:48.261540][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:16:48.274192][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:16:48.753336][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:16:48.780901][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:16:48.782365][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:16:48.782790][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:16:48.812790][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:16:52.253631][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01787567 -0.         ...  0.01122623 -0.1369311
  0.12570487]

[2020-01-27 15:16:52.254019][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:16:52.637804][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.06820152  0.         ...  0.01869243 -0.03276798
  0.01407555], shape=(4547,), dtype=float64)

[2020-01-27 15:16:52.713245][__main__.TRPOAgent.log][linesearch]: improvement: -0.06842603201186825

[2020-01-27 15:16:52.743476][__main__.TRPOAgent.log][linesearch]: improvement: -0.04961029056554511

[2020-01-27 15:16:52.743933][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.40551822819717076

[2020-01-27 15:16:53.573636][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9307441535824

[2020-01-27 15:16:53.574070][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:16:53.583580][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:16:57.737313][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.03246565 -0.         ... -0.17349828 -0.06148063
  0.23497891]

[2020-01-27 15:16:57.737744][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:16:58.146522][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.12732937  0.         ...  0.0432622  -0.00657514
 -0.03668706], shape=(4547,), dtype=float64)

[2020-01-27 15:16:58.228222][__main__.TRPOAgent.log][linesearch]: improvement: -0.09031999871835417

[2020-01-27 15:16:58.256175][__main__.TRPOAgent.log][linesearch]: improvement: -0.05579207040710049

[2020-01-27 15:16:58.282404][__main__.TRPOAgent.log][linesearch]: improvement: -0.03396199370611985

[2020-01-27 15:16:58.310537][__main__.TRPOAgent.log][linesearch]: improvement: -0.020346714205026928

[2020-01-27 15:16:58.335100][__main__.TRPOAgent.log][linesearch]: improvement: -0.012185335796502272

[2020-01-27 15:16:58.362594][__main__.TRPOAgent.log][linesearch]: improvement: -0.0072831294284699255

[2020-01-27 15:16:58.385514][__main__.TRPOAgent.log][linesearch]: improvement: -0.004369048345326587

[2020-01-27 15:16:58.412438][__main__.TRPOAgent.log][linesearch]: improvement: -0.0026233412519628407

[2020-01-27 15:16:58.439394][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015731708874570094

[2020-01-27 15:16:58.463753][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009441985208460402

[2020-01-27 15:16:58.464199][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.718902516619008e-07, Discarded policy loss value: -0.5045023337266317

[2020-01-27 15:16:59.298486][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.04725925806673

[2020-01-27 15:16:59.304434][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:17:03.701901][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00418939 -0.         ...  0.09089026 -0.18062714
  0.08973688]

[2020-01-27 15:17:03.707792][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:04.176278][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -1.65083651  0.         ... -0.30021045 -0.31218499
  0.61239544], shape=(4547,), dtype=float64)

[2020-01-27 15:17:04.262117][__main__.TRPOAgent.log][linesearch]: improvement: -0.06654813142894092

[2020-01-27 15:17:04.291795][__main__.TRPOAgent.log][linesearch]: improvement: -0.0463917694300237

[2020-01-27 15:17:04.320195][__main__.TRPOAgent.log][linesearch]: improvement: -0.030516422760637513

[2020-01-27 15:17:04.349439][__main__.TRPOAgent.log][linesearch]: improvement: -0.019127823527724117

[2020-01-27 15:17:04.377953][__main__.TRPOAgent.log][linesearch]: improvement: -0.011751354537677908

[2020-01-27 15:17:04.404665][__main__.TRPOAgent.log][linesearch]: improvement: -0.007065467644082474

[2020-01-27 15:17:04.432604][__main__.TRPOAgent.log][linesearch]: improvement: -0.00422055768292276

[2020-01-27 15:17:04.461501][__main__.TRPOAgent.log][linesearch]: improvement: -0.002526543688748384

[2020-01-27 15:17:04.487005][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015160244652850374

[2020-01-27 15:17:04.516391][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009097486938673127

[2020-01-27 15:17:04.516835][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.663828353111295e-07, Discarded policy loss value: -0.13137741568790604

[2020-01-27 15:17:05.356749][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.99808717866034

[2020-01-27 15:17:05.364810][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:17:09.406382][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00624316 -0.         ... -0.08314819  0.07840831
  0.00473988]

[2020-01-27 15:17:09.406798][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:09.788734][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.12288571  0.         ...  0.01765199  0.02311632
 -0.04076831], shape=(4547,), dtype=float64)

[2020-01-27 15:17:09.868247][__main__.TRPOAgent.log][linesearch]: improvement: -0.09869042872758635

[2020-01-27 15:17:09.897554][__main__.TRPOAgent.log][linesearch]: improvement: -0.07471967030275926

[2020-01-27 15:17:09.898013][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.042530357257015566

[2020-01-27 15:17:10.620658][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.04814504439844

[2020-01-27 15:17:10.621071][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:17:10.633691][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:17:14.132057][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01421753 -0.         ...  0.0523203  -0.20035154
  0.14803124]

[2020-01-27 15:17:14.132472][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:14.513100][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.32392661  0.         ... -0.14012638 -0.02177019
  0.16189658], shape=(4547,), dtype=float64)

[2020-01-27 15:17:14.591613][__main__.TRPOAgent.log][linesearch]: improvement: -0.07993399011839554

[2020-01-27 15:17:14.618159][__main__.TRPOAgent.log][linesearch]: improvement: -0.04773533164497348

[2020-01-27 15:17:14.618793][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.12311516591999376

[2020-01-27 15:17:15.352522][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9775194125924

[2020-01-27 15:17:15.352919][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:17:15.366783][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:17:19.047578][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.05460575 -0.         ...  0.17308839 -0.11584561
 -0.05724279]

[2020-01-27 15:17:19.047958][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:19.430067][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.55373649  0.         ...  0.36500697  0.43809905
 -0.80310602], shape=(4547,), dtype=float64)

[2020-01-27 15:17:19.505599][__main__.TRPOAgent.log][linesearch]: improvement: -0.18735336948095271

[2020-01-27 15:17:19.532900][__main__.TRPOAgent.log][linesearch]: improvement: -0.1065512168518067

[2020-01-27 15:17:19.557268][__main__.TRPOAgent.log][linesearch]: improvement: -0.058278121837069194

[2020-01-27 15:17:19.557811][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.1522129632319878

[2020-01-27 15:17:20.289782][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9547349467145

[2020-01-27 15:17:20.290182][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:17:20.299235][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:17:23.764241][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00905061 -0.         ...  0.01767976 -0.06931053
  0.05163077]

[2020-01-27 15:17:23.764639][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:24.144400][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.36540149  0.         ...  0.02971411 -0.12625743
  0.09654332], shape=(4547,), dtype=float64)

[2020-01-27 15:17:24.221515][__main__.TRPOAgent.log][linesearch]: improvement: -0.055955854896446

[2020-01-27 15:17:24.250097][__main__.TRPOAgent.log][linesearch]: improvement: -0.023762133351488357

[2020-01-27 15:17:24.272902][__main__.TRPOAgent.log][linesearch]: improvement: -0.014704495822962738

[2020-01-27 15:17:24.273356][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.3231170422581064

[2020-01-27 15:17:25.019989][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.83860933992366

[2020-01-27 15:17:25.020403][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:17:25.032860][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:17:28.463451][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00714577 -0.         ... -0.22905045  0.11203786
  0.11701259]

[2020-01-27 15:17:28.463839][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:28.852591][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.97328615  0.         ...  0.15626255  0.71788025
 -0.8741428 ], shape=(4547,), dtype=float64)

[2020-01-27 15:17:28.925867][__main__.TRPOAgent.log][linesearch]: improvement: -0.19682791734637317

[2020-01-27 15:17:28.952148][__main__.TRPOAgent.log][linesearch]: improvement: -0.11281498097360276

[2020-01-27 15:17:28.952593][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.45837307273137007

[2020-01-27 15:17:29.683741][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9007589621966

[2020-01-27 15:17:29.684144][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:17:29.691468][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:17:33.431089][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01217782 -0.         ... -0.06121457 -0.06221418
  0.12342875]

[2020-01-27 15:17:33.431603][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:33.826933][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.24607162  0.         ...  0.1025645   0.0110362
 -0.1136007 ], shape=(4547,), dtype=float64)

[2020-01-27 15:17:33.903412][__main__.TRPOAgent.log][linesearch]: improvement: -0.08898890483861718

[2020-01-27 15:17:33.931865][__main__.TRPOAgent.log][linesearch]: improvement: -0.06190509658447768

[2020-01-27 15:17:33.956497][__main__.TRPOAgent.log][linesearch]: improvement: -0.04086445999570687

[2020-01-27 15:17:33.982995][__main__.TRPOAgent.log][linesearch]: improvement: -0.026246670987586762

[2020-01-27 15:17:34.008362][__main__.TRPOAgent.log][linesearch]: improvement: -0.015805817925352017

[2020-01-27 15:17:34.037071][__main__.TRPOAgent.log][linesearch]: improvement: -0.009644493980337454

[2020-01-27 15:17:34.068454][__main__.TRPOAgent.log][linesearch]: improvement: -0.005925897967668181

[2020-01-27 15:17:34.093063][__main__.TRPOAgent.log][linesearch]: improvement: -0.0035816967401692823

[2020-01-27 15:17:34.119444][__main__.TRPOAgent.log][linesearch]: improvement: -0.0021495152211066237

[2020-01-27 15:17:34.146583][__main__.TRPOAgent.log][linesearch]: improvement: -0.0012857256119574445

[2020-01-27 15:17:34.147079][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.928739988536147e-07, Discarded policy loss value: -0.29710929754313437

[2020-01-27 15:17:34.954916][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8430617062911

[2020-01-27 15:17:34.960179][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:17:38.501830][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.0070165  -0.         ...  0.06983767 -0.05391624
 -0.01592143]

[2020-01-27 15:17:38.502239][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:17:38.882917][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.07805195  0.         ...  0.01943926  0.41262175
 -0.43206101], shape=(4547,), dtype=float64)

[2020-01-27 15:17:38.960590][__main__.TRPOAgent.log][linesearch]: improvement: -0.15202412140431587

[2020-01-27 15:17:38.990171][__main__.TRPOAgent.log][linesearch]: improvement: -0.0768580984582578

[2020-01-27 15:17:39.018076][__main__.TRPOAgent.log][linesearch]: improvement: -0.039036714283132656

[2020-01-27 15:17:39.041829][__main__.TRPOAgent.log][linesearch]: improvement: -0.021586173737146552

[2020-01-27 15:17:39.069692][__main__.TRPOAgent.log][linesearch]: improvement: -0.012051030784811845

[2020-01-27 15:17:39.092928][__main__.TRPOAgent.log][linesearch]: improvement: -0.006896483278648391

[2020-01-27 15:17:39.119768][__main__.TRPOAgent.log][linesearch]: improvement: -0.004051105664453658

[2020-01-27 15:17:39.145913][__main__.TRPOAgent.log][linesearch]: improvement: -0.0023804336144930827

[2020-01-27 15:17:39.174004][__main__.TRPOAgent.log][linesearch]: improvement: -0.001409503207120244

[2020-01-27 15:17:39.198059][__main__.TRPOAgent.log][linesearch]: improvement: -0.0008399511255876013

[2020-01-27 15:17:39.198709][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.781720516612454e-07, Discarded policy loss value: -0.32277378778501914

[2020-01-27 15:17:39.943341][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0608190468946

[2020-01-27 15:17:39.959829][__main__.TRPOAgent.log][learning]: Episode #31

[2020-01-27 15:17:39.960227][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:17:40.010880][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:17:40.011595][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:17:40.012402][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:17:40.014568][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:17:43.470220][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:17:43.498478][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:17:43.499833][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:17:43.501394][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:17:43.501483][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:17:43.504213][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:17:46.775437][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:17:46.795754][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 15:17:46.796597][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:17:46.797182][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:17:46.797131][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:17:46.799491][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:17:50.166796][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:17:50.171018][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:17:50.172177][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:17:50.172715][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:17:50.172831][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:17:50.174580][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:17:53.629431][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:17:53.666495][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:17:53.667759][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:17:53.668557][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:17:53.668635][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:17:53.669937][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:17:57.165679][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:17:57.208048][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:17:57.208899][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:17:57.209482][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:17:57.209563][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:17:57.214107][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:18:01.433542][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:18:01.470859][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:18:01.472578][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:18:01.473550][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:18:01.473404][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:18:01.475007][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:18:05.428819][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:18:05.439528][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:18:05.441005][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:18:05.441850][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:18:05.441939][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:18:05.445270][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:18:09.233349][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:18:09.239222][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:18:09.240139][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:18:09.240706][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:18:09.240652][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:18:09.241977][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:18:13.013676][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:18:13.068536][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:18:13.069564][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:18:13.070412][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:18:13.070316][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:18:13.071503][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:18:16.707008][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:18:16.757220][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:18:16.758128][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:18:16.758932][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:18:16.758847][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:18:16.760164][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:18:20.332161][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:18:20.334392][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:18:20.335306][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:18:20.335779][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:18:20.335846][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:18:20.337699][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:18:23.611219][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:18:23.679567][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:18:23.680126][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:18:23.680656][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:18:23.680744][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:18:23.683498][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:18:26.938114][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:18:26.967594][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:18:26.968116][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:18:26.968633][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:18:26.968722][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:18:26.971160][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:18:30.277988][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:18:30.328895][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:18:30.329417][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:18:30.329894][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:18:30.329947][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:18:30.331788][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:18:33.664949][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:18:33.707993][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:18:33.708584][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:18:33.721320][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:18:34.211071][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:18:34.241209][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:18:34.242685][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 15:18:34.243164][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:18:34.275403][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:18:37.703467][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.01512175 -0.         ... -0.19043541 -0.00605896
  0.19649437]

[2020-01-27 15:18:37.703876][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:18:38.090072][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.78236772  0.         ...  0.03786997 -0.29594158
  0.25807161], shape=(4547,), dtype=float64)

[2020-01-27 15:18:38.172384][__main__.TRPOAgent.log][linesearch]: improvement: -0.010479836971526713

[2020-01-27 15:18:38.200109][__main__.TRPOAgent.log][linesearch]: improvement: -0.03790175783085316

[2020-01-27 15:18:38.200599][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.09784684937719006

[2020-01-27 15:18:38.995668][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.91308517918935

[2020-01-27 15:18:38.996089][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:18:39.004481][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:18:42.434057][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01632735 -0.         ...  0.07292934  0.02717306
 -0.1001024 ]

[2020-01-27 15:18:42.434432][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:18:42.830400][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.26503795  0.         ... -0.04072851 -0.14807118
  0.18879969], shape=(4547,), dtype=float64)

[2020-01-27 15:18:42.909650][__main__.TRPOAgent.log][linesearch]: improvement: -0.08135960381530438

[2020-01-27 15:18:42.937613][__main__.TRPOAgent.log][linesearch]: improvement: -0.04226995066642242

[2020-01-27 15:18:42.961337][__main__.TRPOAgent.log][linesearch]: improvement: -0.023004418806900295

[2020-01-27 15:18:42.986744][__main__.TRPOAgent.log][linesearch]: improvement: -0.012918989761588201

[2020-01-27 15:18:43.011264][__main__.TRPOAgent.log][linesearch]: improvement: -0.007017712222692346

[2020-01-27 15:18:43.040490][__main__.TRPOAgent.log][linesearch]: improvement: -0.004006401614399191

[2020-01-27 15:18:43.068883][__main__.TRPOAgent.log][linesearch]: improvement: -0.0024464066481201807

[2020-01-27 15:18:43.095345][__main__.TRPOAgent.log][linesearch]: improvement: -0.0015299397164160455

[2020-01-27 15:18:43.124692][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009384580170535561

[2020-01-27 15:18:43.154958][__main__.TRPOAgent.log][linesearch]: improvement: -0.0005747469562239482

[2020-01-27 15:18:43.155405][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.32549229761086e-07, Discarded policy loss value: -0.22255106697279836

[2020-01-27 15:18:43.908198][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9441344027631

[2020-01-27 15:18:43.916082][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:18:47.330015][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05545033 -0.         ...  0.19719312 -0.15360573
 -0.0435874 ]

[2020-01-27 15:18:47.330398][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:18:47.711483][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.77483622  0.         ... -0.07425701 -0.44809539
  0.5223524 ], shape=(4547,), dtype=float64)

[2020-01-27 15:18:47.793064][__main__.TRPOAgent.log][linesearch]: improvement: -0.23817740694834177

[2020-01-27 15:18:47.824055][__main__.TRPOAgent.log][linesearch]: improvement: -0.09411036258536948

[2020-01-27 15:18:47.849856][__main__.TRPOAgent.log][linesearch]: improvement: -0.035099628397046556

[2020-01-27 15:18:47.850523][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.32836208379176607

[2020-01-27 15:18:48.598869][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9574478315554

[2020-01-27 15:18:48.599280][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:18:48.606915][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 15:18:51.984000][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.02344113 -0.         ...  0.0174118  -0.24330101
  0.22588922]

[2020-01-27 15:18:51.984380][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:18:52.369270][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.46278899  0.         ... -0.02821571  0.22709137
 -0.19887566], shape=(4547,), dtype=float64)

[2020-01-27 15:18:52.448142][__main__.TRPOAgent.log][linesearch]: improvement: -0.10237721508868441

[2020-01-27 15:18:52.475811][__main__.TRPOAgent.log][linesearch]: improvement: -0.05969017231901286

[2020-01-27 15:18:52.476271][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.2373902747770929

[2020-01-27 15:18:53.204743][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.80629526272156

[2020-01-27 15:18:53.205165][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:18:53.219979][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 15:18:56.671394][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.02452779 -0.         ... -0.33666725  0.17599618
  0.16067107]

[2020-01-27 15:18:56.671803][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:18:57.054044][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          1.18265341  0.         ...  0.71027911  0.27719516
 -0.98747427], shape=(4547,), dtype=float64)

[2020-01-27 15:18:57.131407][__main__.TRPOAgent.log][linesearch]: improvement: 0.37152349222944697

[2020-01-27 15:18:57.159898][__main__.TRPOAgent.log][linesearch]: improvement: 0.08424703236466402

[2020-01-27 15:18:57.188316][__main__.TRPOAgent.log][linesearch]: improvement: 0.004663138955912971

[2020-01-27 15:18:57.188754][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.5827949196653576

[2020-01-27 15:18:57.925209][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.14732777494436

[2020-01-27 15:18:57.925791][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:18:57.934395][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 15:19:01.375511][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.01927458 -0.         ...  0.14051722  0.13971509
 -0.28023232]

[2020-01-27 15:19:01.375902][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:19:01.763726][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.74787687  0.         ... -0.0807059  -0.3936248
  0.4743307 ], shape=(4547,), dtype=float64)

[2020-01-27 15:19:01.842924][__main__.TRPOAgent.log][linesearch]: improvement: -0.052742653755446234

[2020-01-27 15:19:01.870999][__main__.TRPOAgent.log][linesearch]: improvement: -0.036610769974812996

[2020-01-27 15:19:01.895831][__main__.TRPOAgent.log][linesearch]: improvement: -0.025847891219042962

[2020-01-27 15:19:01.923837][__main__.TRPOAgent.log][linesearch]: improvement: -0.017600529195923698

[2020-01-27 15:19:01.948433][__main__.TRPOAgent.log][linesearch]: improvement: -0.01137638833305149

[2020-01-27 15:19:01.976221][__main__.TRPOAgent.log][linesearch]: improvement: -0.007139645779071058

[2020-01-27 15:19:02.000019][__main__.TRPOAgent.log][linesearch]: improvement: -0.004405879273688684

[2020-01-27 15:19:02.026439][__main__.TRPOAgent.log][linesearch]: improvement: -0.002685454380094987

[2020-01-27 15:19:02.052055][__main__.TRPOAgent.log][linesearch]: improvement: -0.001626151900361139

[2020-01-27 15:19:02.079537][__main__.TRPOAgent.log][linesearch]: improvement: -0.0009815653061222007

[2020-01-27 15:19:02.080015][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.432743689763161e-07, Discarded policy loss value: -0.3524455612651784

[2020-01-27 15:19:02.784450][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.02055387053565

[2020-01-27 15:19:02.789663][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 15:19:06.225387][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00092232 -0.         ... -0.16795539  0.1220253
  0.04593009]

[2020-01-27 15:19:06.225775][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:19:06.633240][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          2.16856433  0.         ...  0.43159822  0.99793647
 -1.4295347 ], shape=(4547,), dtype=float64)

[2020-01-27 15:19:06.718866][__main__.TRPOAgent.log][linesearch]: improvement: -0.06849069218066187

[2020-01-27 15:19:06.719497][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.2613860792170874

[2020-01-27 15:19:07.444528][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.85921559040827

[2020-01-27 15:19:07.444923][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:19:07.452494][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 15:19:11.339539][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.12096729 -0.         ... -0.22825427  0.32692963
 -0.09867536]

[2020-01-27 15:19:11.339937][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:19:11.747906][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.27443333  0.         ...  0.10004257 -0.05499625
 -0.04504632], shape=(4547,), dtype=float64)

[2020-01-27 15:19:11.829993][__main__.TRPOAgent.log][linesearch]: improvement: -0.19893740942067806

[2020-01-27 15:19:11.853311][__main__.TRPOAgent.log][linesearch]: improvement: -0.10892010925986012

[2020-01-27 15:19:11.880367][__main__.TRPOAgent.log][linesearch]: improvement: -0.062333181201160714

[2020-01-27 15:19:11.880814][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.4476084263133473

[2020-01-27 15:19:12.624528][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9120220470157

[2020-01-27 15:19:12.624936][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:19:12.632910][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 15:19:16.094452][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.05230691 -0.         ... -0.33174153  0.20246968
  0.12927185]

[2020-01-27 15:19:16.094834][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:19:16.475873][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.35458393  0.         ...  0.01945809 -0.19051291
  0.17105482], shape=(4547,), dtype=float64)

[2020-01-27 15:19:16.553109][__main__.TRPOAgent.log][linesearch]: improvement: -0.039626354628583514

[2020-01-27 15:19:16.553555][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.09880911057257032

[2020-01-27 15:19:17.324589][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8601847242976

[2020-01-27 15:19:17.325047][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:19:17.336783][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 15:19:21.171577][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.0005592  -0.         ...  0.22963634 -0.15134994
 -0.0782864 ]

[2020-01-27 15:19:21.171970][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:19:21.550979][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.2470719   0.         ... -0.03253772 -0.28336262
  0.31590033], shape=(4547,), dtype=float64)

[2020-01-27 15:19:21.631486][__main__.TRPOAgent.log][linesearch]: improvement: -0.09920776856613073

[2020-01-27 15:19:21.659266][__main__.TRPOAgent.log][linesearch]: improvement: -0.05610915049935877

[2020-01-27 15:19:21.659772][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.3552245664539436

[2020-01-27 15:19:22.390556][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.0111628773973

[2020-01-27 15:19:22.390948][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:19:22.417610][__main__.TRPOAgent.log][learning]: Episode #32

[2020-01-27 15:19:22.418133][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 15:19:22.468138][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 15:19:22.468714][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 15:19:22.469215][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 15:19:22.471332][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 15:19:26.006631][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 15:19:26.091993][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 15:19:26.092608][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 15:19:26.093614][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 15:19:26.093673][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 15:19:26.095911][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 15:19:29.859463][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1462

[2020-01-27 15:19:29.863299][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 15:19:29.864473][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 15:19:29.864941][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 15:19:29.864993][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 15:19:29.866911][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 15:19:34.224230][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 15:19:34.244095][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 15:19:34.244685][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 15:19:34.245165][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 15:19:34.245221][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 15:19:34.247506][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 15:19:37.997672][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 15:19:38.027736][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 15:19:38.028376][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 15:19:38.028950][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 15:19:38.029022][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 15:19:38.033862][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 15:19:41.838042][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 15:19:41.847414][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 15:19:41.849048][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 15:19:41.849948][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 15:19:41.850037][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 15:19:41.851902][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 15:19:45.736205][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 15:19:45.805221][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 15:19:45.806570][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 15:19:45.807226][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 15:19:45.807297][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 15:19:45.809620][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 15:19:49.460546][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 15:19:49.509823][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 15:19:49.510410][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 15:19:49.510941][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 15:19:49.511000][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 15:19:49.512294][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 15:19:52.783453][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 15:19:52.834560][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 15:19:52.835659][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 15:19:52.836271][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 15:19:52.836336][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 15:19:52.839889][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 15:19:56.278658][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 15:19:56.307209][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 15:19:56.308072][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 15:19:56.309088][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 15:19:56.309151][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 15:19:56.311127][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 15:19:59.703203][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 15:19:59.722287][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 15:19:59.722895][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 15:19:59.723433][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 15:19:59.723507][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 15:19:59.725596][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 15:20:03.076376][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 15:20:03.088945][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 15:20:03.089983][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 15:20:03.090811][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 15:20:03.090724][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 15:20:03.091811][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 15:20:06.482549][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 15:20:06.503259][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 15:20:06.503958][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 15:20:06.504768][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 15:20:06.504939][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 15:20:06.508748][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 15:20:09.837868][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 15:20:09.910771][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 15:20:09.911302][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 15:20:09.911832][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 15:20:09.911887][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 15:20:09.913724][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 15:20:13.223832][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 15:20:13.231697][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 15:20:13.232214][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 15:20:13.232893][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 15:20:13.232967][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 15:20:13.237020][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 15:20:16.553705][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 15:20:16.630088][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 15:20:16.630629][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 15:20:16.642670][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 15:20:17.127989][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 15:20:17.155547][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 15:20:17.157158][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 44962, Batch size: 4500, Number of batches: 10

[2020-01-27 15:20:17.157680][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 15:20:17.188767][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 15:20:20.668955][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.0059265  -0.         ... -0.00564602  0.07808684
 -0.07244082]

[2020-01-27 15:20:20.669351][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:20:21.057023][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.02812923  0.         ... -0.16626927 -0.33502371
  0.50129298], shape=(4547,), dtype=float64)

[2020-01-27 15:20:21.144487][__main__.TRPOAgent.log][linesearch]: improvement: -0.21270714704954075

[2020-01-27 15:20:21.169775][__main__.TRPOAgent.log][linesearch]: improvement: -0.12544657185547395

[2020-01-27 15:20:21.193261][__main__.TRPOAgent.log][linesearch]: improvement: -0.06842891169995913

[2020-01-27 15:20:21.218015][__main__.TRPOAgent.log][linesearch]: improvement: -0.03405626769448564

[2020-01-27 15:20:21.218460][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 0.15670498052224643

[2020-01-27 15:20:21.976919][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.053077822065

[2020-01-27 15:20:21.977394][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:20:21.986340][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 15:20:25.527994][__main__.TRPOAgent.log][training]: policy_gradient: [-0.          0.00756596 -0.         ... -0.19455133 -0.12523781
  0.31978914]

[2020-01-27 15:20:25.528383][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:20:25.911342][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.90611543  0.         ... -0.35316618 -0.84045308
  1.19361926], shape=(4547,), dtype=float64)

[2020-01-27 15:20:25.991853][__main__.TRPOAgent.log][linesearch]: improvement: -0.3696187502906679

[2020-01-27 15:20:26.018506][__main__.TRPOAgent.log][linesearch]: improvement: -0.17258395251414993

[2020-01-27 15:20:26.045372][__main__.TRPOAgent.log][linesearch]: improvement: -0.08239105482835254

[2020-01-27 15:20:26.045814][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.10350598565287349

[2020-01-27 15:20:26.778519][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.8814114736268

[2020-01-27 15:20:26.778920][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 15:20:26.787617][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 15:20:30.279849][__main__.TRPOAgent.log][training]: policy_gradient: [-0.         -0.00069564 -0.         ... -0.16138535  0.04533526
  0.11605009]

[2020-01-27 15:20:30.280250][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 15:20:30.664801][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.         -0.17458709  0.         ...  0.19081912  0.63232353
 -0.82314265], shape=(4547,), dtype=float64)

[2020-01-27 15:20:30.741466][__main__.TRPOAgent.log][linesearch]: improvement: -0.040607949069898286

[2020-01-27 15:20:30.768323][__main__.TRPOAgent.log][linesearch]: improvement: -0.012536962577246125

[2020-01-27 15:20:30.794750][__main__.TRPOAgent.log][linesearch]: improvement: -0.0040175216623022925

[2020-01-27 15:20:30.823696][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028294197691092204

[2020-01-27 15:20:30.848815][__main__.TRPOAgent.log][linesearch]: improvement: -0.0030990623675312134

[2020-01-27 15:20:30.880593][__main__.TRPOAgent.log][linesearch]: improvement: -0.0025800089571717533

[2020-01-27 15:20:30.905039][__main__.TRPOAgent.log][linesearch]: improvement: -0.0028552938984824916

[2020-01-27 15:20:30.930575][__main__.TRPOAgent.log][linesearch]: improvement: -0.0018148845646141498

[2020-01-27 15:20:30.957271][__main__.TRPOAgent.log][linesearch]: improvement: -0.0017001335092457492

[2020-01-27 15:20:30.983232][__main__.TRPOAgent.log][linesearch]: improvement: -0.001140744225411272

[2020-01-27 15:20:30.983842][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.159996784986229e-07, Discarded policy loss value: -0.45449740689250756

[2020-01-27 15:20:31.735997][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.9321207508998

[2020-01-27 15:20:31.741328][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

