LOGGER started at 2020-01-27 14:10:57.110574.
Currently active debug channels:
	rollouts
	training
	batch_info
	linesearch
	learning
	thread_rollouts
[2020-01-27 14:10:57.238526][__main__.TRPOAgent.log][learning]: Episode #0

[2020-01-27 14:10:57.445288][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:10:57.478960][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:10:57.479856][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:10:57.479710][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:10:57.484613][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:11:00.828369][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:11:00.878522][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:11:00.878976][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:11:00.879720][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:11:00.879544][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:11:00.880693][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:11:04.238108][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:11:04.271729][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:11:04.272213][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:11:04.272824][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:11:04.272903][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:11:04.275112][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:11:07.159660][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1287

[2020-01-27 14:11:07.348870][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:11:07.349470][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:11:07.350070][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:11:07.350163][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:11:07.353888][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:11:10.625532][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:11:10.639712][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:11:10.640207][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:11:10.640878][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:11:10.640968][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:11:10.643508][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:11:13.881466][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:11:13.957209][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:11:13.957722][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:11:13.958579][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:11:13.958288][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:11:13.959667][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:11:17.162894][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:11:17.225679][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:11:17.226182][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:11:17.226996][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:11:17.226915][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:11:17.228235][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:11:20.882301][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1455

[2020-01-27 14:11:20.985595][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:11:20.986129][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:11:20.987123][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:11:20.987049][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:11:20.988253][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:11:24.249070][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:11:24.317293][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:11:24.317893][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:11:24.318743][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:11:24.318641][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:11:24.319816][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:11:27.595705][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:11:27.616596][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:11:27.617084][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:11:27.617653][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:11:27.617820][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:11:27.619677][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:11:31.529097][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:11:31.532509][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:11:31.533368][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:11:31.533832][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:11:31.533893][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:11:31.534926][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:11:35.074467][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:11:35.126606][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:11:35.127079][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:11:35.127811][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:11:35.127586][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:11:35.128731][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:11:38.422869][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:11:38.462891][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:11:38.463390][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:11:38.463952][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:11:38.464156][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:11:38.468918][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:11:42.159250][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:11:42.192503][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:11:42.193005][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:11:42.193662][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:11:42.193743][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:11:42.196142][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:11:46.470844][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:11:46.517099][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:11:46.517576][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:11:46.518230][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:11:46.518308][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:11:46.522286][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:11:49.809596][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1411

[2020-01-27 14:11:49.907829][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:11:49.908305][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:11:49.919736][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:11:50.416978][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:11:50.423454][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:11:50.424859][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 44653, Batch size: 4500, Number of batches: 10

[2020-01-27 14:11:50.425238][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:11:50.455155][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:11:54.348916][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.          0.          0.10835398 ...  0.97110838 -1.44000706
  0.46889869]

[2020-01-27 14:11:54.349315][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:11:54.752198][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.          1.53414736 ...  3.34745502  0.41338707
 -3.76084209], shape=(4547,), dtype=float64)

[2020-01-27 14:11:54.829156][__main__.TRPOAgent.log][linesearch]: improvement: 0.10549825192555318

[2020-01-27 14:11:54.857953][__main__.TRPOAgent.log][linesearch]: improvement: 0.040500111717847176

[2020-01-27 14:11:54.883197][__main__.TRPOAgent.log][linesearch]: improvement: 0.04171063633474148

[2020-01-27 14:11:54.910069][__main__.TRPOAgent.log][linesearch]: improvement: 0.02536685340760414

[2020-01-27 14:11:54.935387][__main__.TRPOAgent.log][linesearch]: improvement: 0.021344120474580563

[2020-01-27 14:11:54.962446][__main__.TRPOAgent.log][linesearch]: improvement: 0.01732703542320735

[2020-01-27 14:11:54.985478][__main__.TRPOAgent.log][linesearch]: improvement: 0.009482231250487416

[2020-01-27 14:11:55.012538][__main__.TRPOAgent.log][linesearch]: improvement: 0.004789064134399723

[2020-01-27 14:11:55.039607][__main__.TRPOAgent.log][linesearch]: improvement: 0.0033291301747340185

[2020-01-27 14:11:55.065143][__main__.TRPOAgent.log][linesearch]: improvement: 0.002341460528668904

[2020-01-27 14:11:55.065599][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 2.814348066763949e-07, Discarded policy loss value: -93.27252886214987

[2020-01-27 14:11:55.906795][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 341.1329851754607

[2020-01-27 14:11:55.911543][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:11:59.546773][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.001058   -0.00079818  0.00441935 ...  0.05590026 -0.09632752
  0.04042725]

[2020-01-27 14:11:59.547170][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:11:59.933310][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.44685565 -0.28518382  0.31030327 ... -0.15474892  0.19132455
 -0.03657563], shape=(4547,), dtype=float64)

[2020-01-27 14:12:00.011423][__main__.TRPOAgent.log][linesearch]: improvement: -0.01389797309063745

[2020-01-27 14:12:00.034742][__main__.TRPOAgent.log][linesearch]: improvement: -0.01490906260951741

[2020-01-27 14:12:00.062561][__main__.TRPOAgent.log][linesearch]: improvement: -0.008269315335557081

[2020-01-27 14:12:00.087552][__main__.TRPOAgent.log][linesearch]: improvement: -0.0033297472631819147

[2020-01-27 14:12:00.112584][__main__.TRPOAgent.log][linesearch]: improvement: -0.0019361663759149828

[2020-01-27 14:12:00.140723][__main__.TRPOAgent.log][linesearch]: improvement: -0.001200153011635674

[2020-01-27 14:12:00.166666][__main__.TRPOAgent.log][linesearch]: improvement: -0.00025342142086470787

[2020-01-27 14:12:00.195259][__main__.TRPOAgent.log][linesearch]: improvement: 9.371521381895676e-05

[2020-01-27 14:12:00.220354][__main__.TRPOAgent.log][linesearch]: improvement: 0.0002370943676655024

[2020-01-27 14:12:00.248554][__main__.TRPOAgent.log][linesearch]: improvement: 0.00026233989743928987

[2020-01-27 14:12:00.249151][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 1.8900286879374146e-07, Discarded policy loss value: -2.022107559818501

[2020-01-27 14:12:00.985114][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 303.9303148688067

[2020-01-27 14:12:00.990296][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:12:04.412646][__main__.TRPOAgent.log][training]: policy_gradient: [-8.78925708e-06  2.63032552e-06 -5.54264761e-03 ... -7.02480492e-02
  8.21269896e-02 -1.18789404e-02]

[2020-01-27 14:12:04.413032][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:04.840874][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.00564958  0.00194543 -0.14001364 ... -0.10060757  0.56626513
 -0.46565756], shape=(4547,), dtype=float64)

[2020-01-27 14:12:04.920915][__main__.TRPOAgent.log][linesearch]: improvement: 0.09862366785536242

[2020-01-27 14:12:04.948266][__main__.TRPOAgent.log][linesearch]: improvement: 0.0506099321183483

[2020-01-27 14:12:04.976065][__main__.TRPOAgent.log][linesearch]: improvement: 0.02698419508248298

[2020-01-27 14:12:05.003828][__main__.TRPOAgent.log][linesearch]: improvement: 0.013204684550120055

[2020-01-27 14:12:05.030827][__main__.TRPOAgent.log][linesearch]: improvement: 0.0076716941037345

[2020-01-27 14:12:05.058910][__main__.TRPOAgent.log][linesearch]: improvement: 0.004886020210764397

[2020-01-27 14:12:05.084901][__main__.TRPOAgent.log][linesearch]: improvement: 0.003086706644043355

[2020-01-27 14:12:05.113145][__main__.TRPOAgent.log][linesearch]: improvement: 0.0017753463485257887

[2020-01-27 14:12:05.139481][__main__.TRPOAgent.log][linesearch]: improvement: 0.0011163223626277224

[2020-01-27 14:12:05.165318][__main__.TRPOAgent.log][linesearch]: improvement: 0.000659637169492272

[2020-01-27 14:12:05.165884][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 1.7846440847370583e-06, Discarded policy loss value: -0.117268473788667

[2020-01-27 14:12:05.906003][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.4673894207133

[2020-01-27 14:12:05.913879][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:12:09.813973][__main__.TRPOAgent.log][training]: policy_gradient: [-2.20112313e-06  3.51637437e-05 -1.28055395e-02 ...  1.18728871e-02
  1.63523106e-01 -1.75395993e-01]

[2020-01-27 14:12:09.814366][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:10.244111][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.00595372  0.02965722 -0.20354139 ... -1.40356255  1.01991428
  0.38364827], shape=(4547,), dtype=float64)

[2020-01-27 14:12:10.325797][__main__.TRPOAgent.log][linesearch]: improvement: 0.22214132027096856

[2020-01-27 14:12:10.352573][__main__.TRPOAgent.log][linesearch]: improvement: 0.09315969411788227

[2020-01-27 14:12:10.377044][__main__.TRPOAgent.log][linesearch]: improvement: 0.040416096638007826

[2020-01-27 14:12:10.401109][__main__.TRPOAgent.log][linesearch]: improvement: 0.01842730773721346

[2020-01-27 14:12:10.428209][__main__.TRPOAgent.log][linesearch]: improvement: 0.009681617413519472

[2020-01-27 14:12:10.451986][__main__.TRPOAgent.log][linesearch]: improvement: 0.005377777163898312

[2020-01-27 14:12:10.478858][__main__.TRPOAgent.log][linesearch]: improvement: 0.0019892454902938894

[2020-01-27 14:12:10.502959][__main__.TRPOAgent.log][linesearch]: improvement: 0.0006015227189721806

[2020-01-27 14:12:10.530868][__main__.TRPOAgent.log][linesearch]: improvement: 0.000252007010667743

[2020-01-27 14:12:10.554325][__main__.TRPOAgent.log][linesearch]: improvement: 0.000308148921134066

[2020-01-27 14:12:10.554763][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 2.6778681227303934e-07, Discarded policy loss value: -2.291451183608991

[2020-01-27 14:12:11.295447][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.4258796671149

[2020-01-27 14:12:11.300680][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:12:14.984907][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00109753 -0.00038182 -0.01624428 ...  0.18318361  0.09777519
 -0.2809588 ]

[2020-01-27 14:12:14.985308][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:15.377618][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.19257673 -0.06437619 -0.12475572 ...  0.93557896 -1.25504046
  0.3194615 ], shape=(4547,), dtype=float64)

[2020-01-27 14:12:15.457802][__main__.TRPOAgent.log][linesearch]: improvement: 0.009981024007788442

[2020-01-27 14:12:15.484724][__main__.TRPOAgent.log][linesearch]: improvement: 0.03657221928593124

[2020-01-27 14:12:15.507792][__main__.TRPOAgent.log][linesearch]: improvement: 0.03630215881213372

[2020-01-27 14:12:15.534289][__main__.TRPOAgent.log][linesearch]: improvement: 0.024192580263505903

[2020-01-27 14:12:15.560470][__main__.TRPOAgent.log][linesearch]: improvement: 0.013814486268303838

[2020-01-27 14:12:15.588766][__main__.TRPOAgent.log][linesearch]: improvement: 0.007552702470946748

[2020-01-27 14:12:15.614726][__main__.TRPOAgent.log][linesearch]: improvement: 0.004442491666705628

[2020-01-27 14:12:15.642949][__main__.TRPOAgent.log][linesearch]: improvement: 0.0027406529134439417

[2020-01-27 14:12:15.671093][__main__.TRPOAgent.log][linesearch]: improvement: 0.0016593729711864569

[2020-01-27 14:12:15.696719][__main__.TRPOAgent.log][linesearch]: improvement: 0.0010123044288850025

[2020-01-27 14:12:15.697237][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.079133731179628e-07, Discarded policy loss value: -2.5250382133795437

[2020-01-27 14:12:16.436017][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.26004555955024

[2020-01-27 14:12:16.443920][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:12:19.929810][__main__.TRPOAgent.log][training]: policy_gradient: [-3.93496723e-05  5.31676857e-06  1.39452618e-02 ...  2.44241245e-01
 -2.30648996e-01 -1.35922485e-02]

[2020-01-27 14:12:19.930234][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:20.316896][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.02557292  0.00345173 -0.59195831 ...  0.58269474 -0.24581326
 -0.33688147], shape=(4547,), dtype=float64)

[2020-01-27 14:12:20.399207][__main__.TRPOAgent.log][linesearch]: improvement: 0.001164184491376119

[2020-01-27 14:12:20.423468][__main__.TRPOAgent.log][linesearch]: improvement: 0.006525630237026014

[2020-01-27 14:12:20.451757][__main__.TRPOAgent.log][linesearch]: improvement: 0.011791952149744489

[2020-01-27 14:12:20.475222][__main__.TRPOAgent.log][linesearch]: improvement: 0.005362223076064154

[2020-01-27 14:12:20.501692][__main__.TRPOAgent.log][linesearch]: improvement: 0.0033784484576253604

[2020-01-27 14:12:20.525809][__main__.TRPOAgent.log][linesearch]: improvement: 0.002738344955884209

[2020-01-27 14:12:20.552168][__main__.TRPOAgent.log][linesearch]: improvement: 0.0018055172690445431

[2020-01-27 14:12:20.578754][__main__.TRPOAgent.log][linesearch]: improvement: 0.0010487002651302646

[2020-01-27 14:12:20.603664][__main__.TRPOAgent.log][linesearch]: improvement: 0.0007766914900448896

[2020-01-27 14:12:20.627880][__main__.TRPOAgent.log][linesearch]: improvement: 0.0005917916566957615

[2020-01-27 14:12:20.628346][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 3.0590822120101735e-07, Discarded policy loss value: -5.705823343003318

[2020-01-27 14:12:21.366781][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.78966167581956

[2020-01-27 14:12:21.371970][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:12:24.823753][__main__.TRPOAgent.log][training]: policy_gradient: [-6.96660270e-08 -7.89447372e-08  4.83519967e-03 ... -9.85350192e-02
 -3.23149602e-02  1.30849979e-01]

[2020-01-27 14:12:24.824139][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:25.225924][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-4.89618819e-05 -5.82281952e-05 -1.83790722e-01 ... -1.88055026e-01
  2.52057668e-01 -6.40026421e-02], shape=(4547,), dtype=float64)

[2020-01-27 14:12:25.304891][__main__.TRPOAgent.log][linesearch]: improvement: -0.046949537480219794

[2020-01-27 14:12:25.333959][__main__.TRPOAgent.log][linesearch]: improvement: -0.014165291832783866

[2020-01-27 14:12:25.357300][__main__.TRPOAgent.log][linesearch]: improvement: -0.007750664067441515

[2020-01-27 14:12:25.384774][__main__.TRPOAgent.log][linesearch]: improvement: -0.0013575605966674176

[2020-01-27 14:12:25.408824][__main__.TRPOAgent.log][linesearch]: improvement: 0.00068300473116778

[2020-01-27 14:12:25.436181][__main__.TRPOAgent.log][linesearch]: improvement: 0.001219893478040579

[2020-01-27 14:12:25.462974][__main__.TRPOAgent.log][linesearch]: improvement: 0.0011443691468733252

[2020-01-27 14:12:25.487587][__main__.TRPOAgent.log][linesearch]: improvement: 0.0009695617041179383

[2020-01-27 14:12:25.513409][__main__.TRPOAgent.log][linesearch]: improvement: 0.0005608791037593486

[2020-01-27 14:12:25.538301][__main__.TRPOAgent.log][linesearch]: improvement: 0.0003867049599343142

[2020-01-27 14:12:25.538737][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.26632878580185e-07, Discarded policy loss value: -0.3454940712348311

[2020-01-27 14:12:26.325828][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.670477561377

[2020-01-27 14:12:26.330425][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:12:29.808256][__main__.TRPOAgent.log][training]: policy_gradient: [-1.07263584e-04  4.62214550e-05 -1.44902704e-02 ...  1.40520886e-01
  1.14596041e-01 -2.55116927e-01]

[2020-01-27 14:12:29.808647][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:30.207057][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.0056515   0.08752809 -0.34364308 ...  0.05438401 -0.10137464
  0.04699063], shape=(4547,), dtype=float64)

[2020-01-27 14:12:30.282917][__main__.TRPOAgent.log][linesearch]: improvement: -0.07244125741145258

[2020-01-27 14:12:30.306588][__main__.TRPOAgent.log][linesearch]: improvement: -0.018595683483237535

[2020-01-27 14:12:30.333484][__main__.TRPOAgent.log][linesearch]: improvement: -0.00315571094529149

[2020-01-27 14:12:30.357034][__main__.TRPOAgent.log][linesearch]: improvement: 0.0010410407418702805

[2020-01-27 14:12:30.387713][__main__.TRPOAgent.log][linesearch]: improvement: 0.0028376945820041133

[2020-01-27 14:12:30.410581][__main__.TRPOAgent.log][linesearch]: improvement: 0.002544244823869446

[2020-01-27 14:12:30.438948][__main__.TRPOAgent.log][linesearch]: improvement: 0.0021020974390586478

[2020-01-27 14:12:30.463493][__main__.TRPOAgent.log][linesearch]: improvement: 0.0015043545677407844

[2020-01-27 14:12:30.488667][__main__.TRPOAgent.log][linesearch]: improvement: 0.0009925035364706236

[2020-01-27 14:12:30.512958][__main__.TRPOAgent.log][linesearch]: improvement: 0.0006769955438190589

[2020-01-27 14:12:30.513536][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.30733805671003e-07, Discarded policy loss value: -0.4259391955453844

[2020-01-27 14:12:31.230641][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.9604971341956

[2020-01-27 14:12:31.237084][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:12:34.676721][__main__.TRPOAgent.log][training]: policy_gradient: [ 2.91461701e-04 -6.73995447e-04 -6.37476027e-03 ... -4.55950557e-01
  2.87901204e-01  1.68049353e-01]

[2020-01-27 14:12:34.677129][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:35.070038][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.52637928 -0.43862683  0.33818518 ...  0.0657703   1.27576091
 -1.34153121], shape=(4547,), dtype=float64)

[2020-01-27 14:12:35.149787][__main__.TRPOAgent.log][linesearch]: improvement: -0.1843256912560018

[2020-01-27 14:12:35.176882][__main__.TRPOAgent.log][linesearch]: improvement: -0.09636582586049397

[2020-01-27 14:12:35.203396][__main__.TRPOAgent.log][linesearch]: improvement: -0.05250324043882115

[2020-01-27 14:12:35.227482][__main__.TRPOAgent.log][linesearch]: improvement: -0.027095004155831937

[2020-01-27 14:12:35.253584][__main__.TRPOAgent.log][linesearch]: improvement: -0.013179233406746205

[2020-01-27 14:12:35.276498][__main__.TRPOAgent.log][linesearch]: improvement: -0.004548699979469717

[2020-01-27 14:12:35.302944][__main__.TRPOAgent.log][linesearch]: improvement: -0.0003512558570809565

[2020-01-27 14:12:35.326886][__main__.TRPOAgent.log][linesearch]: improvement: 0.0013757952097924742

[2020-01-27 14:12:35.354046][__main__.TRPOAgent.log][linesearch]: improvement: 0.0019322969349805375

[2020-01-27 14:12:35.376768][__main__.TRPOAgent.log][linesearch]: improvement: 0.001636446687427573

[2020-01-27 14:12:35.377224][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.820797996376048e-07, Discarded policy loss value: -0.5165079431454398

[2020-01-27 14:12:36.095715][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.45916372285444

[2020-01-27 14:12:36.100478][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4153

[2020-01-27 14:12:39.269224][__main__.TRPOAgent.log][training]: policy_gradient: [-3.60219995e-03 -2.00955209e-04  1.00031183e-02 ...  3.16604188e-01
  7.54497276e-02 -3.92053916e-01]

[2020-01-27 14:12:39.269620][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:12:39.653966][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.47860364 -0.86119845  0.7746214  ... -0.37517416  5.67311919
 -5.29794503], shape=(4547,), dtype=float64)

[2020-01-27 14:12:39.728180][__main__.TRPOAgent.log][linesearch]: improvement: 0.26023123833348194

[2020-01-27 14:12:39.753792][__main__.TRPOAgent.log][linesearch]: improvement: 0.1855833795847145

[2020-01-27 14:12:39.777019][__main__.TRPOAgent.log][linesearch]: improvement: 0.12888878438365542

[2020-01-27 14:12:39.802136][__main__.TRPOAgent.log][linesearch]: improvement: 0.08599968799622798

[2020-01-27 14:12:39.802582][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 0.1239347677410958

[2020-01-27 14:12:40.476022][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 274.23225371760464

[2020-01-27 14:12:40.476432][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:12:40.491103][__main__.TRPOAgent.log][learning]: Episode #1

[2020-01-27 14:12:40.491453][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:12:40.530130][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:12:40.530770][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:12:40.530688][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:12:40.531995][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:12:43.792476][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:12:43.813548][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:12:43.814055][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:12:43.814658][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:12:43.814602][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:12:43.816495][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:12:47.156547][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:12:47.171165][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:12:47.171936][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:12:47.172677][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:12:47.172755][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:12:47.174829][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:12:50.500237][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:12:50.532251][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:12:50.532740][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:12:50.533223][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:12:50.533284][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:12:50.534819][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:12:53.725207][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:12:53.825363][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:12:53.825892][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:12:53.826427][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:12:53.826498][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:12:53.831299][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:12:57.164098][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:12:57.207868][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:12:57.208366][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:12:57.209024][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:12:57.209102][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:12:57.210964][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:13:00.541122][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:13:00.580721][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:13:00.581166][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:13:00.581634][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:13:00.581769][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:13:00.585077][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:13:03.824472][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:13:03.859068][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:13:03.859575][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:13:03.860155][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:13:03.860248][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:13:03.863082][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:13:07.217865][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:13:07.258314][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:13:07.258803][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:13:07.259355][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:13:07.259416][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:13:07.261347][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:13:10.612010][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:13:10.627811][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:13:10.628334][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:13:10.629050][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:13:10.628962][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:13:10.630145][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:13:13.806501][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:13:13.916919][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:13:13.917482][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:13:13.918137][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:13:13.918214][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:13:13.919686][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:13:17.264296][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:13:17.284587][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:13:17.285123][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:13:17.285786][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:13:17.285725][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:13:17.286838][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:13:21.058940][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:13:21.063309][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:13:21.069843][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:13:21.070529][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:13:21.070591][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:13:21.072428][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:13:24.889054][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:13:24.886822][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:13:24.890105][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:13:24.891157][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:13:24.891235][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:13:24.893655][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:13:29.035564][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:13:29.120962][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:13:29.121557][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:13:29.128377][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:13:29.128701][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:13:29.131116][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:13:32.562190][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:13:32.616969][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:13:32.617495][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:13:32.629143][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:13:33.126759][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:13:33.154821][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:13:33.156306][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:13:33.156676][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:13:33.186658][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:13:36.963393][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00596773  0.00049961 -0.00691901 ...  0.18726292  0.14352791
 -0.33079083]

[2020-01-27 14:13:36.963816][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:13:37.366348][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.90119813 -0.14003825 -0.53448241 ...  1.93468161 -1.02537797
 -0.90930364], shape=(4547,), dtype=float64)

[2020-01-27 14:13:37.444337][__main__.TRPOAgent.log][linesearch]: improvement: -0.05230260712539003

[2020-01-27 14:13:37.474333][__main__.TRPOAgent.log][linesearch]: improvement: -0.01669934645896287

[2020-01-27 14:13:37.499977][__main__.TRPOAgent.log][linesearch]: improvement: -0.002940864559759504

[2020-01-27 14:13:37.525093][__main__.TRPOAgent.log][linesearch]: improvement: 0.003070480862629843

[2020-01-27 14:13:37.552281][__main__.TRPOAgent.log][linesearch]: improvement: 0.004807648863731195

[2020-01-27 14:13:37.575681][__main__.TRPOAgent.log][linesearch]: improvement: 0.0043602897657839534

[2020-01-27 14:13:37.603699][__main__.TRPOAgent.log][linesearch]: improvement: 0.003402775006565939

[2020-01-27 14:13:37.632929][__main__.TRPOAgent.log][linesearch]: improvement: 0.002775467122135389

[2020-01-27 14:13:37.660564][__main__.TRPOAgent.log][linesearch]: improvement: 0.0021875410598761746

[2020-01-27 14:13:37.690179][__main__.TRPOAgent.log][linesearch]: improvement: 0.0015474423422849881

[2020-01-27 14:13:37.690620][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 3.6100230041993415e-07, Discarded policy loss value: -5.375025753284893

[2020-01-27 14:13:38.485242][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 284.7283859653165

[2020-01-27 14:13:38.493288][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:13:42.727579][__main__.TRPOAgent.log][training]: policy_gradient: [-0.0015123   0.00068809 -0.0209392  ...  0.15924302  0.13543771
 -0.29468072]

[2020-01-27 14:13:42.728006][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:13:43.193635][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-2.41352024  0.14984543 -2.22522142 ... -1.97834536  5.76291203
 -3.78456667], shape=(4547,), dtype=float64)

[2020-01-27 14:13:43.277763][__main__.TRPOAgent.log][linesearch]: improvement: 0.3437984979058921

[2020-01-27 14:13:43.306658][__main__.TRPOAgent.log][linesearch]: improvement: 0.2299042798056803

[2020-01-27 14:13:43.332230][__main__.TRPOAgent.log][linesearch]: improvement: 0.1399322909623466

[2020-01-27 14:13:43.358785][__main__.TRPOAgent.log][linesearch]: improvement: 0.08179987946495704

[2020-01-27 14:13:43.359237][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 4.688712328833665

[2020-01-27 14:13:44.256284][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 253.2257142617052

[2020-01-27 14:13:44.256690][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:13:44.263821][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:13:48.538918][__main__.TRPOAgent.log][training]: policy_gradient: [-0.01055336  0.00029313  0.01611924 ... -0.08765929 -0.11944128
  0.20710057]

[2020-01-27 14:13:48.539316][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:13:48.990811][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.01665935  0.07010714  1.5465445  ... -0.54831602  2.52690922
 -1.9785932 ], shape=(4547,), dtype=float64)

[2020-01-27 14:13:49.070137][__main__.TRPOAgent.log][linesearch]: improvement: -0.0784064918671028

[2020-01-27 14:13:49.094821][__main__.TRPOAgent.log][linesearch]: improvement: -0.037907177076122345

[2020-01-27 14:13:49.121364][__main__.TRPOAgent.log][linesearch]: improvement: -0.01378457339063388

[2020-01-27 14:13:49.145969][__main__.TRPOAgent.log][linesearch]: improvement: -0.0037954228059247086

[2020-01-27 14:13:49.174944][__main__.TRPOAgent.log][linesearch]: improvement: 0.0020677058282618788

[2020-01-27 14:13:49.202236][__main__.TRPOAgent.log][linesearch]: improvement: 0.004775789437441169

[2020-01-27 14:13:49.228372][__main__.TRPOAgent.log][linesearch]: improvement: 0.004909350646681077

[2020-01-27 14:13:49.255006][__main__.TRPOAgent.log][linesearch]: improvement: 0.004207027195605839

[2020-01-27 14:13:49.282435][__main__.TRPOAgent.log][linesearch]: improvement: 0.0030039914500052944

[2020-01-27 14:13:49.307588][__main__.TRPOAgent.log][linesearch]: improvement: 0.002023659847498749

[2020-01-27 14:13:49.308125][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.025278520900085e-07, Discarded policy loss value: -5.39785718503045

[2020-01-27 14:13:50.085824][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.14698485085745

[2020-01-27 14:13:50.091050][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:13:54.385940][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00481112 -0.00154141  0.02385852 ...  0.0427744  -0.02088292
 -0.02189148]

[2020-01-27 14:13:54.386341][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:13:54.788088][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.30414806 -0.82281794  1.03676038 ...  2.82055757  0.19961656
 -3.02017412], shape=(4547,), dtype=float64)

[2020-01-27 14:13:54.875252][__main__.TRPOAgent.log][linesearch]: improvement: 0.13026055674363513

[2020-01-27 14:13:54.898869][__main__.TRPOAgent.log][linesearch]: improvement: 0.09909490876982113

[2020-01-27 14:13:54.927468][__main__.TRPOAgent.log][linesearch]: improvement: 0.06827160588722159

[2020-01-27 14:13:54.951429][__main__.TRPOAgent.log][linesearch]: improvement: 0.046394461452467006

[2020-01-27 14:13:54.978756][__main__.TRPOAgent.log][linesearch]: improvement: 0.03073195966113451

[2020-01-27 14:13:55.005084][__main__.TRPOAgent.log][linesearch]: improvement: 0.01881078703343947

[2020-01-27 14:13:55.029442][__main__.TRPOAgent.log][linesearch]: improvement: 0.011416274594065967

[2020-01-27 14:13:55.055765][__main__.TRPOAgent.log][linesearch]: improvement: 0.0071122837417509355

[2020-01-27 14:13:55.080231][__main__.TRPOAgent.log][linesearch]: improvement: 0.004541947300196469

[2020-01-27 14:13:55.109233][__main__.TRPOAgent.log][linesearch]: improvement: 0.0028601836056940666

[2020-01-27 14:13:55.109675][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.183188624141944e-07, Discarded policy loss value: -0.36216009834494556

[2020-01-27 14:13:55.861806][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 283.17788784607217

[2020-01-27 14:13:55.866879][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:13:59.384123][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00720913  0.00155722 -0.01052996 ...  0.07815595 -0.01006599
 -0.06808996]

[2020-01-27 14:13:59.384517][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:13:59.779212][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 2.48803545  0.8094631   0.50548697 ... -0.98238141  0.15942005
  0.82296136], shape=(4547,), dtype=float64)

[2020-01-27 14:13:59.864328][__main__.TRPOAgent.log][linesearch]: improvement: -0.01599160473700323

[2020-01-27 14:13:59.892150][__main__.TRPOAgent.log][linesearch]: improvement: -0.006134133579356771

[2020-01-27 14:13:59.892608][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 5.198402088871973

[2020-01-27 14:14:00.614520][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 276.3512346600811

[2020-01-27 14:14:00.614922][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:14:00.622284][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:14:04.154653][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00242234 -0.00204443 -0.06353435 ...  0.07822165  0.04563746
 -0.12385911]

[2020-01-27 14:14:04.155032][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:14:04.553538][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 1.53931425 -1.63876815  0.09923882 ...  0.73477242  0.06107883
 -0.79585124], shape=(4547,), dtype=float64)

[2020-01-27 14:14:04.638504][__main__.TRPOAgent.log][linesearch]: improvement: 0.13674593274739325

[2020-01-27 14:14:04.665987][__main__.TRPOAgent.log][linesearch]: improvement: 0.0832854447866341

[2020-01-27 14:14:04.691940][__main__.TRPOAgent.log][linesearch]: improvement: 0.045375156063329924

[2020-01-27 14:14:04.692389][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.2574422580483516

[2020-01-27 14:14:05.412724][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 296.6977262597886

[2020-01-27 14:14:05.413130][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:14:05.420379][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:14:08.920435][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.          0.         -0.00856429 ...  0.50390081 -0.39730805
 -0.10659276]

[2020-01-27 14:14:08.920837][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:14:09.316679][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.          0.          1.14862083 ... -1.1247151   2.60152208
 -1.47680698], shape=(4547,), dtype=float64)

[2020-01-27 14:14:09.398484][__main__.TRPOAgent.log][linesearch]: improvement: 0.17158564867686454

[2020-01-27 14:14:09.428954][__main__.TRPOAgent.log][linesearch]: improvement: 0.09571658436508645

[2020-01-27 14:14:09.453232][__main__.TRPOAgent.log][linesearch]: improvement: 0.0587811128447675

[2020-01-27 14:14:09.482008][__main__.TRPOAgent.log][linesearch]: improvement: 0.03840491009201097

[2020-01-27 14:14:09.507326][__main__.TRPOAgent.log][linesearch]: improvement: 0.028051920431749533

[2020-01-27 14:14:09.534107][__main__.TRPOAgent.log][linesearch]: improvement: 0.017916276004945164

[2020-01-27 14:14:09.560274][__main__.TRPOAgent.log][linesearch]: improvement: 0.01111987575493334

[2020-01-27 14:14:09.583927][__main__.TRPOAgent.log][linesearch]: improvement: 0.007011855862714356

[2020-01-27 14:14:09.611273][__main__.TRPOAgent.log][linesearch]: improvement: 0.0042758852552595705

[2020-01-27 14:14:09.636464][__main__.TRPOAgent.log][linesearch]: improvement: 0.002531361462373649

[2020-01-27 14:14:09.636908][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.184962609951019e-07, Discarded policy loss value: -2.890805150215598

[2020-01-27 14:14:10.374483][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.18341719320335

[2020-01-27 14:14:10.379825][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:14:13.926283][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00331057 -0.00290138 -0.05643761 ... -0.12541697  0.13157808
 -0.00616111]

[2020-01-27 14:14:13.926668][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:14:14.322781][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.68349683 -1.00451817 -0.78033558 ... -0.42905889 -0.9223577
  1.35141659], shape=(4547,), dtype=float64)

[2020-01-27 14:14:14.408185][__main__.TRPOAgent.log][linesearch]: improvement: -0.13298430813439863

[2020-01-27 14:14:14.435610][__main__.TRPOAgent.log][linesearch]: improvement: -0.008989824720408962

[2020-01-27 14:14:14.463558][__main__.TRPOAgent.log][linesearch]: improvement: 0.017609515685982846

[2020-01-27 14:14:14.464000][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.1525788243612274

[2020-01-27 14:14:15.192626][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 279.74341440459517

[2020-01-27 14:14:15.193029][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:14:15.200386][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:14:18.725490][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00351833  0.00076743 -0.12621815 ...  0.01569499  0.34008719
 -0.35578218]

[2020-01-27 14:14:18.725877][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:14:19.129912][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.09994633  0.56178072 -0.16349343 ...  0.07793386 -0.58502477
  0.50709091], shape=(4547,), dtype=float64)

[2020-01-27 14:14:19.210744][__main__.TRPOAgent.log][linesearch]: improvement: 0.24330608586933655

[2020-01-27 14:14:19.235275][__main__.TRPOAgent.log][linesearch]: improvement: 0.13550127791199063

[2020-01-27 14:14:19.235718][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.6173823235265752

[2020-01-27 14:14:19.970430][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 270.72799382620366

[2020-01-27 14:14:19.970821][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:14:19.977850][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:14:23.506244][__main__.TRPOAgent.log][training]: policy_gradient: [-0.01646037  0.00771418 -0.0907226  ... -0.07893592  0.19118935
 -0.11225344]

[2020-01-27 14:14:23.506625][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:14:23.903834][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.73222357 -0.97604201 -4.50525442 ... -0.79708047 -2.28433051
  3.08141098], shape=(4547,), dtype=float64)

[2020-01-27 14:14:23.979770][__main__.TRPOAgent.log][linesearch]: improvement: 0.22529816342665313

[2020-01-27 14:14:24.002375][__main__.TRPOAgent.log][linesearch]: improvement: 0.16046948713760237

[2020-01-27 14:14:24.030109][__main__.TRPOAgent.log][linesearch]: improvement: 0.11107232432749939

[2020-01-27 14:14:24.054304][__main__.TRPOAgent.log][linesearch]: improvement: 0.08154487083176898

[2020-01-27 14:14:24.080773][__main__.TRPOAgent.log][linesearch]: improvement: 0.05695191376361716

[2020-01-27 14:14:24.104201][__main__.TRPOAgent.log][linesearch]: improvement: 0.03954090155988463

[2020-01-27 14:14:24.133856][__main__.TRPOAgent.log][linesearch]: improvement: 0.02733956373329824

[2020-01-27 14:14:24.157241][__main__.TRPOAgent.log][linesearch]: improvement: 0.017750173448668027

[2020-01-27 14:14:24.183710][__main__.TRPOAgent.log][linesearch]: improvement: 0.010568607889910098

[2020-01-27 14:14:24.208244][__main__.TRPOAgent.log][linesearch]: improvement: 0.0059251886876589

[2020-01-27 14:14:24.208808][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.033528945089512e-07, Discarded policy loss value: -0.13365798162187478

[2020-01-27 14:14:24.956762][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.63183494527203

[2020-01-27 14:14:24.978643][__main__.TRPOAgent.log][learning]: Episode #2

[2020-01-27 14:14:24.979238][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:14:25.020526][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:14:25.021098][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:14:25.021248][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:14:25.023518][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:14:28.473559][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:14:28.487683][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:14:28.488174][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:14:28.488856][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:14:28.488763][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:14:28.489745][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:14:31.869144][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:14:31.918539][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:14:31.919078][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:14:31.919655][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:14:31.919739][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:14:31.923509][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:14:35.402687][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:14:35.414958][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:14:35.415540][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:14:35.416140][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:14:35.416209][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:14:35.418225][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:14:39.191060][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:14:39.225490][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:14:39.225997][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:14:39.226494][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:14:39.226552][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:14:39.228126][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:14:42.862436][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:14:42.865774][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:14:42.866657][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:14:42.867141][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:14:42.867079][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:14:42.868120][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:14:46.823250][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:14:46.902807][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:14:46.903441][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:14:46.904000][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:14:46.904103][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:14:46.905982][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:14:50.657174][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:14:50.662646][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:14:50.663102][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:14:50.663672][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:14:50.663739][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:14:50.665429][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:14:54.203896][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:14:54.207641][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:14:54.208116][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:14:54.208606][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:14:54.208659][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:14:54.210770][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:14:57.904683][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:14:57.963377][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:14:57.963841][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:14:57.964495][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:14:57.964572][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:14:57.968781][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:15:01.653285][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:15:01.677520][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:15:01.678050][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:15:01.678770][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:15:01.678696][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:15:01.679833][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:15:05.031092][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:15:05.072108][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:15:05.072606][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:15:05.073438][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:15:05.073255][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:15:05.074315][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:15:08.507883][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:15:08.545016][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:15:08.545576][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:15:08.546120][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:15:08.546254][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:15:08.548617][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:15:12.163154][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:15:12.178642][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:15:12.179218][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:15:12.180133][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:15:12.179948][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:15:12.181123][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:15:16.139410][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:15:16.197768][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:15:16.198223][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:15:16.198962][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:15:16.199880][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:15:16.198888][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:15:19.662225][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:15:19.676817][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:15:19.677325][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:15:19.687822][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:15:20.171641][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:15:20.199181][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:15:20.200695][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:15:20.201111][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:15:20.232262][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:15:23.943868][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00905254  0.00338622  0.04038888 ... -0.08129549 -0.02839368
  0.10968918]

[2020-01-27 14:15:23.944267][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:15:24.341541][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.54091856  0.91584242 -0.0826176  ... -0.51638973  0.7422118
 -0.22582207], shape=(4547,), dtype=float64)

[2020-01-27 14:15:24.420098][__main__.TRPOAgent.log][linesearch]: improvement: -0.033525686179680214

[2020-01-27 14:15:24.446370][__main__.TRPOAgent.log][linesearch]: improvement: -0.02033890207617306

[2020-01-27 14:15:24.470820][__main__.TRPOAgent.log][linesearch]: improvement: -0.003347285225468344

[2020-01-27 14:15:24.497692][__main__.TRPOAgent.log][linesearch]: improvement: 0.004205632943503623

[2020-01-27 14:15:24.522415][__main__.TRPOAgent.log][linesearch]: improvement: 0.00586811188463221

[2020-01-27 14:15:24.552674][__main__.TRPOAgent.log][linesearch]: improvement: 0.00431346669603494

[2020-01-27 14:15:24.577991][__main__.TRPOAgent.log][linesearch]: improvement: 0.0030913586874827814

[2020-01-27 14:15:24.602281][__main__.TRPOAgent.log][linesearch]: improvement: 0.0022544605628125325

[2020-01-27 14:15:24.631195][__main__.TRPOAgent.log][linesearch]: improvement: 0.0015287528097180925

[2020-01-27 14:15:24.654751][__main__.TRPOAgent.log][linesearch]: improvement: 0.0010183615461727413

[2020-01-27 14:15:24.655184][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.952751994053903e-07, Discarded policy loss value: -3.080962471799482

[2020-01-27 14:15:25.424826][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 290.8160722274767

[2020-01-27 14:15:25.429858][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:15:29.127054][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00652477 -0.00647817 -0.04768027 ...  0.00845678  0.22192459
 -0.23038137]

[2020-01-27 14:15:29.127490][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:15:29.548629][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.8797397   1.81332703  2.0909518  ...  1.70502988  0.25172917
 -1.95675905], shape=(4547,), dtype=float64)

[2020-01-27 14:15:29.627147][__main__.TRPOAgent.log][linesearch]: improvement: 0.2937526394801555

[2020-01-27 14:15:29.654156][__main__.TRPOAgent.log][linesearch]: improvement: 0.1724188088465617

[2020-01-27 14:15:29.681661][__main__.TRPOAgent.log][linesearch]: improvement: 0.09688343516979181

[2020-01-27 14:15:29.682146][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 4.384725941075472

[2020-01-27 14:15:30.399983][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.24322380075336

[2020-01-27 14:15:30.400376][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:15:30.407335][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:15:33.906255][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00606067  0.0589981   0.02777766 ...  0.16484372 -0.16748117
  0.00263746]

[2020-01-27 14:15:33.906661][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:15:34.302038][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.73965144  3.61855359  1.28472909 ...  0.88632331  0.24412971
 -1.13045301], shape=(4547,), dtype=float64)

[2020-01-27 14:15:34.384970][__main__.TRPOAgent.log][linesearch]: improvement: 0.008613970082765654

[2020-01-27 14:15:34.408964][__main__.TRPOAgent.log][linesearch]: improvement: 0.007911686126715889

[2020-01-27 14:15:34.434564][__main__.TRPOAgent.log][linesearch]: improvement: 0.008303544680538621

[2020-01-27 14:15:34.466502][__main__.TRPOAgent.log][linesearch]: improvement: 0.009044230689014832

[2020-01-27 14:15:34.489980][__main__.TRPOAgent.log][linesearch]: improvement: 0.00724602121073481

[2020-01-27 14:15:34.519246][__main__.TRPOAgent.log][linesearch]: improvement: 0.005787063706605533

[2020-01-27 14:15:34.543843][__main__.TRPOAgent.log][linesearch]: improvement: 0.004661651249594723

[2020-01-27 14:15:34.568608][__main__.TRPOAgent.log][linesearch]: improvement: 0.0034395521191461498

[2020-01-27 14:15:34.597232][__main__.TRPOAgent.log][linesearch]: improvement: 0.0023350388593756932

[2020-01-27 14:15:34.620610][__main__.TRPOAgent.log][linesearch]: improvement: 0.001503515175999226

[2020-01-27 14:15:34.621181][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.623178323885548e-07, Discarded policy loss value: -2.966521930590717

[2020-01-27 14:15:35.373008][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.660439837893

[2020-01-27 14:15:35.378109][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:15:39.054766][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00434127 -0.03131825 -0.03172418 ... -0.10851591  0.16905467
 -0.06053876]

[2020-01-27 14:15:39.055169][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:15:39.452136][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.06896898 -1.8985841  -0.11728785 ...  0.50706244 -0.70100828
  0.19394583], shape=(4547,), dtype=float64)

[2020-01-27 14:15:39.528207][__main__.TRPOAgent.log][linesearch]: improvement: -0.07805369485516694

[2020-01-27 14:15:39.555353][__main__.TRPOAgent.log][linesearch]: improvement: -0.007216954759787397

[2020-01-27 14:15:39.555808][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.2592518550227742

[2020-01-27 14:15:40.305831][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.20892058088043

[2020-01-27 14:15:40.306224][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:15:40.314011][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:15:44.168249][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.04438088  0.04628771  0.05250239 ... -0.07007019 -0.18096073
  0.25103092]

[2020-01-27 14:15:44.168835][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:15:44.588426][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.31951119  0.53204851 -0.01631098 ...  1.28647429 -0.32951256
 -0.95696173], shape=(4547,), dtype=float64)

[2020-01-27 14:15:44.670524][__main__.TRPOAgent.log][linesearch]: improvement: 0.04642316409250391

[2020-01-27 14:15:44.699236][__main__.TRPOAgent.log][linesearch]: improvement: 0.06750287599100724

[2020-01-27 14:15:44.724911][__main__.TRPOAgent.log][linesearch]: improvement: 0.05159904792021197

[2020-01-27 14:15:44.752610][__main__.TRPOAgent.log][linesearch]: improvement: 0.03440841322867105

[2020-01-27 14:15:44.775552][__main__.TRPOAgent.log][linesearch]: improvement: 0.022373532312296884

[2020-01-27 14:15:44.807413][__main__.TRPOAgent.log][linesearch]: improvement: 0.015019269899473109

[2020-01-27 14:15:44.832205][__main__.TRPOAgent.log][linesearch]: improvement: 0.00915519034950707

[2020-01-27 14:15:44.858514][__main__.TRPOAgent.log][linesearch]: improvement: 0.0057047393868290275

[2020-01-27 14:15:44.888042][__main__.TRPOAgent.log][linesearch]: improvement: 0.003622931709279298

[2020-01-27 14:15:44.914109][__main__.TRPOAgent.log][linesearch]: improvement: 0.0023504631290349742

[2020-01-27 14:15:44.914579][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 5.830639619183015e-07, Discarded policy loss value: -1.2111759137514728

[2020-01-27 14:15:45.697828][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 290.2053393986962

[2020-01-27 14:15:45.703599][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:15:50.091584][__main__.TRPOAgent.log][training]: policy_gradient: [-0.02708376 -0.02373501 -0.04104461 ...  0.03736074  0.12411705
 -0.1614778 ]

[2020-01-27 14:15:50.092140][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:15:50.499444][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.41752786 -0.35604714 -0.34733805 ...  0.06735637 -0.04701857
 -0.0203378 ], shape=(4547,), dtype=float64)

[2020-01-27 14:15:50.583531][__main__.TRPOAgent.log][linesearch]: improvement: 0.12845080251011312

[2020-01-27 14:15:50.610150][__main__.TRPOAgent.log][linesearch]: improvement: 0.09330183522943761

[2020-01-27 14:15:50.639883][__main__.TRPOAgent.log][linesearch]: improvement: 0.06477088366090075

[2020-01-27 14:15:50.667879][__main__.TRPOAgent.log][linesearch]: improvement: 0.04253873243536346

[2020-01-27 14:15:50.693606][__main__.TRPOAgent.log][linesearch]: improvement: 0.026834649507488095

[2020-01-27 14:15:50.723177][__main__.TRPOAgent.log][linesearch]: improvement: 0.016969757074191072

[2020-01-27 14:15:50.749755][__main__.TRPOAgent.log][linesearch]: improvement: 0.009898118633076186

[2020-01-27 14:15:50.777026][__main__.TRPOAgent.log][linesearch]: improvement: 0.005743183959363796

[2020-01-27 14:15:50.804931][__main__.TRPOAgent.log][linesearch]: improvement: 0.003315250393331054

[2020-01-27 14:15:50.833702][__main__.TRPOAgent.log][linesearch]: improvement: 0.002201697701194355

[2020-01-27 14:15:50.834182][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.990187474596228e-07, Discarded policy loss value: -0.19218533303464452

[2020-01-27 14:15:51.757194][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.3856671357589

[2020-01-27 14:15:51.762282][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:15:55.986841][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.02179939  0.04314331 -0.01024518 ... -0.14879561  0.07316889
  0.07562672]

[2020-01-27 14:15:55.987234][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:15:56.369966][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.48646698 -0.3586816  -1.13781216 ... -0.03552628 -0.6252182
  0.66074448], shape=(4547,), dtype=float64)

[2020-01-27 14:15:56.452072][__main__.TRPOAgent.log][linesearch]: improvement: 0.1504792587316819

[2020-01-27 14:15:56.478866][__main__.TRPOAgent.log][linesearch]: improvement: 0.09502290826777449

[2020-01-27 14:15:56.505063][__main__.TRPOAgent.log][linesearch]: improvement: 0.06291038968233731

[2020-01-27 14:15:56.530993][__main__.TRPOAgent.log][linesearch]: improvement: 0.03676566810360249

[2020-01-27 14:15:56.558792][__main__.TRPOAgent.log][linesearch]: improvement: 0.018976517568390805

[2020-01-27 14:15:56.584289][__main__.TRPOAgent.log][linesearch]: improvement: 0.010799552415362235

[2020-01-27 14:15:56.608578][__main__.TRPOAgent.log][linesearch]: improvement: 0.006272177980272464

[2020-01-27 14:15:56.635291][__main__.TRPOAgent.log][linesearch]: improvement: 0.004048199892431992

[2020-01-27 14:15:56.661632][__main__.TRPOAgent.log][linesearch]: improvement: 0.00244728695744012

[2020-01-27 14:15:56.685635][__main__.TRPOAgent.log][linesearch]: improvement: 0.0015118890699105103

[2020-01-27 14:15:56.686200][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.972394219665141e-07, Discarded policy loss value: -0.6370797457864082

[2020-01-27 14:15:57.434595][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.63414841537076

[2020-01-27 14:15:57.441561][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:16:00.999887][__main__.TRPOAgent.log][training]: policy_gradient: [-0.0680032  -0.06209266 -0.09755051 ...  0.21228898  0.19419856
 -0.40648754]

[2020-01-27 14:16:01.000272][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:16:01.394098][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.4625322  -0.68272449 -0.23169223 ...  0.15260383  0.14343955
 -0.29604338], shape=(4547,), dtype=float64)

[2020-01-27 14:16:01.470580][__main__.TRPOAgent.log][linesearch]: improvement: 0.10223435009598347

[2020-01-27 14:16:01.495921][__main__.TRPOAgent.log][linesearch]: improvement: 0.08003465173699276

[2020-01-27 14:16:01.496568][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 4.083132642537947

[2020-01-27 14:16:02.216942][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.8850111684293

[2020-01-27 14:16:02.217345][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:16:02.228401][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:16:05.708591][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00636244 -0.00264934  0.02846821 ... -0.0418563  -0.03214895
  0.07400525]

[2020-01-27 14:16:05.708991][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:16:06.090528][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.11219026 -0.90491842  0.24146089 ... -0.76281124  0.95185995
 -0.18904871], shape=(4547,), dtype=float64)

[2020-01-27 14:16:06.169036][__main__.TRPOAgent.log][linesearch]: improvement: -0.027982715494597743

[2020-01-27 14:16:06.196901][__main__.TRPOAgent.log][linesearch]: improvement: -0.020589916195222102

[2020-01-27 14:16:06.221905][__main__.TRPOAgent.log][linesearch]: improvement: -0.009903096184390003

[2020-01-27 14:16:06.246279][__main__.TRPOAgent.log][linesearch]: improvement: 0.007244984033136337

[2020-01-27 14:16:06.275393][__main__.TRPOAgent.log][linesearch]: improvement: 0.009385156768075609

[2020-01-27 14:16:06.305052][__main__.TRPOAgent.log][linesearch]: improvement: 0.0074946657915822

[2020-01-27 14:16:06.329066][__main__.TRPOAgent.log][linesearch]: improvement: 0.004613115364950904

[2020-01-27 14:16:06.354832][__main__.TRPOAgent.log][linesearch]: improvement: 0.0025610554627444326

[2020-01-27 14:16:06.379785][__main__.TRPOAgent.log][linesearch]: improvement: 0.0017138101810629802

[2020-01-27 14:16:06.405512][__main__.TRPOAgent.log][linesearch]: improvement: 0.0012141864832460048

[2020-01-27 14:16:06.406011][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.837804746607664e-07, Discarded policy loss value: -1.3084128125585786

[2020-01-27 14:16:07.119673][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 290.3981064489945

[2020-01-27 14:16:07.124566][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:16:10.755597][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.04711191  0.02880793  0.06615001 ... -0.00565826 -0.15779934
  0.1634576 ]

[2020-01-27 14:16:10.755982][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:16:11.138152][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.54491047 -1.75842094  4.59510805 ...  2.15427628  0.9592993
 -3.11357558], shape=(4547,), dtype=float64)

[2020-01-27 14:16:11.216624][__main__.TRPOAgent.log][linesearch]: improvement: 0.19974504520586983

[2020-01-27 14:16:11.244890][__main__.TRPOAgent.log][linesearch]: improvement: 0.11931069547566303

[2020-01-27 14:16:11.268471][__main__.TRPOAgent.log][linesearch]: improvement: 0.06413577641048726

[2020-01-27 14:16:11.295731][__main__.TRPOAgent.log][linesearch]: improvement: 0.03709931068902833

[2020-01-27 14:16:11.323015][__main__.TRPOAgent.log][linesearch]: improvement: 0.02278081528983178

[2020-01-27 14:16:11.349553][__main__.TRPOAgent.log][linesearch]: improvement: 0.014192887252309633

[2020-01-27 14:16:11.376696][__main__.TRPOAgent.log][linesearch]: improvement: 0.009118805756050258

[2020-01-27 14:16:11.401230][__main__.TRPOAgent.log][linesearch]: improvement: 0.005439122156010029

[2020-01-27 14:16:11.430779][__main__.TRPOAgent.log][linesearch]: improvement: 0.0033842558084997976

[2020-01-27 14:16:11.456443][__main__.TRPOAgent.log][linesearch]: improvement: 0.002269926453305926

[2020-01-27 14:16:11.457008][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.096869361862942e-07, Discarded policy loss value: -0.13936567987699688

[2020-01-27 14:16:12.202710][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.580928926037

[2020-01-27 14:16:12.219616][__main__.TRPOAgent.log][learning]: Episode #3

[2020-01-27 14:16:12.220179][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:16:12.259496][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:16:12.260115][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:16:12.260266][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:16:12.262347][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:16:15.679537][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:16:15.738533][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:16:15.739091][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:16:15.739696][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:16:15.739634][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:16:15.740901][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:16:20.118196][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:16:20.246506][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:16:20.247156][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:16:20.247967][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:16:20.248066][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:16:20.250927][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:16:24.194671][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:16:24.204259][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:16:24.204790][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:16:24.205398][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:16:24.205337][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:16:24.206472][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:16:27.590430][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:16:27.604690][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:16:27.605245][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:16:27.605749][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:16:27.605819][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:16:27.607785][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:16:31.030297][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:16:31.077044][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:16:31.077530][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:16:31.078203][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:16:31.078134][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:16:31.080053][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:16:34.420361][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:16:34.426154][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:16:34.426842][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:16:34.427373][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:16:34.427447][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:16:34.431016][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:16:37.743722][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:16:37.798574][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:16:37.799135][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:16:37.799773][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:16:37.799850][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:16:37.801735][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:16:41.119777][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:16:41.136780][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:16:41.137478][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:16:41.138116][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:16:41.138225][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:16:41.140290][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:16:44.654204][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:16:44.664581][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:16:44.665097][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:16:44.665956][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:16:44.666028][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:16:44.670191][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:16:48.566934][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:16:48.591277][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:16:48.591861][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:16:48.592612][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:16:48.592529][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:16:48.593813][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:16:52.109209][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:16:52.176653][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:16:52.177194][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:16:52.177672][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:16:52.177732][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:16:52.179749][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:16:55.719094][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:16:55.723251][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:16:55.723759][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:16:55.724216][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:16:55.724280][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:16:55.726033][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:16:59.294126][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:16:59.295940][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:16:59.296880][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:16:59.297408][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:16:59.297504][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:16:59.299879][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:17:02.706839][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:17:02.720084][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:17:02.720667][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:17:02.721394][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:17:02.721522][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:17:02.723350][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:17:06.893487][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:17:06.928266][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:17:06.928814][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:17:06.941162][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:17:07.515709][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:17:07.545055][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:17:07.546592][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:17:07.547024][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:17:07.578517][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:17:12.261906][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01562498  0.0212379  -0.00746191 ... -0.04662565 -0.02034028
  0.06696593]

[2020-01-27 14:17:12.262314][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:12.696761][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.32315272 -1.8535575  -0.21762096 ... -0.51307107 -0.77238537
  1.28545644], shape=(4547,), dtype=float64)

[2020-01-27 14:17:12.789504][__main__.TRPOAgent.log][linesearch]: improvement: 0.09128582434752873

[2020-01-27 14:17:12.826472][__main__.TRPOAgent.log][linesearch]: improvement: 0.10196546229994964

[2020-01-27 14:17:12.859878][__main__.TRPOAgent.log][linesearch]: improvement: 0.07196795036268949

[2020-01-27 14:17:12.891546][__main__.TRPOAgent.log][linesearch]: improvement: 0.04433984404013627

[2020-01-27 14:17:12.920758][__main__.TRPOAgent.log][linesearch]: improvement: 0.02712931219990239

[2020-01-27 14:17:12.921361][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 4, New policy loss value: 0.1715027397665779

[2020-01-27 14:17:13.710096][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.80775961645935

[2020-01-27 14:17:13.710508][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:17:13.718287][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:17:17.749453][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00093464 -0.00952676 -0.02670259 ...  0.17059817 -0.07611558
 -0.09448259]

[2020-01-27 14:17:17.749834][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:18.138851][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.53675437  0.22959394  0.3351708  ...  0.59159765 -0.11740409
 -0.47419357], shape=(4547,), dtype=float64)

[2020-01-27 14:17:18.218765][__main__.TRPOAgent.log][linesearch]: improvement: 0.11288810939486515

[2020-01-27 14:17:18.245829][__main__.TRPOAgent.log][linesearch]: improvement: 0.06441111910587782

[2020-01-27 14:17:18.273833][__main__.TRPOAgent.log][linesearch]: improvement: 0.038103736084944995

[2020-01-27 14:17:18.299621][__main__.TRPOAgent.log][linesearch]: improvement: 0.023479041161978853

[2020-01-27 14:17:18.324861][__main__.TRPOAgent.log][linesearch]: improvement: 0.015057677474259568

[2020-01-27 14:17:18.347919][__main__.TRPOAgent.log][linesearch]: improvement: 0.010256859973762134

[2020-01-27 14:17:18.375961][__main__.TRPOAgent.log][linesearch]: improvement: 0.006953674973042556

[2020-01-27 14:17:18.400138][__main__.TRPOAgent.log][linesearch]: improvement: 0.004261055370477385

[2020-01-27 14:17:18.429162][__main__.TRPOAgent.log][linesearch]: improvement: 0.0025765774519672657

[2020-01-27 14:17:18.459303][__main__.TRPOAgent.log][linesearch]: improvement: 0.0015435072171082753

[2020-01-27 14:17:18.459883][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.89278878102201e-07, Discarded policy loss value: -0.5751911094166089

[2020-01-27 14:17:19.209008][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.31546332274456

[2020-01-27 14:17:19.214194][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:17:24.042923][__main__.TRPOAgent.log][training]: policy_gradient: [-0.02051056 -0.02430405 -0.02205374 ...  0.14002564  0.01012176
 -0.1501474 ]

[2020-01-27 14:17:24.043381][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:24.442814][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.91020267 -1.00927734  0.45496565 ...  0.41779074  0.02677566
 -0.4445664 ], shape=(4547,), dtype=float64)

[2020-01-27 14:17:24.525163][__main__.TRPOAgent.log][linesearch]: improvement: -0.011946669314774505

[2020-01-27 14:17:24.550197][__main__.TRPOAgent.log][linesearch]: improvement: 0.012875363628524594

[2020-01-27 14:17:24.550637][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.1270046291484146

[2020-01-27 14:17:25.296075][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.2844689696178

[2020-01-27 14:17:25.296485][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:17:25.304098][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:17:28.737057][__main__.TRPOAgent.log][training]: policy_gradient: [-0.0191788   0.08507943  0.00617078 ... -0.34953216  0.23776598
  0.11176618]

[2020-01-27 14:17:28.737467][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:29.119337][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.06793158  0.51352436 -0.39089951 ...  0.6175809  -1.30758317
  0.69000227], shape=(4547,), dtype=float64)

[2020-01-27 14:17:29.196140][__main__.TRPOAgent.log][linesearch]: improvement: 0.2705909580577863

[2020-01-27 14:17:29.219956][__main__.TRPOAgent.log][linesearch]: improvement: 0.1534835368305809

[2020-01-27 14:17:29.248324][__main__.TRPOAgent.log][linesearch]: improvement: 0.09088250951471188

[2020-01-27 14:17:29.248780][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 2.487712209927731

[2020-01-27 14:17:29.968775][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 289.75502270450335

[2020-01-27 14:17:29.969178][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:17:29.976360][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:17:33.699905][__main__.TRPOAgent.log][training]: policy_gradient: [ 3.41040374e-04  8.29804551e-02 -9.66516272e-02 ... -2.14778349e-01
  4.28438293e-01 -2.13659944e-01]

[2020-01-27 14:17:33.700292][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:34.130931][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.80753032 -0.56403156  1.19158216 ... -0.14586118  0.91643487
 -0.77057369], shape=(4547,), dtype=float64)

[2020-01-27 14:17:34.224265][__main__.TRPOAgent.log][linesearch]: improvement: 0.1667114181109588

[2020-01-27 14:17:34.248527][__main__.TRPOAgent.log][linesearch]: improvement: 0.10305859232795567

[2020-01-27 14:17:34.249008][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.456928288037203

[2020-01-27 14:17:35.001177][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.2946228071265

[2020-01-27 14:17:35.001577][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:17:35.009199][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:17:38.677915][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00644171  0.04100108 -0.02198527 ... -0.06071604  0.06636778
 -0.00565173]

[2020-01-27 14:17:38.678310][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:39.107006][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.35141381  0.44915893 -0.51492164 ...  0.19840296 -2.01303508
  1.81463212], shape=(4547,), dtype=float64)

[2020-01-27 14:17:39.187243][__main__.TRPOAgent.log][linesearch]: improvement: 0.10076226082237072

[2020-01-27 14:17:39.215561][__main__.TRPOAgent.log][linesearch]: improvement: 0.08772255158468462

[2020-01-27 14:17:39.241431][__main__.TRPOAgent.log][linesearch]: improvement: 0.06617363891045525

[2020-01-27 14:17:39.268419][__main__.TRPOAgent.log][linesearch]: improvement: 0.046274966026344067

[2020-01-27 14:17:39.295723][__main__.TRPOAgent.log][linesearch]: improvement: 0.030162805299862205

[2020-01-27 14:17:39.319137][__main__.TRPOAgent.log][linesearch]: improvement: 0.01858457049739587

[2020-01-27 14:17:39.346920][__main__.TRPOAgent.log][linesearch]: improvement: 0.011263597688048521

[2020-01-27 14:17:39.370812][__main__.TRPOAgent.log][linesearch]: improvement: 0.006786339814666809

[2020-01-27 14:17:39.398068][__main__.TRPOAgent.log][linesearch]: improvement: 0.004115551010839935

[2020-01-27 14:17:39.422709][__main__.TRPOAgent.log][linesearch]: improvement: 0.0024843248556990494

[2020-01-27 14:17:39.423150][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.499888834681625e-07, Discarded policy loss value: -0.2578421226644621

[2020-01-27 14:17:40.157573][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.7328843106344

[2020-01-27 14:17:40.162997][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:17:43.847889][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00116176  0.01887063  0.05283576 ... -0.20742839 -0.00665792
  0.21408631]

[2020-01-27 14:17:43.848291][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:44.236234][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.03048859  0.44292326  0.37576361 ...  0.65351586 -0.04459765
 -0.60891821], shape=(4547,), dtype=float64)

[2020-01-27 14:17:44.313216][__main__.TRPOAgent.log][linesearch]: improvement: 0.2075163447697035

[2020-01-27 14:17:44.339097][__main__.TRPOAgent.log][linesearch]: improvement: 0.11140437865821728

[2020-01-27 14:17:44.367484][__main__.TRPOAgent.log][linesearch]: improvement: 0.06282206883961106

[2020-01-27 14:17:44.392194][__main__.TRPOAgent.log][linesearch]: improvement: 0.0364623812703706

[2020-01-27 14:17:44.420601][__main__.TRPOAgent.log][linesearch]: improvement: 0.022299655389616424

[2020-01-27 14:17:44.446424][__main__.TRPOAgent.log][linesearch]: improvement: 0.013328716282555941

[2020-01-27 14:17:44.471163][__main__.TRPOAgent.log][linesearch]: improvement: 0.00790044320138139

[2020-01-27 14:17:44.498308][__main__.TRPOAgent.log][linesearch]: improvement: 0.00460483888715757

[2020-01-27 14:17:44.523530][__main__.TRPOAgent.log][linesearch]: improvement: 0.003053977689709164

[2020-01-27 14:17:44.553408][__main__.TRPOAgent.log][linesearch]: improvement: 0.002094622970061133

[2020-01-27 14:17:44.553854][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.517494202128387e-07, Discarded policy loss value: -1.0964895671936197

[2020-01-27 14:17:45.312701][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 291.5787058554071

[2020-01-27 14:17:45.317929][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:17:48.847940][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00558618 -0.02204961  0.08581197 ...  0.02719174 -0.23985319
  0.21266145]

[2020-01-27 14:17:48.848322][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:49.280431][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.26714569  0.09235699 -0.82180655 ...  0.65792412 -2.05921637
  1.40129224], shape=(4547,), dtype=float64)

[2020-01-27 14:17:49.360097][__main__.TRPOAgent.log][linesearch]: improvement: -0.05395161330694709

[2020-01-27 14:17:49.388668][__main__.TRPOAgent.log][linesearch]: improvement: 0.027986450323647993

[2020-01-27 14:17:49.415032][__main__.TRPOAgent.log][linesearch]: improvement: 0.048541888544732625

[2020-01-27 14:17:49.440248][__main__.TRPOAgent.log][linesearch]: improvement: 0.04266844468092512

[2020-01-27 14:17:49.468062][__main__.TRPOAgent.log][linesearch]: improvement: 0.0343735405866874

[2020-01-27 14:17:49.493603][__main__.TRPOAgent.log][linesearch]: improvement: 0.027574064661636033

[2020-01-27 14:17:49.520892][__main__.TRPOAgent.log][linesearch]: improvement: 0.019915255864378256

[2020-01-27 14:17:49.546604][__main__.TRPOAgent.log][linesearch]: improvement: 0.013380842456045317

[2020-01-27 14:17:49.572818][__main__.TRPOAgent.log][linesearch]: improvement: 0.008424837182296585

[2020-01-27 14:17:49.597915][__main__.TRPOAgent.log][linesearch]: improvement: 0.0051967544080062655

[2020-01-27 14:17:49.598467][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.991016071433647e-07, Discarded policy loss value: -1.6343385599303166

[2020-01-27 14:17:50.345515][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.4400888516517

[2020-01-27 14:17:50.350737][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:17:53.841562][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00960145  0.05998262 -0.0342021  ... -0.2720472   0.30896631
 -0.03691912]

[2020-01-27 14:17:53.841961][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:54.247311][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.13337705  0.71386071 -0.58091361 ...  0.05039603 -0.73304834
  0.68265231], shape=(4547,), dtype=float64)

[2020-01-27 14:17:54.329573][__main__.TRPOAgent.log][linesearch]: improvement: -0.07974644810409315

[2020-01-27 14:17:54.330117][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.7073313808536895

[2020-01-27 14:17:55.072766][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.0468521778824

[2020-01-27 14:17:55.073182][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:17:55.084871][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:17:58.577514][__main__.TRPOAgent.log][training]: policy_gradient: [-0.01875426 -0.01251018  0.03258608 ...  0.17021123 -0.14576662
 -0.02444461]

[2020-01-27 14:17:58.577906][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:17:58.970638][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.17605605  0.3498654   0.13773495 ...  0.4941596  -0.00350132
 -0.49065828], shape=(4547,), dtype=float64)

[2020-01-27 14:17:59.052579][__main__.TRPOAgent.log][linesearch]: improvement: -0.10246083937820727

[2020-01-27 14:17:59.079250][__main__.TRPOAgent.log][linesearch]: improvement: -0.04390712391420537

[2020-01-27 14:17:59.107979][__main__.TRPOAgent.log][linesearch]: improvement: -0.00027039221553359916

[2020-01-27 14:17:59.133301][__main__.TRPOAgent.log][linesearch]: improvement: 0.017774853249821643

[2020-01-27 14:17:59.159117][__main__.TRPOAgent.log][linesearch]: improvement: 0.01682996350628796

[2020-01-27 14:17:59.186893][__main__.TRPOAgent.log][linesearch]: improvement: 0.010943025008527063

[2020-01-27 14:17:59.215243][__main__.TRPOAgent.log][linesearch]: improvement: 0.007191533560066077

[2020-01-27 14:17:59.241054][__main__.TRPOAgent.log][linesearch]: improvement: 0.0044587770588712106

[2020-01-27 14:17:59.267816][__main__.TRPOAgent.log][linesearch]: improvement: 0.0024512475210590745

[2020-01-27 14:17:59.293612][__main__.TRPOAgent.log][linesearch]: improvement: 0.0015861454406181918

[2020-01-27 14:17:59.294081][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.146254594713023e-07, Discarded policy loss value: -0.9634070444998508

[2020-01-27 14:18:00.017376][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.5199651236616

[2020-01-27 14:18:00.036339][__main__.TRPOAgent.log][learning]: Episode #4

[2020-01-27 14:18:00.036710][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:18:00.075579][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:18:00.076394][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:18:00.076121][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:18:00.077273][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:18:03.581277][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:18:03.580712][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:18:03.582276][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:18:03.583058][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:18:03.583128][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:18:03.584932][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:18:06.949757][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:18:06.997561][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:18:06.998052][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:18:06.998543][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:18:06.998680][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:18:07.000936][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:18:10.308062][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:18:10.360885][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:18:10.361358][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:18:10.361858][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:18:10.361917][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:18:10.363823][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:18:13.709067][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:18:13.737084][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:18:13.737691][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:18:13.738561][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:18:13.739525][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:18:13.738464][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:18:17.517317][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:18:17.655582][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:18:17.656115][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:18:17.656829][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:18:17.656921][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:18:17.658876][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:18:21.514975][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:18:21.527602][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:18:21.528450][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:18:21.529152][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:18:21.529228][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:18:21.531548][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:18:25.093873][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:18:25.145060][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:18:25.145583][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:18:25.146128][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:18:25.146205][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:18:25.148113][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:18:28.563952][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:18:28.645734][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:18:28.646602][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:18:28.647228][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:18:28.647300][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:18:28.650936][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:18:32.950571][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:18:32.981701][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:18:32.982427][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:18:32.983187][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:18:32.983297][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:18:32.985427][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:18:36.563125][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:18:36.617686][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:18:36.618248][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:18:36.618697][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:18:36.618758][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:18:36.620921][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:18:40.658798][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:18:40.706236][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:18:40.706754][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:18:40.707387][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:18:40.707581][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:18:40.709490][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:18:44.276059][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:18:44.298438][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:18:44.298932][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:18:44.299516][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:18:44.299625][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:18:44.302343][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:18:47.788586][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:18:47.840407][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:18:47.840939][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:18:47.841578][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:18:47.841653][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:18:47.844701][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:18:51.267603][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:18:51.345164][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:18:51.345633][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:18:51.346140][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:18:51.346205][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:18:51.350051][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:18:55.207907][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:18:55.288793][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:18:55.289300][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:18:55.299875][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:18:55.797091][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:18:55.825319][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:18:55.826808][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:18:55.827201][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:18:55.859129][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:18:59.491019][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00066789  0.00294032  0.00306672 ... -0.01878162  0.02475274
 -0.00597112]

[2020-01-27 14:18:59.491413][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:18:59.885629][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.31621403  0.48759164 -0.12454526 ... -0.06299183  0.39513318
 -0.33214134], shape=(4547,), dtype=float64)

[2020-01-27 14:18:59.966270][__main__.TRPOAgent.log][linesearch]: improvement: 0.0567907681039963

[2020-01-27 14:18:59.992266][__main__.TRPOAgent.log][linesearch]: improvement: 0.052156124170391616

[2020-01-27 14:19:00.021862][__main__.TRPOAgent.log][linesearch]: improvement: 0.026164487773464584

[2020-01-27 14:19:00.045051][__main__.TRPOAgent.log][linesearch]: improvement: 0.011474166714700473

[2020-01-27 14:19:00.073254][__main__.TRPOAgent.log][linesearch]: improvement: 0.006400311014775895

[2020-01-27 14:19:00.098523][__main__.TRPOAgent.log][linesearch]: improvement: 0.003767355059662636

[2020-01-27 14:19:00.123314][__main__.TRPOAgent.log][linesearch]: improvement: 0.0021908823745775408

[2020-01-27 14:19:00.152075][__main__.TRPOAgent.log][linesearch]: improvement: 0.0013455636904846635

[2020-01-27 14:19:00.177512][__main__.TRPOAgent.log][linesearch]: improvement: 0.0014906633213476184

[2020-01-27 14:19:00.206349][__main__.TRPOAgent.log][linesearch]: improvement: 0.0013790162895175317

[2020-01-27 14:19:00.206798][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.952808566545107e-07, Discarded policy loss value: -1.6382582404929862

[2020-01-27 14:19:00.961843][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.6594404918314

[2020-01-27 14:19:00.968465][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:19:04.545747][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00860995 -0.00342079  0.06432881 ... -0.06311499 -0.13736237
  0.20047736]

[2020-01-27 14:19:04.546128][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:04.947360][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.29780013 -0.03612821  0.16676218 ... -0.25534896  0.29060053
 -0.03525157], shape=(4547,), dtype=float64)

[2020-01-27 14:19:05.031543][__main__.TRPOAgent.log][linesearch]: improvement: 0.23594954832014015

[2020-01-27 14:19:05.058657][__main__.TRPOAgent.log][linesearch]: improvement: 0.12668706087656156

[2020-01-27 14:19:05.084680][__main__.TRPOAgent.log][linesearch]: improvement: 0.036347982173738025

[2020-01-27 14:19:05.110184][__main__.TRPOAgent.log][linesearch]: improvement: 0.017225596059671666

[2020-01-27 14:19:05.138598][__main__.TRPOAgent.log][linesearch]: improvement: 0.010771235248323574

[2020-01-27 14:19:05.163795][__main__.TRPOAgent.log][linesearch]: improvement: 0.006855113219589537

[2020-01-27 14:19:05.190254][__main__.TRPOAgent.log][linesearch]: improvement: 0.004287510212882317

[2020-01-27 14:19:05.217143][__main__.TRPOAgent.log][linesearch]: improvement: 0.0029081973047269116

[2020-01-27 14:19:05.243837][__main__.TRPOAgent.log][linesearch]: improvement: 0.001987743881923354

[2020-01-27 14:19:05.267790][__main__.TRPOAgent.log][linesearch]: improvement: 0.0012904341662069996

[2020-01-27 14:19:05.268329][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.13756666965908e-07, Discarded policy loss value: -3.0569342422405232

[2020-01-27 14:19:05.999775][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.67091160579406

[2020-01-27 14:19:06.007600][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:19:09.617070][__main__.TRPOAgent.log][training]: policy_gradient: [-0.02161544 -0.01273828  0.06094348 ...  0.11756476 -0.21507748
  0.09751272]

[2020-01-27 14:19:09.617451][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:10.032026][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.17179189 -0.2246299   0.04048262 ... -0.08152701  0.01850608
  0.06302093], shape=(4547,), dtype=float64)

[2020-01-27 14:19:10.113486][__main__.TRPOAgent.log][linesearch]: improvement: 0.02814940744324157

[2020-01-27 14:19:10.141919][__main__.TRPOAgent.log][linesearch]: improvement: 0.017779698137468047

[2020-01-27 14:19:10.164492][__main__.TRPOAgent.log][linesearch]: improvement: 0.01556164934340365

[2020-01-27 14:19:10.191292][__main__.TRPOAgent.log][linesearch]: improvement: 0.012288003931080427

[2020-01-27 14:19:10.216774][__main__.TRPOAgent.log][linesearch]: improvement: 0.008372752671115036

[2020-01-27 14:19:10.242857][__main__.TRPOAgent.log][linesearch]: improvement: 0.006021519951252774

[2020-01-27 14:19:10.268705][__main__.TRPOAgent.log][linesearch]: improvement: 0.004023807267476531

[2020-01-27 14:19:10.294687][__main__.TRPOAgent.log][linesearch]: improvement: 0.0025401781070033547

[2020-01-27 14:19:10.319316][__main__.TRPOAgent.log][linesearch]: improvement: 0.001513030889527922

[2020-01-27 14:19:10.345835][__main__.TRPOAgent.log][linesearch]: improvement: 0.0009130897506737234

[2020-01-27 14:19:10.346273][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.025063690750272e-07, Discarded policy loss value: -1.738983284703891

[2020-01-27 14:19:11.098398][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.86067070331535

[2020-01-27 14:19:11.103751][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:19:14.656467][__main__.TRPOAgent.log][training]: policy_gradient: [-0.04176896 -0.02259523  0.13373507 ...  0.16225991 -0.416583
  0.25432309]

[2020-01-27 14:19:14.656879][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:15.048746][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.10138606 -0.2588868  -0.16493136 ... -1.08787383  0.18904742
  0.89882641], shape=(4547,), dtype=float64)

[2020-01-27 14:19:15.126920][__main__.TRPOAgent.log][linesearch]: improvement: 0.07450673688797238

[2020-01-27 14:19:15.159906][__main__.TRPOAgent.log][linesearch]: improvement: 0.02932556470755676

[2020-01-27 14:19:15.186922][__main__.TRPOAgent.log][linesearch]: improvement: 0.02002160749831594

[2020-01-27 14:19:15.212136][__main__.TRPOAgent.log][linesearch]: improvement: 0.02157161570575372

[2020-01-27 14:19:15.242639][__main__.TRPOAgent.log][linesearch]: improvement: 0.02100115301715677

[2020-01-27 14:19:15.267120][__main__.TRPOAgent.log][linesearch]: improvement: 0.015072225185627008

[2020-01-27 14:19:15.293774][__main__.TRPOAgent.log][linesearch]: improvement: 0.009488568856409252

[2020-01-27 14:19:15.320814][__main__.TRPOAgent.log][linesearch]: improvement: 0.005644221927012971

[2020-01-27 14:19:15.343921][__main__.TRPOAgent.log][linesearch]: improvement: 0.003327936618931826

[2020-01-27 14:19:15.371189][__main__.TRPOAgent.log][linesearch]: improvement: 0.0019990207269109384

[2020-01-27 14:19:15.371710][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.56879780275392e-07, Discarded policy loss value: -3.1122771413728776

[2020-01-27 14:19:16.100900][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.17925867531284

[2020-01-27 14:19:16.106278][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:19:19.661661][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00645972  0.00152777 -0.01297299 ... -0.11761682  0.09294399
  0.02467284]

[2020-01-27 14:19:19.664315][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:20.054873][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.66455746 -0.071422   -1.82944103 ... -2.40899761  0.31658624
  2.09241137], shape=(4547,), dtype=float64)

[2020-01-27 14:19:20.133443][__main__.TRPOAgent.log][linesearch]: improvement: -0.019249762261686135

[2020-01-27 14:19:20.161419][__main__.TRPOAgent.log][linesearch]: improvement: -0.027561353763978932

[2020-01-27 14:19:20.184889][__main__.TRPOAgent.log][linesearch]: improvement: -0.003177038703998228

[2020-01-27 14:19:20.210966][__main__.TRPOAgent.log][linesearch]: improvement: 0.03531703710448225

[2020-01-27 14:19:20.211409][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 3, New policy loss value: 0.07212704438642302

[2020-01-27 14:19:20.933669][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 288.08371708200144

[2020-01-27 14:19:20.934051][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:19:20.944893][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:19:24.457776][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.02536279  0.01781427 -0.0746391  ... -0.15842656  0.10653629
  0.05189027]

[2020-01-27 14:19:24.458165][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:24.855746][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.8395252   0.05758623 -2.39357863 ...  4.9675082  -4.83205929
 -0.13544891], shape=(4547,), dtype=float64)

[2020-01-27 14:19:24.934194][__main__.TRPOAgent.log][linesearch]: improvement: 0.3769848213681577

[2020-01-27 14:19:24.964531][__main__.TRPOAgent.log][linesearch]: improvement: 0.24189348855174653

[2020-01-27 14:19:24.964970][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.6962656654453545

[2020-01-27 14:19:25.714102][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.57043026031613

[2020-01-27 14:19:25.714525][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:19:25.722089][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:19:29.419345][__main__.TRPOAgent.log][training]: policy_gradient: [-0.04021475 -0.03677899  0.10373944 ...  0.1007765  -0.16800441
  0.06722792]

[2020-01-27 14:19:29.419735][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:29.812357][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.65176898 -1.18705306  0.11458979 ... -1.51191717  0.09896652
  1.41295065], shape=(4547,), dtype=float64)

[2020-01-27 14:19:29.892345][__main__.TRPOAgent.log][linesearch]: improvement: 0.6367784204232962

[2020-01-27 14:19:29.917121][__main__.TRPOAgent.log][linesearch]: improvement: 0.3450532492923599

[2020-01-27 14:19:29.943725][__main__.TRPOAgent.log][linesearch]: improvement: 0.17821436264897383

[2020-01-27 14:19:29.969356][__main__.TRPOAgent.log][linesearch]: improvement: 0.08957843825395817

[2020-01-27 14:19:29.998618][__main__.TRPOAgent.log][linesearch]: improvement: 0.05368204877451088

[2020-01-27 14:19:30.024271][__main__.TRPOAgent.log][linesearch]: improvement: 0.03101714308265402

[2020-01-27 14:19:30.049661][__main__.TRPOAgent.log][linesearch]: improvement: 0.016245459349570446

[2020-01-27 14:19:30.075990][__main__.TRPOAgent.log][linesearch]: improvement: 0.007432696931418459

[2020-01-27 14:19:30.099709][__main__.TRPOAgent.log][linesearch]: improvement: 0.003969864836590009

[2020-01-27 14:19:30.129963][__main__.TRPOAgent.log][linesearch]: improvement: 0.0027974960304859575

[2020-01-27 14:19:30.130456][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 1.1932521488775358e-06, Discarded policy loss value: -1.3763652030895328

[2020-01-27 14:19:30.884151][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.1785986508109

[2020-01-27 14:19:30.890826][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:19:34.536396][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00216018  0.01675096 -0.03339613 ... -0.03441716  0.0334594
  0.00095776]

[2020-01-27 14:19:34.536816][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:35.011098][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-1.57933615  0.67037469 -2.69021912 ... -4.43448798  1.34387462
  3.09061336], shape=(4547,), dtype=float64)

[2020-01-27 14:19:35.100667][__main__.TRPOAgent.log][linesearch]: improvement: 0.2195067707339886

[2020-01-27 14:19:35.130249][__main__.TRPOAgent.log][linesearch]: improvement: 0.13458037311742999

[2020-01-27 14:19:35.158623][__main__.TRPOAgent.log][linesearch]: improvement: 0.08345825403971818

[2020-01-27 14:19:35.183857][__main__.TRPOAgent.log][linesearch]: improvement: 0.051898794540311766

[2020-01-27 14:19:35.212745][__main__.TRPOAgent.log][linesearch]: improvement: 0.034027881068648735

[2020-01-27 14:19:35.237309][__main__.TRPOAgent.log][linesearch]: improvement: 0.02348496670871958

[2020-01-27 14:19:35.262628][__main__.TRPOAgent.log][linesearch]: improvement: 0.016385262970011416

[2020-01-27 14:19:35.289877][__main__.TRPOAgent.log][linesearch]: improvement: 0.010483874304165308

[2020-01-27 14:19:35.318970][__main__.TRPOAgent.log][linesearch]: improvement: 0.006806845802104

[2020-01-27 14:19:35.349524][__main__.TRPOAgent.log][linesearch]: improvement: 0.004289398988679638

[2020-01-27 14:19:35.349990][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 7.800623085029926e-07, Discarded policy loss value: -1.3912955573834829

[2020-01-27 14:19:36.115462][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.9606077842013

[2020-01-27 14:19:36.120471][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:19:39.975042][__main__.TRPOAgent.log][training]: policy_gradient: [-0.01014036  0.01797231  0.0100315  ...  0.16871077 -0.38443345
  0.21572268]

[2020-01-27 14:19:39.975509][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:40.469745][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-4.47336469  3.12292151 -4.22429037 ... -1.58047638  0.54873115
  1.03174522], shape=(4547,), dtype=float64)

[2020-01-27 14:19:40.556563][__main__.TRPOAgent.log][linesearch]: improvement: 0.2541348443638368

[2020-01-27 14:19:40.585834][__main__.TRPOAgent.log][linesearch]: improvement: 0.14535126529109746

[2020-01-27 14:19:40.613477][__main__.TRPOAgent.log][linesearch]: improvement: 0.08822359049104003

[2020-01-27 14:19:40.641118][__main__.TRPOAgent.log][linesearch]: improvement: 0.05882672694056822

[2020-01-27 14:19:40.668850][__main__.TRPOAgent.log][linesearch]: improvement: 0.041992446300102126

[2020-01-27 14:19:40.696913][__main__.TRPOAgent.log][linesearch]: improvement: 0.029223711485873837

[2020-01-27 14:19:40.723606][__main__.TRPOAgent.log][linesearch]: improvement: 0.019201613060807074

[2020-01-27 14:19:40.753351][__main__.TRPOAgent.log][linesearch]: improvement: 0.012720927063673015

[2020-01-27 14:19:40.780389][__main__.TRPOAgent.log][linesearch]: improvement: 0.008611508316368902

[2020-01-27 14:19:40.810418][__main__.TRPOAgent.log][linesearch]: improvement: 0.005834481977251118

[2020-01-27 14:19:40.810953][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 4.5194197751180815e-07, Discarded policy loss value: -0.003939315319128986

[2020-01-27 14:19:41.556598][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.67656032447445

[2020-01-27 14:19:41.561823][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:19:45.146594][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.01144957  0.00979742  0.01526603 ... -0.13022212 -0.00658811
  0.13681023]

[2020-01-27 14:19:45.146984][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:19:45.544600][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.34724904  0.6983735  -0.21444452 ...  0.40003407 -0.08454876
 -0.31548531], shape=(4547,), dtype=float64)

[2020-01-27 14:19:45.624256][__main__.TRPOAgent.log][linesearch]: improvement: 0.03964738902843579

[2020-01-27 14:19:45.651455][__main__.TRPOAgent.log][linesearch]: improvement: 0.04643134071383559

[2020-01-27 14:19:45.652041][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.5406060009052848

[2020-01-27 14:19:46.415106][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.8174680122691

[2020-01-27 14:19:46.415507][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:19:46.438554][__main__.TRPOAgent.log][learning]: Episode #5

[2020-01-27 14:19:46.439101][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:19:46.480069][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:19:46.480632][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:19:46.480855][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:19:46.483422][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:19:50.750860][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:19:50.750132][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:19:50.751983][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:19:50.752785][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:19:50.752706][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:19:50.753729][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:19:54.734468][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:19:54.769595][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:19:54.770266][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:19:54.770857][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:19:54.770928][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:19:54.773003][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:19:58.421659][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:19:58.455258][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:19:58.456253][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:19:58.456987][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:19:58.457095][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:19:58.459100][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:20:02.110216][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:20:02.124062][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:20:02.124719][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:20:02.125363][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:20:02.125303][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:20:02.126768][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:20:06.212563][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:20:06.305474][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:20:06.306236][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:20:06.306831][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:20:06.306901][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:20:06.310316][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:20:10.213443][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:20:10.250921][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:20:10.252104][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:20:10.252763][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:20:10.252863][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:20:10.254701][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:20:13.896317][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:20:13.964747][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:20:13.965634][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:20:13.966530][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:20:13.966637][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:20:13.968779][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:20:18.103944][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:20:18.109919][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:20:18.110571][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:20:18.111216][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:20:18.111270][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:20:18.113311][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:20:22.425084][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:20:22.500193][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:20:22.500677][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:20:22.501266][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:20:22.501210][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:20:22.502239][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:20:26.137209][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:20:26.206133][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:20:26.206828][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:20:26.207588][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:20:26.207688][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:20:26.210067][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:20:29.679225][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:20:29.702090][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:20:29.702907][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:20:29.704105][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:20:29.703997][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:20:29.706648][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:20:34.474580][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:20:34.560630][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:20:34.561310][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:20:34.561971][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:20:34.561872][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:20:34.563364][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:20:38.090622][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:20:38.126445][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:20:38.126996][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:20:38.127740][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:20:38.127878][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:20:38.131179][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:20:41.824317][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:20:41.881575][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:20:41.882104][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:20:41.882833][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:20:41.882755][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:20:41.883934][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:20:45.320666][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:20:45.336473][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:20:45.336995][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:20:45.348771][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:20:45.844572][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:20:45.869587][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:20:45.871086][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:20:45.871479][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:20:45.901406][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:20:49.371848][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00896593  0.01025902 -0.02165274 ... -0.10302032  0.06775741
  0.03526292]

[2020-01-27 14:20:49.372236][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:20:49.772399][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.47815385 -0.11977372 -0.17260726 ... -2.51406819  0.37964416
  2.13442403], shape=(4547,), dtype=float64)

[2020-01-27 14:20:49.853765][__main__.TRPOAgent.log][linesearch]: improvement: 0.09230485397753962

[2020-01-27 14:20:49.882735][__main__.TRPOAgent.log][linesearch]: improvement: 0.0700021183113042

[2020-01-27 14:20:49.907303][__main__.TRPOAgent.log][linesearch]: improvement: 0.04913305490217934

[2020-01-27 14:20:49.934206][__main__.TRPOAgent.log][linesearch]: improvement: 0.030540323060453733

[2020-01-27 14:20:49.959748][__main__.TRPOAgent.log][linesearch]: improvement: 0.021043963528429965

[2020-01-27 14:20:49.982654][__main__.TRPOAgent.log][linesearch]: improvement: 0.015012407477341339

[2020-01-27 14:20:50.010309][__main__.TRPOAgent.log][linesearch]: improvement: 0.00953252033117069

[2020-01-27 14:20:50.034671][__main__.TRPOAgent.log][linesearch]: improvement: 0.005901745385900159

[2020-01-27 14:20:50.064558][__main__.TRPOAgent.log][linesearch]: improvement: 0.003615524807550008

[2020-01-27 14:20:50.088105][__main__.TRPOAgent.log][linesearch]: improvement: 0.0022096998513377875

[2020-01-27 14:20:50.088561][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.136088538920658e-07, Discarded policy loss value: -0.7031793654834697

[2020-01-27 14:20:50.808406][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.92288858158514

[2020-01-27 14:20:50.814667][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:20:54.491686][__main__.TRPOAgent.log][training]: policy_gradient: [-0.01603628 -0.00985695  0.09320986 ...  0.0255589  -0.10041422
  0.07485533]

[2020-01-27 14:20:54.492102][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:20:54.885580][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.00714671 -0.30135657  1.05914496 ...  0.43871294  0.21639985
 -0.65511279], shape=(4547,), dtype=float64)

[2020-01-27 14:20:54.959836][__main__.TRPOAgent.log][linesearch]: improvement: 0.661504264994063

[2020-01-27 14:20:54.989229][__main__.TRPOAgent.log][linesearch]: improvement: 0.2789670531453987

[2020-01-27 14:20:55.016449][__main__.TRPOAgent.log][linesearch]: improvement: 0.13410302520926165

[2020-01-27 14:20:55.041131][__main__.TRPOAgent.log][linesearch]: improvement: 0.06821623689873846

[2020-01-27 14:20:55.068053][__main__.TRPOAgent.log][linesearch]: improvement: 0.03667384912034688

[2020-01-27 14:20:55.097231][__main__.TRPOAgent.log][linesearch]: improvement: 0.02040450257235049

[2020-01-27 14:20:55.122062][__main__.TRPOAgent.log][linesearch]: improvement: 0.011565968312071684

[2020-01-27 14:20:55.149198][__main__.TRPOAgent.log][linesearch]: improvement: 0.006711203539015553

[2020-01-27 14:20:55.174561][__main__.TRPOAgent.log][linesearch]: improvement: 0.004075524880518877

[2020-01-27 14:20:55.203936][__main__.TRPOAgent.log][linesearch]: improvement: 0.0025002342862201488

[2020-01-27 14:20:55.204386][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.085867813888136e-07, Discarded policy loss value: -1.4990399974884707

[2020-01-27 14:20:55.955150][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.3246977247259

[2020-01-27 14:20:55.962012][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:21:00.545501][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.0199394   0.00275588 -0.0998622  ... -0.1090462   0.2315379
 -0.12249171]

[2020-01-27 14:21:00.545888][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:00.942450][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.10838405  0.17256811  0.0191303  ...  0.02316067 -0.22337675
  0.20021608], shape=(4547,), dtype=float64)

[2020-01-27 14:21:01.025917][__main__.TRPOAgent.log][linesearch]: improvement: 0.1782036475959317

[2020-01-27 14:21:01.054224][__main__.TRPOAgent.log][linesearch]: improvement: 0.12289839005297937

[2020-01-27 14:21:01.077569][__main__.TRPOAgent.log][linesearch]: improvement: 0.07753783954073551

[2020-01-27 14:21:01.105527][__main__.TRPOAgent.log][linesearch]: improvement: 0.04423050326663275

[2020-01-27 14:21:01.135503][__main__.TRPOAgent.log][linesearch]: improvement: 0.021267993471751734

[2020-01-27 14:21:01.162248][__main__.TRPOAgent.log][linesearch]: improvement: 0.011481117731325818

[2020-01-27 14:21:01.186377][__main__.TRPOAgent.log][linesearch]: improvement: 0.006080980665665647

[2020-01-27 14:21:01.218082][__main__.TRPOAgent.log][linesearch]: improvement: 0.0037759752674296765

[2020-01-27 14:21:01.241944][__main__.TRPOAgent.log][linesearch]: improvement: 0.002289809148651123

[2020-01-27 14:21:01.270908][__main__.TRPOAgent.log][linesearch]: improvement: 0.0013769733064586398

[2020-01-27 14:21:01.271352][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.95412077188225e-07, Discarded policy loss value: -0.25168088818013673

[2020-01-27 14:21:02.032286][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 285.4387794793541

[2020-01-27 14:21:02.037408][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:21:05.743953][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.03509525  0.03505367 -0.17652175 ... -0.12804134  0.23942792
 -0.11138658]

[2020-01-27 14:21:05.744342][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:06.155095][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[  1.47462442   8.31453589 -11.84866442 ...   5.84192612  -6.59639212
   0.754466  ], shape=(4547,), dtype=float64)

[2020-01-27 14:21:06.235252][__main__.TRPOAgent.log][linesearch]: improvement: 0.22072248710207054

[2020-01-27 14:21:06.260791][__main__.TRPOAgent.log][linesearch]: improvement: 0.22444593641849675

[2020-01-27 14:21:06.289153][__main__.TRPOAgent.log][linesearch]: improvement: 0.2396205488361116

[2020-01-27 14:21:06.315961][__main__.TRPOAgent.log][linesearch]: improvement: 0.179111062876105

[2020-01-27 14:21:06.338904][__main__.TRPOAgent.log][linesearch]: improvement: 0.09733235469056778

[2020-01-27 14:21:06.339372][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 4, New policy loss value: 2.6111438713470294

[2020-01-27 14:21:07.110240][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.36974461724134

[2020-01-27 14:21:07.110639][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:21:07.120132][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:21:10.578006][__main__.TRPOAgent.log][training]: policy_gradient: [-0.03318333 -0.0318687   0.30792898 ...  0.31303145 -0.59705015
  0.28401871]

[2020-01-27 14:21:10.578393][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:10.964762][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.22818852  0.42692404 -0.70713507 ...  0.61925497 -1.20940166
  0.59014668], shape=(4547,), dtype=float64)

[2020-01-27 14:21:11.045806][__main__.TRPOAgent.log][linesearch]: improvement: 0.33855538663662976

[2020-01-27 14:21:11.072766][__main__.TRPOAgent.log][linesearch]: improvement: 0.15348859885692256

[2020-01-27 14:21:11.099967][__main__.TRPOAgent.log][linesearch]: improvement: 0.08285855767468009

[2020-01-27 14:21:11.125946][__main__.TRPOAgent.log][linesearch]: improvement: 0.05102126715529165

[2020-01-27 14:21:11.155614][__main__.TRPOAgent.log][linesearch]: improvement: 0.032300290922123054

[2020-01-27 14:21:11.183010][__main__.TRPOAgent.log][linesearch]: improvement: 0.020823698611905694

[2020-01-27 14:21:11.208727][__main__.TRPOAgent.log][linesearch]: improvement: 0.013568626259401029

[2020-01-27 14:21:11.234271][__main__.TRPOAgent.log][linesearch]: improvement: 0.008249051938889362

[2020-01-27 14:21:11.260731][__main__.TRPOAgent.log][linesearch]: improvement: 0.005036465986793193

[2020-01-27 14:21:11.287113][__main__.TRPOAgent.log][linesearch]: improvement: 0.003091663990231197

[2020-01-27 14:21:11.287605][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.317036078570604e-07, Discarded policy loss value: -3.550239221172498

[2020-01-27 14:21:12.030855][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.0869809302457

[2020-01-27 14:21:12.036201][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:21:15.816344][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00052863 -0.00538517 -0.04413538 ... -0.05795074  0.11960509
 -0.06165435]

[2020-01-27 14:21:15.816734][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:16.250982][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.26546423 -0.3075622  -0.10213617 ...  0.75951938 -0.14811491
 -0.61140447], shape=(4547,), dtype=float64)

[2020-01-27 14:21:16.336013][__main__.TRPOAgent.log][linesearch]: improvement: 0.012327623606180449

[2020-01-27 14:21:16.336557][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 0.2816204938875542

[2020-01-27 14:21:17.075652][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.94151943209846

[2020-01-27 14:21:17.076047][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:21:17.086825][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:21:20.924140][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00703973  0.00301392 -0.01003366 ...  0.00617237 -0.00591426
 -0.00025811]

[2020-01-27 14:21:20.924528][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:21.397582][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.29771506  0.35353233  0.25808795 ...  1.3444991   0.00772728
 -1.35222638], shape=(4547,), dtype=float64)

[2020-01-27 14:21:21.492691][__main__.TRPOAgent.log][linesearch]: improvement: 0.12812736309424813

[2020-01-27 14:21:21.522822][__main__.TRPOAgent.log][linesearch]: improvement: 0.0780602583927244

[2020-01-27 14:21:21.552484][__main__.TRPOAgent.log][linesearch]: improvement: 0.047398981194910046

[2020-01-27 14:21:21.579497][__main__.TRPOAgent.log][linesearch]: improvement: 0.028576241911082136

[2020-01-27 14:21:21.609084][__main__.TRPOAgent.log][linesearch]: improvement: 0.017156167351543343

[2020-01-27 14:21:21.638739][__main__.TRPOAgent.log][linesearch]: improvement: 0.010246014969131162

[2020-01-27 14:21:21.664331][__main__.TRPOAgent.log][linesearch]: improvement: 0.006156417640111211

[2020-01-27 14:21:21.694822][__main__.TRPOAgent.log][linesearch]: improvement: 0.0036967060791719478

[2020-01-27 14:21:21.721134][__main__.TRPOAgent.log][linesearch]: improvement: 0.0022131424022382926

[2020-01-27 14:21:21.753592][__main__.TRPOAgent.log][linesearch]: improvement: 0.001328277807522281

[2020-01-27 14:21:21.754064][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.002121210811553e-07, Discarded policy loss value: -0.05266520640173561

[2020-01-27 14:21:22.556478][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.0608360898267

[2020-01-27 14:21:22.563443][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:21:26.961623][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00416587  0.00554032  0.01932121 ... -0.0216584  -0.06937137
  0.09102977]

[2020-01-27 14:21:26.962009][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:27.373471][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.04517722 -0.06198261 -0.23991099 ... -0.45730333  0.32170735
  0.13559598], shape=(4547,), dtype=float64)

[2020-01-27 14:21:27.454921][__main__.TRPOAgent.log][linesearch]: improvement: -0.0726352475601438

[2020-01-27 14:21:27.479143][__main__.TRPOAgent.log][linesearch]: improvement: -0.026369075027924016

[2020-01-27 14:21:27.507261][__main__.TRPOAgent.log][linesearch]: improvement: -0.006939679409226296

[2020-01-27 14:21:27.507757][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 0.14949084632107204

[2020-01-27 14:21:28.266977][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.3674974888215

[2020-01-27 14:21:28.267394][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:21:28.275052][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:21:32.267585][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00562695 -0.00745681 -0.09710078 ... -0.09597857  0.22723251
 -0.13125394]

[2020-01-27 14:21:32.267980][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:32.693664][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.0974112  -0.0811057  -0.63393139 ... -0.77840598 -0.18502234
  0.96342832], shape=(4547,), dtype=float64)

[2020-01-27 14:21:32.778713][__main__.TRPOAgent.log][linesearch]: improvement: 0.14627820953944726

[2020-01-27 14:21:32.806059][__main__.TRPOAgent.log][linesearch]: improvement: 0.08657511259422601

[2020-01-27 14:21:32.806806][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 1.1928522080622375

[2020-01-27 14:21:33.703814][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.98149070066097

[2020-01-27 14:21:33.704223][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:21:33.711624][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:21:37.536087][__main__.TRPOAgent.log][training]: policy_gradient: [-0.02235235 -0.01465474  0.12025361 ...  0.21666334 -0.17875008
 -0.03791326]

[2020-01-27 14:21:37.536486][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:21:37.925365][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.09555992 -0.06934469 -0.0394335  ... -0.3323408  -0.10330268
  0.43564348], shape=(4547,), dtype=float64)

[2020-01-27 14:21:38.003720][__main__.TRPOAgent.log][linesearch]: improvement: 0.1712090027222461

[2020-01-27 14:21:38.031930][__main__.TRPOAgent.log][linesearch]: improvement: 0.09456483748488932

[2020-01-27 14:21:38.059477][__main__.TRPOAgent.log][linesearch]: improvement: 0.055262773412795355

[2020-01-27 14:21:38.083438][__main__.TRPOAgent.log][linesearch]: improvement: 0.03289644205304648

[2020-01-27 14:21:38.111109][__main__.TRPOAgent.log][linesearch]: improvement: 0.019723626727416357

[2020-01-27 14:21:38.134344][__main__.TRPOAgent.log][linesearch]: improvement: 0.011908899715764462

[2020-01-27 14:21:38.161706][__main__.TRPOAgent.log][linesearch]: improvement: 0.007174021428560884

[2020-01-27 14:21:38.186788][__main__.TRPOAgent.log][linesearch]: improvement: 0.004318774515312013

[2020-01-27 14:21:38.214929][__main__.TRPOAgent.log][linesearch]: improvement: 0.0025991746880283384

[2020-01-27 14:21:38.239896][__main__.TRPOAgent.log][linesearch]: improvement: 0.0015597771744063071

[2020-01-27 14:21:38.240430][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.278896574812982e-07, Discarded policy loss value: -0.08456829761850147

[2020-01-27 14:21:38.979841][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.67978380221047

[2020-01-27 14:21:38.996615][__main__.TRPOAgent.log][learning]: Episode #6

[2020-01-27 14:21:38.996973][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:21:39.032791][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:21:39.033799][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:21:39.033491][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:21:39.034690][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:21:42.489327][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

[2020-01-27 14:21:42.521080][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:21:42.521682][Environment.Environment.log][rollouts]: Rollout thread #3

[2020-01-27 14:21:42.522465][Environment.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-27 14:21:42.522548][Environment.Environment.log][rollouts]: Rollout thread #4

[2020-01-27 14:21:42.524572][Environment.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-27 14:21:45.865008][Environment.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 1500

[2020-01-27 14:21:45.930584][Environment.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 1500

[2020-01-27 14:21:45.931117][Environment.Environment.log][rollouts]: Rollout thread #5

[2020-01-27 14:21:45.931777][Environment.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-27 14:21:45.931833][Environment.Environment.log][rollouts]: Rollout thread #6

[2020-01-27 14:21:45.933356][Environment.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-27 14:21:49.623889][Environment.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 1500

[2020-01-27 14:21:49.698454][Environment.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 1500

[2020-01-27 14:21:49.699022][Environment.Environment.log][rollouts]: Rollout thread #7

[2020-01-27 14:21:49.699906][Environment.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-27 14:21:49.699984][Environment.Environment.log][rollouts]: Rollout thread #8

[2020-01-27 14:21:49.702678][Environment.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-27 14:21:53.750341][Environment.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 1500

[2020-01-27 14:21:53.789631][Environment.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 1500

[2020-01-27 14:21:53.790154][Environment.Environment.log][rollouts]: Rollout thread #9

[2020-01-27 14:21:53.790789][Environment.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-27 14:21:53.790990][Environment.Environment.log][rollouts]: Rollout thread #10

[2020-01-27 14:21:53.793374][Environment.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-27 14:21:57.436972][Environment.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 1500

[2020-01-27 14:21:57.530541][Environment.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 1500

[2020-01-27 14:21:57.531602][Environment.Environment.log][rollouts]: Rollout thread #11

[2020-01-27 14:21:57.532350][Environment.Environment.log][thread_rollouts]: Thread number: 10

[2020-01-27 14:21:57.532430][Environment.Environment.log][rollouts]: Rollout thread #12

[2020-01-27 14:21:57.534954][Environment.Environment.log][thread_rollouts]: Thread number: 11

[2020-01-27 14:22:01.569720][Environment.Environment.log][thread_rollouts]: Thread number: 11, Steps performed: 1500

[2020-01-27 14:22:01.635317][Environment.Environment.log][thread_rollouts]: Thread number: 10, Steps performed: 1500

[2020-01-27 14:22:01.636245][Environment.Environment.log][rollouts]: Rollout thread #13

[2020-01-27 14:22:01.636846][Environment.Environment.log][thread_rollouts]: Thread number: 12

[2020-01-27 14:22:01.636914][Environment.Environment.log][rollouts]: Rollout thread #14

[2020-01-27 14:22:01.640966][Environment.Environment.log][thread_rollouts]: Thread number: 13

[2020-01-27 14:22:04.922423][Environment.Environment.log][thread_rollouts]: Thread number: 12, Steps performed: 1500

[2020-01-27 14:22:04.985314][Environment.Environment.log][thread_rollouts]: Thread number: 13, Steps performed: 1500

[2020-01-27 14:22:04.985832][Environment.Environment.log][rollouts]: Rollout thread #15

[2020-01-27 14:22:04.986402][Environment.Environment.log][thread_rollouts]: Thread number: 14

[2020-01-27 14:22:04.986525][Environment.Environment.log][rollouts]: Rollout thread #16

[2020-01-27 14:22:04.990105][Environment.Environment.log][thread_rollouts]: Thread number: 15

[2020-01-27 14:22:08.160751][Environment.Environment.log][thread_rollouts]: Thread number: 15, Steps performed: 1500

[2020-01-27 14:22:08.240700][Environment.Environment.log][thread_rollouts]: Thread number: 14, Steps performed: 1500

[2020-01-27 14:22:08.241351][Environment.Environment.log][rollouts]: Rollout thread #17

[2020-01-27 14:22:08.242018][Environment.Environment.log][thread_rollouts]: Thread number: 16

[2020-01-27 14:22:08.242373][Environment.Environment.log][rollouts]: Rollout thread #18

[2020-01-27 14:22:08.243895][Environment.Environment.log][thread_rollouts]: Thread number: 17

[2020-01-27 14:22:11.681266][Environment.Environment.log][thread_rollouts]: Thread number: 17, Steps performed: 1500

[2020-01-27 14:22:11.733770][Environment.Environment.log][thread_rollouts]: Thread number: 16, Steps performed: 1500

[2020-01-27 14:22:11.734413][Environment.Environment.log][rollouts]: Rollout thread #19

[2020-01-27 14:22:11.734895][Environment.Environment.log][thread_rollouts]: Thread number: 18

[2020-01-27 14:22:11.735066][Environment.Environment.log][rollouts]: Rollout thread #20

[2020-01-27 14:22:11.736842][Environment.Environment.log][thread_rollouts]: Thread number: 19

[2020-01-27 14:22:15.261571][Environment.Environment.log][thread_rollouts]: Thread number: 18, Steps performed: 1500

[2020-01-27 14:22:15.331654][Environment.Environment.log][thread_rollouts]: Thread number: 19, Steps performed: 1500

[2020-01-27 14:22:15.332275][Environment.Environment.log][rollouts]: Rollout thread #21

[2020-01-27 14:22:15.333357][Environment.Environment.log][rollouts]: Rollout thread #22

[2020-01-27 14:22:15.333063][Environment.Environment.log][thread_rollouts]: Thread number: 20

[2020-01-27 14:22:15.334401][Environment.Environment.log][thread_rollouts]: Thread number: 21

[2020-01-27 14:22:18.552164][Environment.Environment.log][thread_rollouts]: Thread number: 21, Steps performed: 1500

[2020-01-27 14:22:18.586949][Environment.Environment.log][thread_rollouts]: Thread number: 20, Steps performed: 1500

[2020-01-27 14:22:18.587478][Environment.Environment.log][rollouts]: Rollout thread #23

[2020-01-27 14:22:18.587956][Environment.Environment.log][thread_rollouts]: Thread number: 22

[2020-01-27 14:22:18.588032][Environment.Environment.log][rollouts]: Rollout thread #24

[2020-01-27 14:22:18.590197][Environment.Environment.log][thread_rollouts]: Thread number: 23

[2020-01-27 14:22:21.913289][Environment.Environment.log][thread_rollouts]: Thread number: 23, Steps performed: 1500

[2020-01-27 14:22:21.951081][Environment.Environment.log][thread_rollouts]: Thread number: 22, Steps performed: 1500

[2020-01-27 14:22:21.951721][Environment.Environment.log][rollouts]: Rollout thread #25

[2020-01-27 14:22:21.952316][Environment.Environment.log][thread_rollouts]: Thread number: 24

[2020-01-27 14:22:21.952378][Environment.Environment.log][rollouts]: Rollout thread #26

[2020-01-27 14:22:21.954418][Environment.Environment.log][thread_rollouts]: Thread number: 25

[2020-01-27 14:22:25.175063][Environment.Environment.log][thread_rollouts]: Thread number: 25, Steps performed: 1500

[2020-01-27 14:22:25.220688][Environment.Environment.log][thread_rollouts]: Thread number: 24, Steps performed: 1500

[2020-01-27 14:22:25.221203][Environment.Environment.log][rollouts]: Rollout thread #27

[2020-01-27 14:22:25.221957][Environment.Environment.log][rollouts]: Rollout thread #28

[2020-01-27 14:22:25.222784][Environment.Environment.log][thread_rollouts]: Thread number: 27

[2020-01-27 14:22:25.221870][Environment.Environment.log][thread_rollouts]: Thread number: 26

[2020-01-27 14:22:28.456856][Environment.Environment.log][thread_rollouts]: Thread number: 26, Steps performed: 1500

[2020-01-27 14:22:28.517437][Environment.Environment.log][thread_rollouts]: Thread number: 27, Steps performed: 1500

[2020-01-27 14:22:28.517983][Environment.Environment.log][rollouts]: Rollout thread #29

[2020-01-27 14:22:28.518740][Environment.Environment.log][thread_rollouts]: Thread number: 28

[2020-01-27 14:22:28.518861][Environment.Environment.log][rollouts]: Rollout thread #30

[2020-01-27 14:22:28.521272][Environment.Environment.log][thread_rollouts]: Thread number: 29

[2020-01-27 14:22:32.027143][Environment.Environment.log][thread_rollouts]: Thread number: 28, Steps performed: 1500

[2020-01-27 14:22:32.056405][Environment.Environment.log][thread_rollouts]: Thread number: 29, Steps performed: 1500

[2020-01-27 14:22:32.056931][__main__.TRPOAgent.log][rollouts]: Unpacking actions

[2020-01-27 14:22:32.068433][__main__.TRPOAgent.log][rollouts]: Unpacking rewards

[2020-01-27 14:22:32.559538][__main__.TRPOAgent.log][rollouts]: Unpacking observations

[2020-01-27 14:22:32.585056][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-27 14:22:32.586717][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 45000, Batch size: 4500, Number of batches: 10

[2020-01-27 14:22:32.587162][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-27 14:22:32.618113][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 4500

[2020-01-27 14:22:36.234237][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00428755 -0.00445764 -0.11310838 ...  0.01996414  0.08268991
 -0.10265405]

[2020-01-27 14:22:36.234624][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:22:36.633133][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.38930004 -0.45062825 -1.14404912 ... -1.16261648 -1.30227996
  2.46489644], shape=(4547,), dtype=float64)

[2020-01-27 14:22:36.730144][__main__.TRPOAgent.log][linesearch]: improvement: 0.2328813394483543

[2020-01-27 14:22:36.764391][__main__.TRPOAgent.log][linesearch]: improvement: 0.18592246388726227

[2020-01-27 14:22:36.796369][__main__.TRPOAgent.log][linesearch]: improvement: 0.13143599502651

[2020-01-27 14:22:36.829187][__main__.TRPOAgent.log][linesearch]: improvement: 0.08842237880189169

[2020-01-27 14:22:36.861953][__main__.TRPOAgent.log][linesearch]: improvement: 0.05768306898131151

[2020-01-27 14:22:36.890688][__main__.TRPOAgent.log][linesearch]: improvement: 0.03635957446776916

[2020-01-27 14:22:36.916731][__main__.TRPOAgent.log][linesearch]: improvement: 0.022489209831033002

[2020-01-27 14:22:36.945682][__main__.TRPOAgent.log][linesearch]: improvement: 0.013746878320864603

[2020-01-27 14:22:36.978971][__main__.TRPOAgent.log][linesearch]: improvement: 0.008336812382170641

[2020-01-27 14:22:37.004083][__main__.TRPOAgent.log][linesearch]: improvement: 0.005036607700187032

[2020-01-27 14:22:37.004559][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.608828163784507e-07, Discarded policy loss value: -0.09323497410442562

[2020-01-27 14:22:37.814904][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.1016538826224

[2020-01-27 14:22:37.820405][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 4500

[2020-01-27 14:22:41.261110][__main__.TRPOAgent.log][training]: policy_gradient: [-0.0074555  -0.00384313  0.12652029 ...  0.10830003 -0.23918388
  0.13088385]

[2020-01-27 14:22:41.261490][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:22:41.703172][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.18584478  0.11808866  0.22993617 ...  0.60877201  0.46827052
 -1.07704253], shape=(4547,), dtype=float64)

[2020-01-27 14:22:41.794558][__main__.TRPOAgent.log][linesearch]: improvement: 0.12913465409204528

[2020-01-27 14:22:41.820765][__main__.TRPOAgent.log][linesearch]: improvement: 0.07773855292939458

[2020-01-27 14:22:41.847114][__main__.TRPOAgent.log][linesearch]: improvement: 0.04631710198486516

[2020-01-27 14:22:41.873543][__main__.TRPOAgent.log][linesearch]: improvement: 0.027590285948285498

[2020-01-27 14:22:41.898776][__main__.TRPOAgent.log][linesearch]: improvement: 0.016461818506755987

[2020-01-27 14:22:41.925383][__main__.TRPOAgent.log][linesearch]: improvement: 0.00983482177596895

[2020-01-27 14:22:41.948843][__main__.TRPOAgent.log][linesearch]: improvement: 0.005874682880370186

[2020-01-27 14:22:41.975455][__main__.TRPOAgent.log][linesearch]: improvement: 0.0035131481506456286

[2020-01-27 14:22:42.001419][__main__.TRPOAgent.log][linesearch]: improvement: 0.0021169417185511463

[2020-01-27 14:22:42.031301][__main__.TRPOAgent.log][linesearch]: improvement: 0.0012744336717054328

[2020-01-27 14:22:42.031912][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.948124904687959e-07, Discarded policy loss value: -1.5566294530522282

[2020-01-27 14:22:42.754769][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.8446735523752

[2020-01-27 14:22:42.762378][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 4500

[2020-01-27 14:22:46.315982][__main__.TRPOAgent.log][training]: policy_gradient: [-0.01221513 -0.00898477 -0.01501811 ...  0.1322436  -0.00342425
 -0.12881935]

[2020-01-27 14:22:46.316358][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:22:46.702766][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.07045385 -0.14528046 -0.27927737 ... -0.11065498 -0.06600146
  0.17665643], shape=(4547,), dtype=float64)

[2020-01-27 14:22:46.777699][__main__.TRPOAgent.log][linesearch]: improvement: 0.05325964082493467

[2020-01-27 14:22:46.801684][__main__.TRPOAgent.log][linesearch]: improvement: 0.06729230844536022

[2020-01-27 14:22:46.802140][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New policy loss value: 0.2718520187675624

[2020-01-27 14:22:47.524845][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.146822426533

[2020-01-27 14:22:47.525260][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:22:47.535229][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 4500

[2020-01-27 14:22:50.987145][__main__.TRPOAgent.log][training]: policy_gradient: [-0.05506906 -0.02777002  0.34083057 ...  0.16392855 -0.44856284
  0.28463428]

[2020-01-27 14:22:50.987545][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:22:51.391982][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.27645691  0.07210665  0.76682453 ...  0.49389268  0.48065209
 -0.97454477], shape=(4547,), dtype=float64)

[2020-01-27 14:22:51.591860][__main__.TRPOAgent.log][linesearch]: improvement: 1.3848587173697529

[2020-01-27 14:22:51.655364][__main__.TRPOAgent.log][linesearch]: improvement: 0.49391730055548244

[2020-01-27 14:22:51.716013][__main__.TRPOAgent.log][linesearch]: improvement: 0.22401888799206926

[2020-01-27 14:22:51.782143][__main__.TRPOAgent.log][linesearch]: improvement: 0.1162714992668561

[2020-01-27 14:22:51.842580][__main__.TRPOAgent.log][linesearch]: improvement: 0.06423947906862404

[2020-01-27 14:22:51.901516][__main__.TRPOAgent.log][linesearch]: improvement: 0.036789531518969465

[2020-01-27 14:22:51.964508][__main__.TRPOAgent.log][linesearch]: improvement: 0.021825766550630732

[2020-01-27 14:22:52.023759][__main__.TRPOAgent.log][linesearch]: improvement: 0.01309147449821868

[2020-01-27 14:22:52.085087][__main__.TRPOAgent.log][linesearch]: improvement: 0.007820264710820979

[2020-01-27 14:22:52.145157][__main__.TRPOAgent.log][linesearch]: improvement: 0.004690312537941388

[2020-01-27 14:22:52.146302][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.839578722966813e-07, Discarded policy loss value: -3.149833300438202

[2020-01-27 14:22:52.949809][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.64460856038465

[2020-01-27 14:22:52.955438][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 4500

[2020-01-27 14:22:56.365914][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.03793308  0.02603737 -0.26504672 ... -0.22039528  0.42762519
 -0.20722991]

[2020-01-27 14:22:56.366294][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:22:56.748416][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.31967667  0.30777329 -2.44575352 ...  1.47228531 -1.10369528
 -0.36859003], shape=(4547,), dtype=float64)

[2020-01-27 14:22:56.827159][__main__.TRPOAgent.log][linesearch]: improvement: 0.12284899998632426

[2020-01-27 14:22:56.827626][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New policy loss value: 2.398650913408074

[2020-01-27 14:22:57.546844][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.35545361173223

[2020-01-27 14:22:57.547239][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:22:57.554925][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 4500

[2020-01-27 14:23:00.949406][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.07258429  0.07162866 -0.25322638 ... -0.28889736  0.20068093
  0.08821644]

[2020-01-27 14:23:00.949785][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:23:01.334094][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.60115621  0.61234764 -1.68591593 ...  1.44743962 -0.48659059
 -0.96084903], shape=(4547,), dtype=float64)

[2020-01-27 14:23:01.410396][__main__.TRPOAgent.log][linesearch]: improvement: 0.21842715137392432

[2020-01-27 14:23:01.438174][__main__.TRPOAgent.log][linesearch]: improvement: 0.18842988302657282

[2020-01-27 14:23:01.464032][__main__.TRPOAgent.log][linesearch]: improvement: 0.14112067543621087

[2020-01-27 14:23:01.487993][__main__.TRPOAgent.log][linesearch]: improvement: 0.09360673465014246

[2020-01-27 14:23:01.516137][__main__.TRPOAgent.log][linesearch]: improvement: 0.06279903833596157

[2020-01-27 14:23:01.539539][__main__.TRPOAgent.log][linesearch]: improvement: 0.04042013737731287

[2020-01-27 14:23:01.566313][__main__.TRPOAgent.log][linesearch]: improvement: 0.02553855059191934

[2020-01-27 14:23:01.594929][__main__.TRPOAgent.log][linesearch]: improvement: 0.016017378581246078

[2020-01-27 14:23:01.621551][__main__.TRPOAgent.log][linesearch]: improvement: 0.010043079083923656

[2020-01-27 14:23:01.648768][__main__.TRPOAgent.log][linesearch]: improvement: 0.00602229318860481

[2020-01-27 14:23:01.649236][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 1.0023490352056022e-06, Discarded policy loss value: -0.5335763652672736

[2020-01-27 14:23:02.388814][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.0165731788868

[2020-01-27 14:23:02.394373][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 4500

[2020-01-27 14:23:05.852060][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.10249394  0.10399545 -0.45124958 ... -0.15060378  0.52350932
 -0.37290554]

[2020-01-27 14:23:05.852454][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:23:06.249389][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.51015366  0.57820996 -1.77408389 ...  1.0209097  -0.42424999
 -0.59665971], shape=(4547,), dtype=float64)

[2020-01-27 14:23:06.333099][__main__.TRPOAgent.log][linesearch]: improvement: 0.596350228751996

[2020-01-27 14:23:06.357556][__main__.TRPOAgent.log][linesearch]: improvement: 0.3989616339685873

[2020-01-27 14:23:06.386507][__main__.TRPOAgent.log][linesearch]: improvement: 0.20420518733238047

[2020-01-27 14:23:06.386950][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 2, New policy loss value: 1.8437958573425337

[2020-01-27 14:23:07.103705][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 287.07416168373527

[2020-01-27 14:23:07.104104][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-27 14:23:07.111505][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 4500

[2020-01-27 14:23:10.524502][__main__.TRPOAgent.log][training]: policy_gradient: [-0.16785142 -0.17923265  0.42459491 ... -0.066196    0.25733398
 -0.19113799]

[2020-01-27 14:23:10.524896][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:23:10.946830][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-24.45615445 -34.84627298  26.26850449 ... -11.95208923  25.26604375
 -13.31395452], shape=(4547,), dtype=float64)

[2020-01-27 14:23:11.033785][__main__.TRPOAgent.log][linesearch]: improvement: 0.24652832863063978

[2020-01-27 14:23:11.059201][__main__.TRPOAgent.log][linesearch]: improvement: 0.5290268291097334

[2020-01-27 14:23:11.087956][__main__.TRPOAgent.log][linesearch]: improvement: 0.7164721594116478

[2020-01-27 14:23:11.111374][__main__.TRPOAgent.log][linesearch]: improvement: 0.633995590416797

[2020-01-27 14:23:11.139236][__main__.TRPOAgent.log][linesearch]: improvement: 0.486431343264469

[2020-01-27 14:23:11.164539][__main__.TRPOAgent.log][linesearch]: improvement: 0.35104951405849316

[2020-01-27 14:23:11.193059][__main__.TRPOAgent.log][linesearch]: improvement: 0.24045814355980843

[2020-01-27 14:23:11.219763][__main__.TRPOAgent.log][linesearch]: improvement: 0.15790582673481912

[2020-01-27 14:23:11.249095][__main__.TRPOAgent.log][linesearch]: improvement: 0.09984801860082271

[2020-01-27 14:23:11.277022][__main__.TRPOAgent.log][linesearch]: improvement: 0.06187407697587888

[2020-01-27 14:23:11.277489][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 6.818404581177502e-07, Discarded policy loss value: -1.013959961956763

[2020-01-27 14:23:12.059519][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.2632081594268

[2020-01-27 14:23:12.066299][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 4500

[2020-01-27 14:23:15.529292][__main__.TRPOAgent.log][training]: policy_gradient: [-0.10567493 -0.10116202  0.38031181 ...  0.29882505 -0.40269904
  0.103874  ]

[2020-01-27 14:23:15.529688][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:23:15.930890][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[ 0.30493159  0.3431537  -1.69210733 ...  0.63813786 -0.49432278
 -0.14381507], shape=(4547,), dtype=float64)

[2020-01-27 14:23:16.020192][__main__.TRPOAgent.log][linesearch]: improvement: 0.2297461882586953

[2020-01-27 14:23:16.043953][__main__.TRPOAgent.log][linesearch]: improvement: 0.14944577374865364

[2020-01-27 14:23:16.070563][__main__.TRPOAgent.log][linesearch]: improvement: 0.10943473351252608

[2020-01-27 14:23:16.094708][__main__.TRPOAgent.log][linesearch]: improvement: 0.07736016083092512

[2020-01-27 14:23:16.123548][__main__.TRPOAgent.log][linesearch]: improvement: 0.049118671452769824

[2020-01-27 14:23:16.148465][__main__.TRPOAgent.log][linesearch]: improvement: 0.029938997094253406

[2020-01-27 14:23:16.173525][__main__.TRPOAgent.log][linesearch]: improvement: 0.018175658930450478

[2020-01-27 14:23:16.199278][__main__.TRPOAgent.log][linesearch]: improvement: 0.011079501207983178

[2020-01-27 14:23:16.223705][__main__.TRPOAgent.log][linesearch]: improvement: 0.006711676357975094

[2020-01-27 14:23:16.251681][__main__.TRPOAgent.log][linesearch]: improvement: 0.004012517786190717

[2020-01-27 14:23:16.252183][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 9.454413212407925e-07, Discarded policy loss value: -3.2526040483270906

[2020-01-27 14:23:17.002906][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.77742733816694

[2020-01-27 14:23:17.008395][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 4500

[2020-01-27 14:23:20.529809][__main__.TRPOAgent.log][training]: policy_gradient: [-0.01696238 -0.01362218  0.0633361  ... -0.07327617  0.16976754
 -0.09649137]

[2020-01-27 14:23:20.530188][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-27 14:23:20.914775][__main__.TRPOAgent.log][training]: gradient_step_direction: tf.Tensor(
[-0.94847718 -1.27473486 -4.79444762 ...  4.70915999  2.57861839
 -7.28777838], shape=(4547,), dtype=float64)

[2020-01-27 14:23:20.994767][__main__.TRPOAgent.log][linesearch]: improvement: 0.2068838524850293

[2020-01-27 14:23:21.026282][__main__.TRPOAgent.log][linesearch]: improvement: 0.2817904635743925

[2020-01-27 14:23:21.052873][__main__.TRPOAgent.log][linesearch]: improvement: 0.3249719238825316

[2020-01-27 14:23:21.079598][__main__.TRPOAgent.log][linesearch]: improvement: 0.24781808514136425

[2020-01-27 14:23:21.108587][__main__.TRPOAgent.log][linesearch]: improvement: 0.18634606060543937

[2020-01-27 14:23:21.137535][__main__.TRPOAgent.log][linesearch]: improvement: 0.12506072916966768

[2020-01-27 14:23:21.162796][__main__.TRPOAgent.log][linesearch]: improvement: 0.08106629734486301

[2020-01-27 14:23:21.191113][__main__.TRPOAgent.log][linesearch]: improvement: 0.05147918797700732

[2020-01-27 14:23:21.217627][__main__.TRPOAgent.log][linesearch]: improvement: 0.03190311806183005

[2020-01-27 14:23:21.245124][__main__.TRPOAgent.log][linesearch]: improvement: 0.019502789606127197

[2020-01-27 14:23:21.245567][__main__.TRPOAgent.log][linesearch]: Linesearch failed. Mean kl divergence: 8.11896256162426e-07, Discarded policy loss value: -0.19625620676939676

[2020-01-27 14:23:22.004012][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 286.4435588527673

[2020-01-27 14:23:22.019657][__main__.TRPOAgent.log][learning]: Episode #7

[2020-01-27 14:23:22.020005][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 1500

[2020-01-27 14:23:22.057996][Environment.Environment.log][rollouts]: Rollout thread #1

[2020-01-27 14:23:22.058765][Environment.Environment.log][rollouts]: Rollout thread #2

[2020-01-27 14:23:22.058587][Environment.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-27 14:23:22.059703][Environment.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-27 14:23:25.498917][Environment.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 1500

[2020-01-27 14:23:25.563011][Environment.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 1500

