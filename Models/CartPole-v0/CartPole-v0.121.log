LOGGER started at 2020-01-28 16:59:00.638611.
Currently active debug channels:
	rollouts
	act
	training
	batch_info
	linesearch
	learning
	thread_rollouts
[2020-01-28 16:59:01.084939][__main__.TRPOAgent.log][learning]: Episode #0

[2020-01-28 16:59:01.328545][__main__.TRPOAgent.log][learning]: Performing rollouts: rollout length: 50

[2020-01-28 16:59:01.343072][EnvironmentNew.Environment.log][rollouts]: Rollout thread #1

[2020-01-28 16:59:01.343969][EnvironmentNew.Environment.log][rollouts]: Rollout thread #2

[2020-01-28 16:59:01.343897][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 0

[2020-01-28 16:59:01.347759][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 1

[2020-01-28 16:59:01.503765][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 0, Steps performed: 50

[2020-01-28 16:59:01.505023][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 1, Steps performed: 49

[2020-01-28 16:59:01.505657][EnvironmentNew.Environment.log][rollouts]: Rollout thread #3

[2020-01-28 16:59:01.506408][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 2

[2020-01-28 16:59:01.506508][EnvironmentNew.Environment.log][rollouts]: Rollout thread #4

[2020-01-28 16:59:01.508862][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 3

[2020-01-28 16:59:01.660630][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 3, Steps performed: 50

[2020-01-28 16:59:01.662340][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 2, Steps performed: 47

[2020-01-28 16:59:01.663253][EnvironmentNew.Environment.log][rollouts]: Rollout thread #5

[2020-01-28 16:59:01.663937][EnvironmentNew.Environment.log][rollouts]: Rollout thread #6

[2020-01-28 16:59:01.663865][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 4

[2020-01-28 16:59:01.664921][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 5

[2020-01-28 16:59:01.798023][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 5, Steps performed: 48

[2020-01-28 16:59:01.799214][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 4, Steps performed: 50

[2020-01-28 16:59:01.799934][EnvironmentNew.Environment.log][rollouts]: Rollout thread #7

[2020-01-28 16:59:01.800441][EnvironmentNew.Environment.log][rollouts]: Rollout thread #8

[2020-01-28 16:59:01.800384][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 6

[2020-01-28 16:59:01.801416][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 7

[2020-01-28 16:59:01.922878][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 6, Steps performed: 47

[2020-01-28 16:59:01.925382][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 7, Steps performed: 42

[2020-01-28 16:59:01.925940][EnvironmentNew.Environment.log][rollouts]: Rollout thread #9

[2020-01-28 16:59:01.926507][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 8

[2020-01-28 16:59:01.926566][EnvironmentNew.Environment.log][rollouts]: Rollout thread #10

[2020-01-28 16:59:01.929829][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 9

[2020-01-28 16:59:02.010284][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 8, Steps performed: 32

[2020-01-28 16:59:02.046570][EnvironmentNew.Environment.log][thread_rollouts]: Thread number: 9, Steps performed: 50

[2020-01-28 16:59:02.047428][__main__.TRPOAgent.log][learning]: Rollouts performed

[2020-01-28 16:59:02.048586][__main__.TRPOAgent.log][batch_info]: Rollout statistics size: 465, Batch size: 1000, Number of batches: 10

[2020-01-28 16:59:02.048934][__main__.TRPOAgent.log][learning]: 

***************
BEGINNING TRAINING
***************



[2020-01-28 16:59:02.049300][__main__.TRPOAgent.log][batch_info]: Batch #0, batch length: 50

[2020-01-28 16:59:02.069075][__main__.TRPOAgent.log][training]: policy_gradient: [ 2.96236797e-03  1.01317165e-02  3.87401323e-03  9.05983054e-03
  5.56711114e-03 -4.14081109e-04  3.01237115e-04  0.00000000e+00
  2.07109896e-04 -5.72023552e-04  3.53640415e-04  0.00000000e+00
  2.31460802e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
 -2.78943196e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00
 -4.73226757e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00
  4.70431316e-04 -4.55684194e-04  1.48593237e-04  0.00000000e+00
  1.85520006e-05  0.00000000e+00 -3.82482728e-04  6.38649938e-03
 -4.64607201e-03  0.00000000e+00 -3.19431917e-03  8.82249391e-03
 -5.45430411e-03  0.00000000e+00 -3.56989063e-03  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  4.30222609e-03  0.00000000e+00
  0.00000000e+00  0.00000000e+00  7.29872076e-03  0.00000000e+00
  0.00000000e+00  0.00000000e+00 -7.25560582e-03  7.02815646e-03
 -2.29179887e-03  0.00000000e+00 -2.86133170e-04  0.00000000e+00
  5.89914790e-03  3.12852746e-04 -3.12852746e-04  1.71945585e-04
 -1.71945585e-04  0.00000000e+00  0.00000000e+00  1.63774842e-04
 -1.63774842e-04  4.98046202e-04 -4.98046202e-04  3.58370941e-04
 -3.58370941e-04  0.00000000e+00  0.00000000e+00  7.30368114e-05
 -7.30368114e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.23273454e-04
 -4.23273454e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  8.22821861e-05
 -8.22821861e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.36047160e-04
 -1.36047160e-04  3.28819305e-04 -3.28819305e-04  1.73289795e-04
 -1.73289795e-04  0.00000000e+00  0.00000000e+00  3.20506070e-04
 -3.20506070e-04  0.00000000e+00  0.00000000e+00  7.93601544e-05
 -7.93601544e-05 -1.16302500e-02  1.16302500e-02]

[2020-01-28 16:59:02.069586][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:02.129620][__main__.TRPOAgent.log][training]: gradient_step_direction: [ 2.15000559e+00  6.74072835e+00  2.42253456e+00  5.47036974e+00
 -3.93704197e-02 -4.63088796e-01  3.36889400e-01  0.00000000e+00
  2.31621953e-01 -6.39724180e-01  3.95494785e-01  0.00000000e+00
  2.58854857e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
 -3.11956920e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00
 -5.29234506e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00
  5.26108220e-01 -5.09615726e-01  1.66179678e-01  0.00000000e+00
  2.07476892e-02  0.00000000e+00 -4.27750660e-01 -4.51652966e-02
  3.28570534e-02  0.00000000e+00  2.25902532e-02 -6.23926311e-02
  3.85728967e-02  0.00000000e+00  2.52462835e-02  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00 -3.04252972e-02  0.00000000e+00
  0.00000000e+00  0.00000000e+00 -5.16165580e-02  0.00000000e+00
  0.00000000e+00  0.00000000e+00  5.13116774e-02 -4.97030947e-02
  1.62076289e-02  0.00000000e+00  2.02357592e-03  0.00000000e+00
 -4.17187762e-02  3.49879769e-01 -3.49879769e-01  1.92295841e-01
 -1.92295841e-01  0.00000000e+00  0.00000000e+00  1.83158067e-01
 -1.83158067e-01  5.56991405e-01 -5.56991405e-01  4.00785175e-01
 -4.00785175e-01  0.00000000e+00  0.00000000e+00  8.16809291e-02
 -8.16809291e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.73369087e-01
 -4.73369087e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  9.20205190e-02
 -9.20205190e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.52148734e-01
 -1.52148734e-01  3.67736017e-01 -3.67736017e-01  1.93799142e-01
 -1.93799142e-01  0.00000000e+00  0.00000000e+00  3.58438888e-01
 -3.58438888e-01  0.00000000e+00  0.00000000e+00  8.87526567e-02
 -8.87526567e-02  2.67482611e-02 -2.67482611e-02]

[2020-01-28 16:59:02.168053][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.013935465623612095, New policy loss value: 0.004920444112152138

[2020-01-28 16:59:02.433246][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 235.6341953766346

[2020-01-28 16:59:02.433617][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:02.436461][__main__.TRPOAgent.log][batch_info]: Batch #1, batch length: 49

[2020-01-28 16:59:02.452201][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00173716 -0.0202228  -0.01024665 -0.02767087 -0.07932598  0.0321076
 -0.06089733  0.         -0.04612037  0.03827362 -0.08609756  0.
 -0.04127789  0.          0.          0.          0.          0.
  0.          0.          0.00202824  0.          0.          0.
  0.          0.          0.          0.         -0.08273817  0.03676138
 -0.03829591  0.         -0.03267313  0.          0.          0.02037273
 -0.06882472  0.         -0.05212415  0.02428516 -0.09730542  0.
 -0.04665129  0.          0.          0.          0.          0.
  0.          0.          0.00174001  0.          0.          0.
  0.          0.          0.          0.         -0.09350872  0.02332562
 -0.04328112  0.         -0.0369264   0.          0.         -0.02578344
  0.02578344 -0.04120641  0.04120641  0.          0.         -0.03511357
  0.03511357 -0.04531913  0.04531913 -0.07167773  0.07167773  0.
  0.         -0.02286526  0.02286526  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.04887944
  0.04887944  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.04462674  0.04462674 -0.02609587
  0.02609587 -0.03349876  0.03349876  0.          0.         -0.04868068
  0.04868068  0.          0.          0.          0.         -0.19886685
  0.19886685]

[2020-01-28 16:59:02.452599][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:02.512363][__main__.TRPOAgent.log][training]: gradient_step_direction: [ 0.08694419  1.34370728  0.03340708 -1.3268635   0.92477979  0.0697148
 -0.30644744  0.         -0.23208682  0.08310307 -0.43325987  0.
 -0.20771852  0.          0.          0.          0.          0.
  0.          0.          0.00975382  0.          0.          0.
  0.          0.          0.          0.         -0.41635489  0.07981947
 -0.19271254  0.         -0.16441752  0.          0.         -4.20450481
 -0.30529004  0.         -0.23121027 -5.01194751 -0.43162352  0.
 -0.206934    0.          0.          0.          0.          0.
  0.          0.         -0.13664964  0.          0.          0.
  0.          0.          0.          0.         -0.4147824  -4.81391917
 -0.19198469  0.         -0.16379653  0.          0.         -0.30268067
  0.30268067 -0.20643456  0.20643456  0.          0.         -0.17606301
  0.17606301 -0.43919589  0.43919589 -0.3596115   0.3596115   0.
  0.         -0.11435235  0.11435235  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.32575383
  0.32575383  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.22312723  0.22312723 -0.32814491
  0.32814491 -0.16811647  0.16811647  0.          0.         -0.24491391
  0.24491391  0.          0.          0.          0.         -0.88212648
  0.88212648]

[2020-01-28 16:59:02.553162][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.011483023555076641, New policy loss value: -0.066666119457434

[2020-01-28 16:59:02.590286][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 90.84026356239754

[2020-01-28 16:59:02.590686][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:02.593770][__main__.TRPOAgent.log][batch_info]: Batch #2, batch length: 47

[2020-01-28 16:59:02.611465][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00057041 -0.01184491 -0.00692871 -0.01971775 -0.05675359  0.
 -0.04550997  0.         -0.03370418  0.         -0.06369248  0.
 -0.03067642  0.          0.          0.          0.          0.
  0.          0.          0.01507806  0.          0.          0.
  0.          0.          0.          0.         -0.06171505  0.
 -0.02800447  0.         -0.02016233  0.          0.          0.
 -0.0437238   0.         -0.03238136  0.         -0.07401702  0.
 -0.02426022  0.          0.          0.          0.          0.
  0.          0.          0.0192338   0.          0.          0.
  0.          0.          0.          0.         -0.04880689  0.
 -0.03254399  0.         -0.02571938  0.          0.          0.
  0.         -0.02439555  0.02439555  0.          0.         -0.02192126
  0.02192126  0.          0.         -0.04636585  0.04636585  0.
  0.         -0.01213152  0.01213152  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.04122801
  0.04122801  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.02328841  0.02328841  0.
  0.         -0.02208772  0.02208772  0.          0.         -0.03727842
  0.03727842  0.          0.          0.          0.         -0.20673384
  0.20673384]

[2020-01-28 16:59:02.611917][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:02.675314][__main__.TRPOAgent.log][training]: gradient_step_direction: [-0.10146034  1.03525683  0.12603702 -1.26517942  1.76518649  0.
 -0.17122155  0.         -0.12680479  0.         -0.48346543  0.
  0.08280626  0.          0.          0.          0.          0.
  0.          0.          0.14393254  0.          0.          0.
  0.          0.          0.          0.          0.16659025  0.
 -0.21257132  0.         -0.19246595  0.          0.          0.
  1.68613324  0.          1.24873174  0.          0.40724255  0.
  2.28469957  0.          0.          0.          0.          0.
  0.          0.          0.16580695  0.          0.          0.
  0.          0.          0.          0.          4.59637535  0.
  0.17905745  0.         -0.22171658  0.          0.          0.
  0.         -0.28285601  0.28285601  0.          0.         -0.23060127
  0.23060127  0.          0.         -0.4607823   0.4607823   0.
  0.         -0.18166259  0.18166259  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.3982048
  0.3982048   0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.36509581  0.36509581  0.
  0.         -0.21661609  0.21661609  0.          0.         -0.35968064
  0.35968064  0.          0.          0.          0.         -1.78217029
  1.78217029]

[2020-01-28 16:59:02.715246][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.005970508571949688, New policy loss value: -0.0472950357905935

[2020-01-28 16:59:02.756926][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 69.49175635344804

[2020-01-28 16:59:02.757342][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:02.759637][__main__.TRPOAgent.log][batch_info]: Batch #3, batch length: 50

[2020-01-28 16:59:02.773803][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00041225 -0.00032825 -0.00028703 -0.0019237  -0.01688927  0.
 -0.01047958  0.         -0.00749309  0.         -0.01323389  0.
 -0.00753678  0.          0.          0.          0.          0.
  0.          0.          0.00949763  0.          0.          0.
  0.          0.          0.          0.         -0.0151673   0.
 -0.00564085  0.         -0.00138081  0.          0.          0.
 -0.02035798  0.         -0.01455631  0.         -0.02570858  0.
 -0.01464119  0.          0.          0.          0.          0.
  0.          0.          0.01845041  0.          0.          0.
  0.          0.          0.          0.         -0.02946449  0.
 -0.0109581   0.         -0.00268242  0.          0.          0.
  0.         -0.02580834  0.02580834  0.          0.         -0.02025538
  0.02025538  0.          0.         -0.01416878  0.01416878  0.
  0.         -0.03147814  0.03147814  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.01433665
  0.01433665  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.06298665  0.06298665  0.
  0.         -0.00672252  0.00672252  0.          0.         -0.00639944
  0.00639944  0.          0.          0.          0.         -0.09282037
  0.09282037]

[2020-01-28 16:59:02.774248][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:02.802133][__main__.TRPOAgent.log][training]: gradient_step_direction: [ 0.24777809  0.78814443 -0.03685692 -0.74867223 -0.11492867  0.
  0.40669163  0.          0.29079164  0.          0.51358066  0.
  0.29248734  0.          0.          0.          0.          0.
  0.          0.         -0.36858403  0.          0.          0.
  0.          0.          0.          0.          0.58861249  0.
  0.21891018  0.          0.05358665  0.          0.          0.
 -0.13853266  0.         -0.09905328  0.         -0.17494259  0.
 -0.0996309   0.          0.          0.          0.          0.
  0.          0.          0.1255519   0.          0.          0.
  0.          0.          0.          0.         -0.20050097  0.
 -0.07456806  0.         -0.01825339  0.          0.          0.
  0.          0.18246823 -0.18246823  0.          0.          0.18391291
 -0.18391291  0.          0.          0.51082168 -0.51082168  0.
  0.          0.02158435 -0.02158435  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.56609227
 -0.56609227  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.02884518 -0.02884518  0.
  0.          0.24564635 -0.24564635  0.          0.          0.45770113
 -0.45770113  0.          0.          0.          0.         -0.63162705
  0.63162705]

[2020-01-28 16:59:02.845720][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.004794657459532522, New policy loss value: -0.022679573269634455

[2020-01-28 16:59:02.885824][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 73.57479796271888

[2020-01-28 16:59:02.886220][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:02.889138][__main__.TRPOAgent.log][batch_info]: Batch #4, batch length: 50

[2020-01-28 16:59:02.910875][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.0069285  -0.01698072 -0.00837875 -0.01258085 -0.18093107  0.
 -0.03827906  0.         -0.03182655  0.         -0.07171979  0.
 -0.01838742  0.          0.          0.          0.          0.
  0.          0.         -0.02629864  0.          0.          0.
  0.          0.          0.          0.         -0.03578683  0.
 -0.03289743  0.         -0.0412031   0.          0.          0.
 -0.07127619  0.         -0.05926151  0.         -0.13354333  0.
 -0.03423766  0.          0.          0.          0.          0.
  0.          0.         -0.04896848  0.          0.          0.
  0.          0.          0.          0.         -0.06663562  0.
 -0.06125552  0.         -0.0767208   0.          0.          0.
  0.         -0.0613625   0.0613625   0.          0.         -0.04780821
  0.04780821  0.          0.         -0.03893688  0.03893688  0.
  0.         -0.07133704  0.07133704  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.0259528
  0.0259528   0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.14276533  0.14276533  0.
  0.         -0.01810297  0.01810297  0.          0.         -0.01532078
  0.01532078  0.          0.          0.          0.         -0.19581029
  0.19581029]

[2020-01-28 16:59:02.911404][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:02.950682][__main__.TRPOAgent.log][training]: gradient_step_direction: [ 7.44766811e-02 -2.21552744e-01  9.74909054e-02  8.93027773e-01
 -4.31334802e-01  0.00000000e+00  1.84592293e-01  0.00000000e+00
  1.53476485e-01  0.00000000e+00  3.45852817e-01  0.00000000e+00
  8.86692781e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  1.26819398e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  1.72574070e-01  0.00000000e+00  1.58640605e-01  0.00000000e+00
  1.98692835e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00
 -1.69920527e-01  0.00000000e+00 -1.41277866e-01  0.00000000e+00
 -3.18363731e-01  0.00000000e+00 -8.16216717e-02  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00 -1.16739539e-01  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00 -1.58857548e-01  0.00000000e+00
 -1.46031527e-01  0.00000000e+00 -1.82900324e-01  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00411015e-01
 -1.00411015e-01  0.00000000e+00  0.00000000e+00  8.52668627e-02
 -8.52668627e-02  0.00000000e+00  0.00000000e+00  2.72618744e-01
 -2.72618744e-01  0.00000000e+00  0.00000000e+00 -2.05130400e-04
  2.05130400e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  5.81538620e-02
 -5.81538620e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  0.00000000e+00 -4.44352436e-03
  4.44352436e-03  0.00000000e+00  0.00000000e+00  1.23932561e-01
 -1.23932561e-01  0.00000000e+00  0.00000000e+00  1.53865996e-01
 -1.53865996e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00
  0.00000000e+00 -4.66806526e-01  4.66806526e-01]

[2020-01-28 16:59:02.991657][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.004227561079311988, New policy loss value: -0.044276175205561666

[2020-01-28 16:59:03.032474][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 35.81815203472972

[2020-01-28 16:59:03.032867][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:03.035387][__main__.TRPOAgent.log][batch_info]: Batch #5, batch length: 48

[2020-01-28 16:59:03.053191][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00399951 -0.04137313 -0.02026826 -0.04413306 -0.26104464  0.
 -0.06897006  0.         -0.05745201  0.         -0.13433911  0.
 -0.03019654  0.          0.          0.          0.          0.
  0.          0.         -0.04672819  0.          0.          0.
  0.          0.          0.          0.         -0.05852561  0.
 -0.06155296  0.         -0.06784538  0.          0.          0.
 -0.0958147   0.         -0.07981358  0.         -0.18662682  0.
 -0.04194969  0.          0.          0.          0.          0.
  0.          0.         -0.06491582  0.          0.          0.
  0.          0.          0.          0.         -0.08130505  0.
 -0.08551071  0.         -0.0727236   0.          0.          0.
  0.         -0.087938    0.087938    0.          0.         -0.06883921
  0.06883921  0.          0.         -0.06596235  0.06596235  0.
  0.         -0.09652611  0.09652611  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.0376892
  0.0376892   0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.19298342  0.19298342  0.
  0.         -0.03052313  0.03052313  0.          0.         -0.02906229
  0.02906229  0.          0.          0.          0.         -0.24001526
  0.24001526]

[2020-01-28 16:59:03.053600][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:03.113316][__main__.TRPOAgent.log][training]: gradient_step_direction: [-0.36498782 -0.10031715  0.21882375  0.49218595  0.07234575  0.
  0.02032522  0.          0.01693089  0.          0.03958922  0.
  0.00889882  0.          0.          0.          0.          0.
  0.          0.          0.01377062  0.          0.          0.
  0.          0.          0.          0.          0.01724729  0.
  0.01813942  0.          0.71906502  0.          0.          0.
 -0.23795601  0.         -0.1982172   0.         -0.46348804  0.
 -0.10418219  0.          0.          0.          0.          0.
  0.          0.         -0.16121857  0.          0.          0.
  0.          0.          0.          0.         -0.20192133  0.
 -0.212366    0.          2.33862167  0.          0.          0.
  0.         -0.04661088  0.04661088  0.          0.         -0.03185531
  0.03185531  0.          0.          0.09674325 -0.09674325  0.
  0.         -0.12467638  0.12467638  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.00691082
  0.00691082  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.25199951  0.25199951  0.
  0.          0.0433733  -0.0433733   0.          0.         -0.00411142
  0.00411142  0.          0.          0.          0.         -0.59607845
  0.59607845]

[2020-01-28 16:59:03.146125][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New mean kl div: 0.007589991882316222, New policy loss value: -0.05665480625089977

[2020-01-28 16:59:03.185749][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 14.931296517347846

[2020-01-28 16:59:03.186147][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:03.188508][__main__.TRPOAgent.log][batch_info]: Batch #6, batch length: 47

[2020-01-28 16:59:03.204872][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00329173 -0.0286854  -0.01895336 -0.04981375 -0.214642    0.
 -0.06383388  0.         -0.05392465  0.         -0.10836474  0.
 -0.01671435  0.          0.          0.          0.          0.
  0.          0.         -0.04590605  0.          0.          0.
  0.          0.          0.          0.         -0.03127918  0.
 -0.05044698  0.         -0.07644036  0.          0.          0.
 -0.09156293  0.         -0.0773492   0.         -0.09313205  0.
 -0.02397496  0.          0.          0.          0.          0.
  0.          0.         -0.06584736  0.          0.          0.
  0.          0.          0.          0.         -0.04486667  0.
 -0.04447193  0.         -0.1096456   0.          0.          0.
  0.         -0.07134802  0.07134802  0.          0.         -0.05493044
  0.05493044  0.          0.         -0.04282581  0.04282581  0.
  0.         -0.09058446  0.09058446  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.02612198
  0.02612198  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.1815991   0.1815991   0.
  0.         -0.01974382  0.01974382  0.          0.         -0.24681903
  0.24681903  0.          0.          0.          0.         -0.24740809
  0.24740809]

[2020-01-28 16:59:03.205541][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:03.289695][__main__.TRPOAgent.log][training]: gradient_step_direction: [-0.16457158  0.09908701  0.04227896  0.17793174  0.32453983  0.
  0.82386906  0.          0.69597615  0.         -1.57718766  0.
  0.21572278  0.          0.          0.          0.          0.
  0.          0.          0.59248439  0.          0.          0.
  0.          0.          0.          0.          0.40370282  0.
 -0.27616984  0.          0.98657322  0.          0.          0.
 -0.60267584  0.         -0.5091197   0.          1.60965175  0.
 -0.157806    0.          0.          0.          0.          0.
  0.          0.         -0.43341342  0.          0.          0.
  0.          0.          0.          0.         -0.2953177   0.
  1.68905909  0.         -0.72169959  0.          0.          0.
  0.          0.75474494 -0.75474494  0.          0.          0.63040599
 -0.63040599  0.          0.         -1.54570115  1.54570115  0.
  0.          0.21879571 -0.21879571  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.44962599
 -0.44962599  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.41444988 -0.41444988  0.
  0.         -0.70231279  0.70231279  0.          0.          0.09186065
 -0.09186065  0.          0.          0.          0.         -1.62846369
  1.62846369]

[2020-01-28 16:59:03.329730][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.0025725656103254887, New policy loss value: -0.04392637987964224

[2020-01-28 16:59:03.368365][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 9.996797639004727

[2020-01-28 16:59:03.368776][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:03.371409][__main__.TRPOAgent.log][batch_info]: Batch #7, batch length: 42

[2020-01-28 16:59:03.386827][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00550568  0.00965218 -0.00247837 -0.02153613 -0.12926787  0.
 -0.0214565   0.         -0.0180536   0.         -0.01588154  0.
 -0.00583093  0.          0.          0.          0.          0.
  0.          0.         -0.01449448  0.          0.          0.
  0.          0.          0.          0.         -0.01096219  0.
 -0.00731387  0.         -0.01753571  0.          0.          0.
 -0.05930051  0.         -0.04989573  0.         -0.04389268  0.
 -0.01611527  0.          0.          0.          0.          0.
  0.          0.         -0.04005919  0.          0.          0.
  0.          0.          0.          0.         -0.03029681  0.
 -0.02021373  0.         -0.04846442  0.          0.          0.
  0.         -0.01299368  0.01299368  0.          0.         -0.00917728
  0.00917728  0.          0.         -0.00823165  0.00823165  0.
  0.         -0.02840099  0.02840099  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.00147324
  0.00147324  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.05733444  0.05733444  0.
  0.         -0.01937218  0.01937218  0.          0.         -0.08055026
  0.08055026  0.          0.          0.          0.         -0.10360075
  0.10360075]

[2020-01-28 16:59:03.387231][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:03.428034][__main__.TRPOAgent.log][training]: gradient_step_direction: [-0.07629554  0.94442993  0.16979468 -0.71304231 -0.51515182  0.
  0.30008779  0.          0.25249531  0.          0.22211713  0.
  0.08155068  0.          0.          0.          0.          0.
  0.          0.          0.2027179   0.          0.          0.
  0.          0.          0.          0.          0.15331579  0.
  0.10229076  0.          0.24525218  0.          0.          0.
 -0.23632144  0.         -0.198842    0.         -0.17491895  0.
 -0.0642218   0.          0.          0.          0.          0.
  0.          0.         -0.15964192  0.          0.          0.
  0.          0.          0.          0.         -0.12073738  0.
 -0.08055478  0.         -0.19313808  0.          0.          0.
  0.          0.26797129 -0.26797129  0.          0.          0.22505883
 -0.22505883  0.          0.          0.19882018 -0.19882018  0.
  0.          0.06963149 -0.06963149  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.17218641
 -0.17218641  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.13056608 -0.13056608  0.
  0.          0.06894435 -0.06894435  0.          0.          0.11205413
 -0.11205413  0.          0.          0.          0.         -0.41286479
  0.41286479]

[2020-01-28 16:59:03.465923][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.0049817733494651305, New policy loss value: -0.03602763051613539

[2020-01-28 16:59:03.511135][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 9.765834850125906

[2020-01-28 16:59:03.511522][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:03.513953][__main__.TRPOAgent.log][batch_info]: Batch #8, batch length: 32

[2020-01-28 16:59:03.526373][__main__.TRPOAgent.log][training]: policy_gradient: [-0.00885128  0.04162536  0.01423048 -0.02678339  0.05875837  0.
  0.03197013  0.          0.02689129  0.          0.02367326  0.
  0.00573558  0.          0.          0.          0.          0.
  0.          0.          0.02211161  0.          0.          0.
  0.          0.          0.          0.          0.01077824  0.
  0.00694167  0.          0.01591218  0.          0.          0.
  0.04934758  0.          0.04150812  0.          0.03654092  0.
 -0.00399439  0.          0.          0.          0.          0.
  0.          0.          0.04357395  0.          0.          0.
  0.          0.          0.          0.         -0.00750621  0.
 -0.00483433  0.         -0.0110816   0.          0.          0.
  0.          0.01795392 -0.01795392  0.          0.          0.01367399
 -0.01367399  0.          0.          0.01218243 -0.01218243  0.
  0.          0.00544029 -0.00544029  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.00376185
 -0.00376185  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.01029113 -0.01029113  0.
  0.          0.00543173 -0.00543173  0.          0.          0.01069948
 -0.01069948  0.          0.          0.          0.         -0.02139815
  0.02139815]

[2020-01-28 16:59:03.526813][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:03.637699][__main__.TRPOAgent.log][training]: gradient_step_direction: [-3.27307795  0.23372492  3.78987523 -0.57195238  2.17150956  0.
  0.96373305  0.          0.81063218  0.          0.71362534  0.
 -0.11616491  0.          0.          0.          0.          0.
  0.          0.         -0.51457826  0.          0.          0.
  0.          0.          0.          0.         -0.21829598  0.
 -0.14059135  0.         -0.32227563  0.          0.          0.
  1.74211195  0.          1.46535597  0.          1.28999956  0.
 -0.33578079  0.          0.          0.          0.          0.
  0.          0.          4.10499109  0.          0.          0.
  0.          0.          0.          0.         -0.63099589  0.
 -0.4063872   0.         -0.93155451  0.          0.          0.
  0.          0.50476962 -0.50476962  0.          0.          0.37676379
 -0.37676379  0.          0.          0.33633669 -0.33633669  0.
  0.         -0.47089526  0.47089526  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -1.38036358
  1.38036358  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.95305044  0.95305044  0.
  0.         -0.31200509  0.31200509  0.          0.         -1.33668576
  1.33668576  0.          0.          0.          0.         -1.79877913
  1.79877913]

[2020-01-28 16:59:03.663020][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 0, New mean kl div: 0.0008639270731757219, New policy loss value: -0.018653320043867026

[2020-01-28 16:59:03.803586][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 3.7175778969713065

[2020-01-28 16:59:03.803959][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

[2020-01-28 16:59:03.806887][__main__.TRPOAgent.log][batch_info]: Batch #9, batch length: 50

[2020-01-28 16:59:03.821485][__main__.TRPOAgent.log][training]: policy_gradient: [ 0.00402888 -0.00058966 -0.00746189 -0.03483823 -0.15713121  0.
 -0.09204364  0.         -0.07542159  0.         -0.0665909   0.
  0.00056332  0.          0.          0.          0.          0.
  0.          0.          0.01021656  0.          0.          0.
  0.          0.          0.          0.          0.00390872  0.
 -0.01010451  0.          0.00282938  0.          0.          0.
 -0.09910006  0.         -0.0812037   0.         -0.07169601  0.
  0.00060651  0.          0.          0.          0.          0.
  0.          0.          0.0109998   0.          0.          0.
  0.          0.          0.          0.          0.00420838  0.
 -0.01087916  0.          0.00304629  0.          0.          0.
  0.         -0.10395256  0.10395256  0.          0.         -0.08489232
  0.08489232  0.          0.         -0.07501627  0.07501627  0.
  0.         -0.03752047  0.03752047  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.10047013
  0.10047013  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.         -0.0761701   0.0761701   0.
  0.         -0.02247102  0.02247102  0.          0.         -0.09914772
  0.09914772  0.          0.          0.          0.         -0.11033917
  0.11033917]

[2020-01-28 16:59:03.822049][__main__.TRPOAgent.log][training]: Initiating conjugate gradients

[2020-01-28 16:59:03.862331][__main__.TRPOAgent.log][training]: gradient_step_direction: [ 0.02437921  0.70347447  0.02616712 -0.71966216 -0.48152621  0.
  0.26594553  0.          0.21791875  0.          0.19240386  0.
 -0.00162761  0.          0.          0.          0.          0.
  0.          0.         -0.0295192   0.          0.          0.
  0.          0.          0.          0.         -0.0112936   0.
  0.02919539  0.         -0.00817501  0.          0.          0.
 -0.3036907   0.         -0.24884757  0.         -0.2197114   0.
  0.00185864  0.          0.          0.          0.          0.
  0.          0.          0.03370867  0.          0.          0.
  0.          0.          0.          0.          0.01289654  0.
 -0.03333901  0.          0.00933533  0.          0.          0.
  0.          0.13195484 -0.13195484  0.          0.          0.11217122
 -0.11217122  0.          0.          0.0990381  -0.0990381   0.
  0.          0.04725928 -0.04725928  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.         -0.18303961
  0.18303961  0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.08551598 -0.08551598  0.
  0.          0.05995224 -0.05995224  0.          0.          0.08172225
 -0.08172225  0.          0.          0.          0.         -0.33813285
  0.33813285]

[2020-01-28 16:59:03.908606][__main__.TRPOAgent.log][linesearch]: Linesearch worked at 1, New mean kl div: 0.004716109574144005, New policy loss value: -0.03468323655973787

[2020-01-28 16:59:03.953446][__main__.TRPOAgent.log][batch_info]: Current batch value loss: 54.897009996324776

[2020-01-28 16:59:03.953856][__main__.TRPOAgent.log][linesearch]: Linesearch successful, updating policy parameters

